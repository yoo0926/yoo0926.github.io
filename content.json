{"pages":[{"title":"about","text":"Intro email : wodydasktp@gmail.com github : https://github.com/yoo0926 blog : https://yoo0926.github.io/ 자바와 스프링을 주력으로 개발해 온 5년차 개발자 유재용입니다. HR인사시스템, 소스코드 정적분석 및 검증, OTA 여행사 등 다양한 분야에서 개발했으며 새로운 환경에 적응도 빠른 편입니다. 항상 심심하지 않게 개발하고 싶어서 사내 개발 문화를 개선하고 새로운 기술을 적용하는 것을 좋아합니다. 무엇이든 배우는 걸 좋아하고 배운 걸 함께 이야기하며 나누는 건 더 좋아하는 편입니다. Skill Language: java, typescript, python, c#, scala Framework: spring boot, spring webFlux, spring mvc, myBatis, angular Database: oracle, postgresql, redis, mysql Etc: git, jenkins Career 노랑풍선(2021.11 ~ ) OTA시스템개발팀 - 백엔드 개발, 프론트엔드 개발, 서비스 운영 패키지여행 및 실시간 항공권, 호텔 등 다양한 여행정보를 제공하는 여행사의 OTA(Online Travel Agency) 플랫폼의 항공 시스템을 개발하였고 운영 중 발생하는 다양한 이슈에 대응하였습니다. 그 외 API 문서 자동화, 코딩 컨벤션 정리, git merge 템플릿 작성 등 개발 문화 개선에 기여하였습니다. java(spring webFlux, myBatis, junit), oracle, redis, typescript(angular), jenkins, git(Gitlab) 코드마인드(2019.07 ~ 2021.04) SW연구소 정적분석팀 - 백엔드 개발, 윈도우 앱 개발 소프트웨어 분석 및 검증을 전문적으로 하는 기업의 정적분석팀 선임연구원으로 오픈소스 수집기 개발과 윈도우용 오픈소스 라이선스 분석 프로그램, 분석엔진의 개선작업을 하였습니다. 또한 기업의 주력 솔루션인 정적분석 도구 CodeMind CSI/CQI의 기능개발, 개선 업무를 하였습니다. java(spring boot, myBatis), oracle, postgresql, c#, python, scala 휴먼컨설팅그룹(2017.04 ~ 2019.03) Solution Division - 풀스택 개발 웹기반 인사시스템인 E-HR 솔루션을 개발하는 회사에서 인사,근태,급여 등 관련 모듈의 풀스택 개발과 각 고객사의 커스텀 개발, 운영 및 유지보수를 진행하였습니다. java(Struts, jsp), oracle Edu [2011.03 ~ 2017.02] 성공회대학교 소프트웨어공학과 졸업 License 취득일 자격증명 발급기관 2019.08.16 정보처리기사 한국산업인력공단 Learn 비트교육센터(2016.07 ~ 2016.12) JAVA Enterprise 고급양성과정 E 교내에서 진행한 청년취업아카데미 연수과정으로 Java 기반의 웹개발자 양성을 위한 기초부터 심화과정을 교육받았습니다. 졸업팀플로 Node.js와 Socket.io를 사용하여 채팅 프로그램을 구현하고 HTML Canvas를 사용하여 UML다이어그램으로 설계 후 Spring MVC 패턴의 스켈레톤 코드를 생성하는 프로그램을 개발했습니다. Work 노랑풍선 그 외 Enum 리팩토링 노랑풍선과 네이버의 예약,결제,발권 등 시스템코드가 달라서 매핑하는 작업이 필요한데 if문을 중첩시켜서 매핑되는 상태값을 사용중 Enum으로 시스템코드들을 그룹화하여 각 타입은 본인이 수행해야할 기능만 책임지도록 하고 입출력을 Enum으로 하여 별도의 추가검증로직이 필요없어 side-effect를 줄임 랜딩페이지 제거 후 재검색 기능 추가 제휴사용 운임규정 모듈 구현 비회원 인증처리 개선 프로모션 코드 서비스 모듈 구현 오류메시지 백오피스 관리자메모 등록 로직 개선 스카이스캐너 태그 트래킹 개발 (1개월 / 2023.05 ~ 2023.05) 스카이스캐너의 SAT(SkyScanner Analytics Tag)트래킹 방식이 변경되서 신규 DownStream Visibility 태그방식의 트래킹 스크립트 구현 랜딩, 예약완료 화면에서 로깅을 위한 동적 임베딩 구현 기존 Enum 으로 작성된 타입들은 번들러에서 트리쉐이킹이 되지 않아서 union Type으로 모두 리팩토링 네이버 국내선 서비스 개발 (3개월 / 2023.02 ~ 2023.05) 네이버 여행상품의 항공 카테고리의 국내선 서비스 입점하기 위해 필요한 API 구현 네이버에서 호출하는 항공 스케줄 조회 API 구현 기존 노랑풍선의 항공스케줄 조회는 10분의 캐시를 잡고 있었으나 네이버 관련 API는 운임조회부터 발권까지 모든 프로세스를 캐시없는 완전 실시간 조회로 구현 운임조회 시 캐시데이터의 index로 예약을 진행했으나 실시간조회는 예약에 필요한 필수값들을 모두 검증하여 일관성을 유지하며 예약진행 네이버 예약데이터 연동 서비스 구현 예약,발권,결제,취소 등 예약데이터가 생성,수정될 경우 네이버로 예약데이터를 연동시키기 위한 서비스를 구현 spring Event 로 구현하여 직접 의존성을 주입하지 않고 ApplicationContext에 이벤트를 넘겨줘서 Listener로 처리하여 서비스간의 의존성을 줄이고 추후 다른 제휴사로 확장해서 사용할 수 있도록 개발 네이버 프라이싱 리포트 개발 (1개월 / 2023.01 ~ 2023.01) 네이버에서 조회되는 전체 항공권 정보를 가져오는 API를 사용하여 백오피스에서 확인가능하도록 개발 특정 날짜 범위에선 데이터가 너무 커서 WebClient Buffer의 기본사이즈에 담을 수 없어서 10MB로 메모리 할당 네이버 응답값을 파싱하여 노랑풍선 키와 매칭시켜야 하여 서비스 시작 시 DB에서 미리 매핑에 필요한 데이터를 가져와 HashMap으로 메모리에 들고 있다가 응답값을 키로 map에서 찾는 방식으로 구현하여 매핑 시간 단축 백오피스 화면은 angular와 kendo-UI를 사용하여 구현 BC카드 페이북,에어머니,VIP 서비스 개발 (2개월 / 2022.10 ~ 2022.12) 항공 검색 오류처리 개선 느린 항공사 웹서비스와 자사의 캐시로 인해 빈번하게 발생하는 Timeout을 Custom Exception으로 새로 구현 StackTrace를 만들지 않고 캐싱해둔 오류를 반환하고 Handler에서 정상응답으로 변환하여 처리 불필요한 오류를 걸러내서 모니터링의 효율을 높임 작성중 네이버페이 연동 개발 (3개월 / 2022.03 ~ 2022.06) 대한항공 본인인증 모듈 구현 (1개월 / 2022.02 ~ 2022.02) 마케팅 스크립트 공통 모듈 개선 (1개월 / 2022.01 ~ 2022.01) 코드마인드 CodeMind CSI/CQI 기능 개선 저작권위원회 오픈소스 라이선스 시스템(OLIS) 개선 휴먼컨설팅그룹 CMS에듀 인사시스템 구축 우아한형제들 인사시스템 구축 한국중부발전 웹표준화 및 고도화 대림산업 글로벌 인사시스템 구축","link":"/about/index.html"}],"posts":[{"title":"Angular 개요","text":"회사 프론트엔드가 Angular로 되어있는데 이쪽을 메인잡으로 일하진 않지만 프론트 개발자를 많이 채용해서 완전히 업무를 분리하기 전까지 이쪽도 어느정도 할줄 알아야 할거같아서 Angular에 대해서 공부하면서 간단히 정리해보자 Angular❓ Angular는 Google에서 만든 **SPA(Single Page Application)**방식의 프론트엔드 개발을 위한 자바스크립트 프레임워크 이다. 개발언어로 es6, Dart 등을 지원하기도 하지만 공식적으로 TypeScript를 권장하고 있다. Angular 1.x 버전을 AngularJS, Angular 2 이상 버전을 Angular 라고 부른다. AngularJS와 Angular는 하위 호환성이 없는 브레이킹 체인지를 다수 포함하여 큰 차이점이 많은데 몇가지 정리를 하자면 Controller와 $scope 기반에서 컴포넌트 기반 개발(CBD, Component Based Development)로 전환되었다. 이전보다 향상된 모듈 시스템과 DOM 제어 기능을 제공하며 API가 단순화 되었다. 주력 개발언어로 TypeScript를 도입하여 대규모 개발에 적합한 정적타입과 인터페이스, 제네릭 등 타입체크지원 기능을 제공한다. ECMAScript6에서 새롭게 도입된 모듈, 클래스, ECMAScript7의 데코레이터를 지원한다. 간단한 명령어로 개발환경을 지원하여 프로젝트 스케폴딩을 생성, 실행, 빌드할 수 있는 Angular CLI를 제공한다. Angular 장점👍 컴포넌트 기반의 기능에 따라 코드분리와 재사용성이 쉬운 장점이 있다. SPA방식으로 다른 페이지로의 전환 속도가 빠르다. 프레임워크로서 개발에 필요한 대부분의 기능을 탑재하고 있다. Angular 단점👎 TypeScript를 주언어로 사용하여 이에 대한 학습이 필요하고 추가로 Angular의 여러 개념들과 기능들을 익혀야 하는만큼 학습량이 높은 편이다. 웹페이지가 모두 로딩된 이후 전환은 빠른편이지만 초기 로딩 시 느린 편이다. 검색엔진 최적화가 잘 되지 않아서 구글을 제외한 다른 사이트들에서 제대로 수집되지 않는 경우가 있다고 한다.","link":"/2021/11/13/angular/angular-1/"},{"title":"유클리드 호제법","text":"유클리드 호제법(Euclidean algorithm)은 최대공약수, 최소공배수를 구하는 가장 대중적인 알고리즘으로 호제법이란 두 수가 서로 상대방 수를 나누어서 결국 원하는 수를 얻는 알고리즘을 말한다. 2개의 자연수 a,b에 대해서 a를 b로 나눈 나머지를 r이라고 한다면 (단, a&gt;b) a와 b의 최대공야수는 b와 r의 최대공약수와 같다. 이 성질에 따라, b를 r로 나눈 나머지 r’를 구하고, 다시 r을 r’로 나눈 나머지를 구하는 과정을 반복하여 나머지가 0이 되었을 때 나누는 수가 a와 b의 최대공약수이다. 이해하기 쉽게 예를 들어 108과 78의 최대공약수를 구해보면 아래와 같은 연산을 할 수 있다. 108 % 78 = 30 // 큰수를 작은수로 나누고 나머지를 구하기 78 % 30 = 18 // 위에서 나눈수(78)를 가져와 위의 나머지로 다시 나눠서 나머지 구하기 30 % 18 = 12 // 반복 18 % 12 = 6 12 % 6 = 0 // 나머지가 0이 되면 이때 나눈 수가 최대공약수(=6) 그렇다면 최소공배수는 어떻게 구할까? 최소공배수의 규칙에서 두 수 a,b가 있을 때 a * b = 최대공약수 * 최소공배수 라는 공식이 성립한다. 따라서 주어진 두수를 곱하고 최대공약수로 나눠준다면 그 수가 최소공배수가 될 것이다. 108 * 78 / 6 = 1404 이를 간단하게 코드로 구현해보면 다음과 같다.","link":"/2021/09/02/algo/algo-1/"},{"title":"[프로그래머스] 신규 아이디 추천","text":"🔗 출처 신규 아이디 추천 : https://programmers.co.kr/learn/courses/30/lessons/72410 📔 문제 설명 카카오에 입사한 신입 개발자 네오는 &quot;카카오계정개발팀&quot;에 배치되어, 카카오 서비스에 가입하는 유저들의 아이디를 생성하는 업무를 담당하게 되었습니다. &quot;네오&quot;에게 주어진 첫 업무는 새로 가입하는 유저들이 카카오 아이디 규칙에 맞지 않는 아이디를 입력했을 때, 입력된 아이디와 유사하면서 규칙에 맞는 아이디를 추천해주는 프로그램을 개발하는 것입니다. 다음은 카카오 아이디의 규칙입니다. 아이디의 길이는 3자 이상 15자 이하여야 합니다. 아이디는 알파벳 소문자, 숫자, 빼기(-), 밑줄(_), 마침표(.) 문자만 사용할 수 있습니다. 단, 마침표(.)는 처음과 끝에 사용할 수 없으며 또한 연속으로 사용할 수 없습니다. &quot;네오&quot;는 다음과 같이 7단계의 순차적인 처리 과정을 통해 신규 유저가 입력한 아이디가 카카오 아이디 규칙에 맞는 지 검사하고 규칙에 맞지 않은 경우 규칙에 맞는 새로운 아이디를 추천해 주려고 합니다. 신규 유저가 입력한 아이디가 new_id 라고 한다면, 1단계 new_id의 모든 대문자를 대응되는 소문자로 치환합니다. 2단계 new_id에서 알파벳 소문자, 숫자, 빼기(-), 밑줄(_), 마침표(.)를 제외한 모든 문자를 제거합니다. 3단계 new_id에서 마침표(.)가 2번 이상 연속된 부분을 하나의 마침표(.)로 치환합니다. 4단계 new_id에서 마침표(.)가 처음이나 끝에 위치한다면 제거합니다. 5단계 new_id가 빈 문자열이라면, new_id에 &quot;a&quot;를 대입합니다. 6단계 new_id의 길이가 16자 이상이면, new_id의 첫 15개의 문자를 제외한 나머지 문자들을 모두 제거합니다. 만약 제거 후 마침표(.)가 new_id의 끝에 위치한다면 끝에 위치한 마침표(.) 문자를 제거합니다. 7단계 new_id의 길이가 2자 이하라면, new_id의 마지막 문자를 new_id의 길이가 3이 될 때까지 반복해서 끝에 붙입니다. 신규 유저가 입력한 아이디를 나타내는 new_id가 매개변수로 주어질 때, &quot;네오&quot;가 설계한 7단계의 처리 과정을 거친 후의 추천 아이디를 return 하도록 solution 함수를 완성해 주세요. ✅ 제한사항 new_id는 길이 1 이상 1,000 이하인 문자열입니다. new_id는 알파벳 대문자, 알파벳 소문자, 숫자, 특수문자로 구성되어 있습니다. new_id에 나타날 수 있는 특수문자는 -_.~!@#$%^&amp;*()=+[{]}:?,&lt;&gt;/ 로 한정됩니다. 🔍 입출력 예 no new_id result 예1 \"...!@BaT#*..y.abcdefghijklm\" \"bat.y.abcdefghi\" 예2 \"z-+.^.\" \"z--\" 예3 \"=.=\" \"aaa\" 예4 \"123_.def\" \"123_.def\" 예4 \"abcdefghijklmn.p\" \"abcdefghijklmn\" 📝 풀이 정규식을 사용하면 어렵지 않게 풀 수있는데 정규식을 자주 쓰진 않아서 자세한 규칙들은 블로그를 검색해보고 정규식 검증은 https://regexr.com에서 진행했다.","link":"/2021/05/09/algo/programmers-72410/"},{"title":"[프로그래머스] 약수의 개수와 덧셈","text":"🔗 출처 약수의 개수와 덧셈 : https://programmers.co.kr/learn/courses/30/lessons/77884 📔 문제설명 두 정수 left와 right가 매개변수로 주어집니다. left부터 right까지의 모든 수들 중에서, 약수의 개수가 짝수인 수는 더하고, 약수의 개수가 홀수인 수는 뺀 수를 return 하도록 solution 함수를 완성해주세요. ✅ 제한사항 1 ≤ left ≤ right ≤ 1,000 🔍 입출력 예 left right result 13 17 43 24 27 52 📝 풀이 어떤 자연수의 약수를 구하는 가장 쉬운 방법은 자연수 N을 i = 1 ~ N 까지 나눠서 나머지가 0으로 나오는 i의 개수를 찾으면 된다. 이러한 경우 최소값 1 ~ 자기자신 N까지 확인하므로 시간복잡도는 O(n) 이 나온다. 더 빠르게 여기서 약수의 특성에 대해서 조금 더 생각해 본다면 항상 약수는 그 짝이 되는 수가 존재한다. (ex. 15 = 3 * 5) 즉, N의 약수들 중 두 약수의 곱이 N이 되는 약수 a,b는 반드시 존재하므로 N의 제곱근까지 약수를 구하면 그 짝이 되는 약수는 자동으로 구했다고 볼 수 있다. 이 방법을 사용하여 약수를 구하면 시간복잡도는 O(n^(1/2)) 이 나온다. ➕ 추가 프로그래머스에서 올린 해설을 찾아보니 애초에 문제에서 요구하는건 모든 약수를 구하는게 아니라 약수의 개수가 짝수인건 더하고 홀수인건 빼는 것이라서 약수가 홀수인지 짝수인지만 구하면 된다. 약수를 구해보면 약수가 홀수라면 그 수는 약수의 제곱수로 나오므로 매개변수 left ~ right 의 제곱수만 구하면 문제를 풀 수 있다.","link":"/2021/06/10/algo/programmers-77884/"},{"title":"Hexo 버전 업그레이드, 경고 지우기","text":"버전 업그레이드 오랜만에 블로그를 정비하다가 hexo 버전을 업그레이드 하고 싶었다. node가 그렇게 익숙하진 않아서 구글링으로 찾아보니 그냥 원하는 버전을 npm으로 설치하면 된다고 한다. How can I upgrade hexo? #4572 7.0.0버전이 최신이긴 한데 이슈 생기면 대응하기 귀찮으니까 그냥 바로 아래의 6.3.0 으로 업그레이드 후 실행했다. 12345678910111213141516171819202122232425npm i hexo@6.3.0hexo sWARN YAMLException: please see https://github.com/hexojs/hexo/issues/4917FATAL TypeError: Cannot read property 'length' of undefined at composeNode (D:\\workspace\\blog\\node_modules\\hexo\\node_modules\\js-yaml\\lib\\loader.js:1492:60) at composeNode (D:\\workspace\\blog\\node_modules\\hexo\\node_modules\\js-yaml\\lib\\loader.js:1441:12) at readDocument (D:\\workspace\\blog\\node_modules\\hexo\\node_modules\\js-yaml\\lib\\loader.js:1625:3) at loadDocuments (D:\\workspace\\blog\\node_modules\\hexo\\node_modules\\js-yaml\\lib\\loader.js:1688:5) at Object.load (D:\\workspace\\blog\\node_modules\\hexo\\node_modules\\js-yaml\\lib\\loader.js:1714:19) at Hexo.yamlHelper (D:\\workspace\\blog\\node_modules\\hexo\\lib\\plugins\\renderer\\yaml.js:20:15) at Hexo.tryCatcher (D:\\workspace\\blog\\node_modules\\bluebird\\js\\release\\util.js:16:23) at Hexo.&lt;anonymous&gt; (D:\\workspace\\blog\\node_modules\\bluebird\\js\\release\\method.js:15:34) at D:\\workspace\\blog\\node_modules\\hexo\\lib\\hexo\\render.js:81:22 at tryCatcher (D:\\workspace\\blog\\node_modules\\bluebird\\js\\release\\util.js:16:23) at Promise._settlePromiseFromHandler (D:\\workspace\\blog\\node_modules\\bluebird\\js\\release\\promise.js:547:31) at Promise._settlePromise (D:\\workspace\\blog\\node_modules\\bluebird\\js\\release\\promise.js:604:18) at Promise._settlePromise0 (D:\\workspace\\blog\\node_modules\\bluebird\\js\\release\\promise.js:649:10) at Promise._settlePromises (D:\\workspace\\blog\\node_modules\\bluebird\\js\\release\\promise.js:729:18) at _drainQueueStep (D:\\workspace\\blog\\node_modules\\bluebird\\js\\release\\async.js:93:12) at _drainQueue (D:\\workspace\\blog\\node_modules\\bluebird\\js\\release\\async.js:86:9) at Async._drainQueues (D:\\workspace\\blog\\node_modules\\bluebird\\js\\release\\async.js:102:5) at Immediate.Async.drainQueues [as _onImmediate] (D:\\workspace\\blog\\node_modules\\bluebird\\js\\release\\async.js:15:14) at processImmediate (internal/timers.js:464:21) ??? 뭐야 갑자기 왜… 당황했지만 일단 에러 로그에서 들어가보라고 나온 github issue 를 들어가보니 비슷한 오류에 대해서 이미 많은 토론이 진행되어 있었다. 6.1.0 버전으로 올렸을때 이런 이슈가 발생했고(2,3 도 비슷한듯) 해결책은 크게 두가지 였는데 6.0.0 으로 다운그레이드 js-yaml 을 4.1.0 으로 업그레이드 이와 별개로 현재 사용중인 icarus 테마도 지원하는 버전이 있을거라고 생각해서 들어가보니 5.0.1 버전에서 hexo 6에 대한 언급이 있어서 이참에 최신버전으로 올렸다. 일단 위 내용들을 고려해서 6.0.0 으로 다운그레이드를 시도 했고 다시 실행해보니 정상적으로 실행되었다. 12npm i hexo@6.0.0hexo s 아직 의문인 점은 6.3.0 버전에서 설치되는 js-yaml은 분명 4.1.0 인데 서버 실행 시 오류가 발생하고 6.0.0으로 한번 내렸다가 다시 6.3.0 으로 올리면 정상적으로 실행된다는 점이다 … ㅎㅎ;; 거슬리던 경고들 하는 김에 그동안 거슬렸던 경고들도 몇개 있었는데 대략 아래와 같은 내용이었다. 1234567891011121314151617181920212223242526INFO Validating configInferno is in development mode.INFO ======================================= ██╗ ██████╗ █████╗ ██████╗ ██╗ ██╗███████╗ ██║██╔════╝██╔══██╗██╔══██╗██║ ██║██╔════╝ ██║██║ ███████║██████╔╝██║ ██║███████╗ ██║██║ ██╔══██║██╔══██╗██║ ██║╚════██║ ██║╚██████╗██║ ██║██║ ██║╚██████╔╝███████║ ╚═╝ ╚═════╝╚═╝ ╚═╝╚═╝ ╚═╝ ╚═════╝ ╚══════╝=============================================INFO === Checking package dependencies ===INFO === Checking theme configurations ===WARN Theme configurations failed one or more checks.WARN Icarus may still run, but you will encounter unexcepted results.WARN Here is some information for you to correct the configuration file.WARN [ { keyword: 'const', dataPath: '.widgets[7].type', schemaPath: '#/properties/type/const', params: { allowedValue: 'profile' }, message: 'should be equal to constant' },......] 로그로 봤을땐 테마의 위젯 설정값이 뭔가 잘못된거 같길래 한참 이것저것 검색하다가 위젯 설정값을 찬찬히 보니까 type의 값을 주석처리 해놔서 발생한 오류였다. 진짜 이거 찾느라 시간 보낸거 아까워서 ㅠㅠ 나처럼 바보짓 하는 사람 없길 바라면 적는다. 1234567- # Where should the widget be placed, left sidebar or right sidebar position: left type: #adsense # AdSense client ID client_id: &quot;&quot; # AdSense AD unit ID slot_id: &quot;&quot; 작업들을 하면서 느끼는건데 Jekyll을 사용해서 샘플로 만들어봤을때 너무 느린 속도가 싫어서 Hexo를 선택했는데 포스팅한 글을 git으로 동시관리가 안되고 별도 repo 설정해서 이중관리 해야하는 부분이나 영어보다 중국어 레퍼런스가 더 많아서 관련 자료 찾는데 쉽지 않은 부분 때문에 요즘엔 다른 엔진으로 갈아타는건 어떨까 하는 생각도 든다. 일단 블로그 글도 별로 없는데 이런거 고민할 필요는 없을거 같기도 하고 ㅋㅋㅋ 나중에 다시 생각해보자","link":"/2023/05/21/blog/hexo-upgrade/"},{"title":"Github Page와 Hexo를 활용하여 블로그 개설하기","text":"블로그를 시작하며 그동안 OneNote, Notion 등 노트프로그램을 사용하여 단편적으로 공부하던 내용을 블로그를 통해 좀 더 체계적으로 정리하고 다른사람들과 공유하고 싶은 생각에 블로그를 시작하기로 했다.(끈기가 부족한 편이라 얼마나 갈진 모르겠다^^;;) 일반적인 블로그를 생각하면 네이버 블로그, 티스토리, 혹은 미디엄, 브런치 등을 떠올릴 수 있지만 기술블로그를 표방하는만큼 github page를 이용하기로 마음먹었다. github page에서는 username.github.io 도메인을 무료로 제공하여 정적 웹페이지를 무료 호스팅해주고 있다. github page에서 사용할 정적 웹페이지 생성기는 생각보다 다양한 종류가 있지만 마크다운 문서를 지원하면서 국내에서 가장 많이 사용되는 두가지는 다음과 같다. Jekyll Ruby 기반 다양한 테마, 플러그인 지원 환경설정 및 커스터마이징이 다소 복잡하며(특히 윈도우에서) 글이 많아지면 빌드속도가 느려진다는 이야기가 있다. Hexo Javascript(Node.js) 기반 마찬가지로 다양한 테마, 플러그인 지원 npm을 통해 쉽게 설치 가능하며 Github 배포과정도 편하다. 처음엔 Jekyll을 사용하여 초기설정을 해봤는데 윈도우에서 설정이 너무 복잡하고 오류도 많이나서 익숙한 Node.js기반의 Hexo로 시작하기로 했다. 1. 설치 1.1 사전준비 Node.js 설치 Git 설치 Github 가입 후 신규 Repository 2개 생성 각 프로그램 설치는 이미 많은 문서들이 있기 때문에 별도로 언급하진 않겠다. Repository를 2개 생성하는 이유는 각각 블로그 운영, hexo 설정을 저장할 목적이며 블로그로 운영할 Repository name은 USERNAME.github.io 의 Public으로 생성한다. 2. Hexo 설치 및 블로그 생성 Hexo설정을 저장할 github repository를 clone 한 후 해당 위치에서 작업한다. 1234npm install -g hexo-clihexo init $디렉토리명cd $디렉토리명npm install 2.1 설정파일 수정 블로그 생성이 정상적으로 완료되었으면 이제 설정파일을 수정해보자. root 경로에 _config.yml파일을 확인할 수 있는데 기본적인 블로그 설정은 이곳에서 관리한다. 자세한 내용은 공식문서에서 확인할 수 있다. Hexo Docs : https://hexo.io/docs/ Site 설정 블로그 이름 및 간략한 소개를 설정한다. 12345title: Hello Worldsubtitle: No pain, No gaindescription:author: Jaeyong Yoo... URL 설정 블로그 URL 정보를 설정한다. 1234url: https://USERNAME.github.ioroot: /permalink: :year/:month/:day/:title/permalink_defaults: Github 설정 배포할 Github Page의 Repository 정보를 입력한다. 123deploy: type: git repo: https://github.com/yoo0926/yoo0926.github.io 정상적으로 설치와 설정이 끝났다면 아래 명령어로 서버를 실행시켜 웹브라우저에서 http://localhost:4000 으로 접속하여 확인할 수 있다. 1hexo server #or hexo s 만약 로컬에서 실행 시 permission denied 가 발생하는 경우가 있다면 그냥 port를 변경해서 테스트하자 1hexo server -p 8088 3. Github에 배포하기 로컬에서 테스트가 완료되었다면 이제 앞서 언급한 Github Page의 정적 웹페이즈 호스팅을 사용해보자. Hexo Generate and Deploy github에 배포하기 위해선 hexo-deployer-git 이라는 플러그인을 설치해야 한다. 1npm install hexo-deployer-git 플러그인 설치가 완료되면 배포할 리소스를 생성하여 앞서 _config.yml 파일에서 설정한 배포설정의 저장소로 배포하게 된다. 1234hexo generate #hexo ghexo deploy #hexo d#동시에 하고 싶으면hexo deploy --generate #hexo d -g 생성된 리소스는 USERNAME.github.io 저장소에 배포되며 https://USERNAME.github.io로 접속하게되면 블로그를 확인할 수 있다. 주의 간혹 deploy가 정상적으로 되지 않는다면 아래 명령어로 clean 후 다시 배포를 해보자. 12hexo cleanhexo d -g 이상으로 기본적인 블로그 생성과 관련된 내용을 마무리 한다. 추가메모 12hexo new 포스트명 //기본설정값 draft 로 바꿔놓음hexo publish 포스트명 //draft -&gt; posts 로 이동 Reference Hexo로 github 블로그 만들기 (Hueman 테마) Github Page와 Hexo를 통해 30분만에 기술 블로그 만들기 Hexo 공식문서","link":"/2020/11/01/blog/start-blog/"},{"title":"카카오톡 오픈그래프 캐시 삭제 방법","text":"블로그 대표 이미지를 변경했음에도 카카오톡에 공유 시 기존 이미지가 계속 나오는 문제가 있었다. 사이트의 소스코드의 태그를 확인해봐도, 오픈그래프 적용을 확인해볼수 있는 페이스북의 Sharing Debugger 사이트에서도 확인했을 때 문제가 없었으니 카카오에서 이미지 캐시가 문제라고 생각되서 구글링을 해봤고 역시 삭제하는 방법을 찾을 수 있었다. 삭제방법 카카오 개발자 사이트의 초기화 도구에서 OG(Open Graph) 캐시로 들어간다. 개발자 계정이 없다면 카카오 계정으로 간단하게 가입하고 있으면 로그인 삭제하려는 URL을 입력하고 초기화 한다. 이제 다시 카카오톡에 링크를 공유해보면 바뀐 이미지가 나오는걸 확인할 수 있다. 참고로 open graph 라는건 html 메타 태그의 종류 중 하나로 사용자가 링크를 sns의 입력창에 입력하면 크롤러가 미리 그 웹사이트를 방문해서 HTML head의 오픈그래프 메타 데이터를 긁어온다. og:title, og:description, og:image 등 태그의 데이터를 바탕으로 미리보기 이미지와 설명을 만들어서 보여주게 된다.","link":"/2021/11/21/blog/ogimage-reset/"},{"title":"Google Tag Manager 간단 정리","text":"웹사이트 또는 모바일 앱에서 태그라고 통칭되는 추적 코드 및 관련 코드 조각을 쉽고 빠르게 업데이트할 수 있는 태그 관리 시스템 여기서 태그는 웹페이지의 HTML 태그를 말하는게 아니고 마케팅 업계에서 자바스크립트로 데이터를 수집해서 웹 분석 및 광고 성과 추적용도로 사용하기 위해 서비스 제공업체(구글,네이버,페이스북 등)로 전달하는 역할을 수행하는 기능을 말한다. 컨테이너 코드 스니펫(컨테이너 생성 시 헤더, 바디에 심으라고 하는 코드)이라는 기본 스크립트를 소스코드에 한번 추가하면 이후 개발자가 관여하지 않더라도 GTM에서 제공하는 UI를 통해 태그를 효율적으로 추가,삭제,변경 등 관리하기 위해 사용한다. 1234567&lt;!-- Google Tag Manager --&gt;&lt;script&gt;(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&amp;l='+l:'';j.async=true;j.src='https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-A123456');&lt;/script&gt;&lt;!-- End Google Tag Manager --&gt; 컨테이너 태그를 관리하는 프로젝트 단위로 일반적으로 도메인별로 생성하며 태그,트리거,변수 3가지 요소를 사용해서 데이터를 추적 변수(How) 특정 값을 담아두는 가상의 메모리 영역으로 어떠한 데이터를 수집할 지 지정하고 여러 이벤트에서 재사용하여 효율증가 ex) 특정 페이지에 접속할때, 클릭할때 데이터를 수집할 경우 사전 정의된 url이라는 변수는 현재 로드중인 페이지의 URL을 변수로 사용할 수 있다. 트리거 : 태그 실행 조건을 지정하는 필터를 정의하는 용도 (ex: url 변수가 ‘example.com/index.html’일 때 페이지뷰 트리거를 실행하는 용도) 태그 : 동적 값을 포착하여 전송하는 데 변수가 사용 (ex: 거래 금액과 구매 제품을 전환추적 태그에 전달하는 용도) 기본 제공하는 변수로 처리할 수 없는 특정 요구사항에 맞추고 싶다면 사용자 정의 변수를 생성할 수 있다. 트리거(When) 태그(명령어)가 실행되는 조건을 정의하여 조건 충족 시 연결된 태그가 실행 ex) 페이지뷰, 클릭할때 등 이벤트 실행 조건 태그(What) 데이터를 추적하여 수집하기 위해 명령하는 명령어의 역할로 트리거의 조건이 충족되면 태그가 인식하여 데이터를 추적하여 Google Analytics 등의 툴에서 데이터를 수집 쉽게 생각해서 트리거가 실행 조건을 담고 있고 태그는 실행 내용을 담고 있다고 생각하면 된다. 데이터 영역(dataLayer) 웹사이트에서 태그 관리자 컨테이너로 정보를 전달할 때 사용되는 자바스크립트 객체 웹사이트 --&gt; dataLayer &lt;–&gt; GTM --&gt; 구글 애널리틱스, 네이버광고 등 구글 애널리틱스 사용 시 전자상거래 구매 데이터, 맞춤 측정기준에서 사용하는 데이터 등은 기본 추적코드만으로는 수집할 수 없어 추가적인 설정이 필요하다. 이때 주로 웹페이지 내 별도의 추적 코드를 소스코드에 삽입하여 이들 정보를 GA서버로 직접 보내는 방식을 사용하는데 GTM에선 이런 정보를 수집할 때 데이터 영역을 주로 사용한다. 코드 스니펫을 header에 추가하고 123&lt;script&gt; window.dataLayer = window.dataLayer || [];&lt;/script&gt; dataLayer.push() 명령어를 사용하면 데이터 영역에 정보를 추가할 수 있다. 12345&lt;a href=&quot;#&quot; onclick=&quot;dataLayer.push({ 'bookCategory': 'fiction', 'bookTitle': 'Cien años de soledad', 'bookAuthor': 'Gabriel García Márquez'});&quot;&gt;도서 세부정보&lt;/a&gt; 주의사항으로 GTM은 컨테이너 스니펫 실행 시 자동으로 데이터영역을 생성하며 이미 있는 경우 그 안의 변수를 가져다 사용하기 때문에 데이터영역은 GTM 컨테이너 스니펫보다 앞에 위치해야 한다고 한다. 그 외 태그,트리커,변수 생성 과정에 대한 자세한 방법은 아래 링크 참고 구글-태그-관리자-설치-및-사용법 태그, 트리거, 변수에서 설정가능한 유형에 대해선 태그 관리자 고객센터에서 각 항목 참고 태그 관리자 고객센터","link":"/2021/12/13/etc/gtm-1/"},{"title":"Docker Desktop for window 경로 변경","text":"회사pc에서 docker를 로컬에 설치해서 사용해볼 일이 있어서 설치를 하려는데 보안 프로그램 때문에 C드라이브에 제대로 설치가 안됐다. D드라이브에 설치를 하고 싶었는데 Docker Desktop for window 설치파일에서 설치경로를 바꾸는 옵션을 제공하지 않고 있는데 🔗 https://github.com/docker/roadmap/issues/94 (최근까지도 기다리는 사람이 많은듯?) 도커포럼에서 해당 이슈에 대한 임시 해결책으로 링크를 생성해서 설치경로를 바꾸는 방법이 있었다. 🔗 https://forums.docker.com/t/docker-installation-directory/32773/11 docker 설치 전 cmd를 관리자모드로 실행하고 아래 명령어를 실행한다.(xxx = 본인 계정) 만약 이미 한번 설치를 해서 각 위치에 폴더가 이미 생성된 상태라 C드라이브 경로에 생성된 폴더들 다 지우고 명령어 실행하면 된다. (타겟 경로의 폴더들은 나중에 만들어줘도 됨) 1234mklink /j &quot;C:\\ProgramData\\Docker&quot; &quot;D:\\ProgramData\\Docker&quot;mklink /j &quot;C:\\ProgramData\\DockerDesktop&quot; &quot;D:\\ProgramData\\DockerDesktop&quot;mklink /j &quot;C:\\Program Files\\Docker&quot; &quot;D:\\Program Files\\Docker&quot;mklink /j &quot;C:\\Users\\xxx\\AppData\\Local\\Docker&quot; &quot;D:\\Users\\xxx\\AppData\\Local\\Docker&quot; 이후 installer를 실행해보면 로그에는 C드라이브에 설치하는거로 나오지만 실제 파일은 링크를 걸어놓은 D드라이브에 생성되는 모습을 볼 수 있다.","link":"/2022/01/08/docker/installation-directory/"},{"title":"함수형 프로그래밍 - 개요","text":"함수형~ 함수형~ 여러 곳에서 이야기는 종종 들었지만 제대로 찾아본 적이 없다보니 기본적인 개념부터 많이 부족해서 간단히 스터디를 시작했다. 특정 언어를 선정해서 언어적 특성에 종속되기 보단 우선 함수형 프로그래밍의 패러다임에 대해서 먼저 학습 해보자. 개요 함수형 프로그래밍은 크게 두 그룹으로 분류된다. 구분 지원범위 언어 순수 함수형 언어 오직 함수형 패러다임만 지원 Haskell 불순 함수형 언어 함수형 패러다임과 명령형 프로그래밍을 지원 LISP 그럼 여기서 명령형은 뭐고 함수형 패러다임은 뭘 말하는 걸까? 프로그래밍 패러다임은 크게 보면 2가지로 분류할 수 있다. 명령형 프로그래밍 : 프로그래밍의 상태와 상태를 변경시키는 구문의 관점에서 연산을 설명하여 기능을 구현하기 위한 알고리즘을 명시하지만 결국 무엇을 해야하는지는 명시하지 않는다. 절차지향 프로그래밍 : 수행되어야 할 연속적인 계산 과정을 포함 (C, C++) 객체지향 프로그래밍 : 객체들의 집합으로 프로그램의 상호작용을 표현 (C++, Java, C#) 선언형 프로그래밍 : How(어떻게) 보단 What(무엇을) 해야하는지 설명하는 방식으로 알고리즘에 대해서 명시하진 않고 목표를 명시한다. 함수형 프로그래밍 : 순수 함수를 조합하고 소프트웨어를 만드는 방식 (Clojure, Haskell, LISP) 명령형 프로그래밍은 어떻게 할 것이가(How)를 표현하고, 선언형 프로그래밍은 무엇을 할것인가(What)를 표현한다. 특성 함수형 프로그래밍은 아래와 같은 특징을 같는다. 계산을 수행하기 위해 조건식과 재귀를 사용하는 수학 함수의 개념에 따라 설계되었다. 고차함수high order function와 지연연산lazy evaluation 기능을 지원한다. 고차 함수 람다 계산법에서 만들어진 용어로 아래 조건을 만족하는 함수 함수에 함수를 파라미터로 전달할 수 있다. 함수의 반환값으로 함수를 사용할 수 있다. 고차 함수는 1급 함수의 부분집합이다. 지연 연산 불필요한 연산을 피하기 위해서 결과값이 필요한 시점까지 연산을 늦추는 것을 말한다. 값을 미리 계산해서 저장하지 않기 때문에 메모리의 효율적인 사용이 가능 루프문과 같은 흐름제어와 If-Else, Switch문과 같은 조건문을 지원하지 않고 함수와 함수호출을 직접 사용한다. OOP와 마찬가지로 추상화, 캡슐화, 상속, 다형성과 같은 개념을 지원한다. 장점 Bugs-Free Code : State를 지원하지 않으므로 부작용이 없어서no side-effect 오류없는 코드작성이 가능하다.(없다기 보단 그냥 적은게 맞을 것 같다.) 효율적인 병렬 프로그래밍 : 상태 변경이 없기 때문에 병렬로 작동하도록 기능에 대해서 프로그래밍할 수 있으며 이는 재사용 및 테스트를 더 쉽게 지원한다. 효율성 : 독립적인 유닛으로 구성되서 동시에 실행할 수 있다. 중첩 함수 지원 : 중첩함수를 지원한다. 지연 연산 : Lazy List, Lazy Map 등과 같은 지연 함수 구조를 지원한다. 단점 큰 메모리 공간이 필요하며 상태가 없기 때문에 작업을 수행할 때마다 새 객체를 만들어야한다. 함수형 vs 객체지향 함수형 OOP 불변 데이터 사용 가변 데이터 사용 선언적 프로그래밍 모델 명령형 프로그래밍 모델 무엇을 하는가에 초점 어떻게 하는가 에 초점 병렬 프로그래밍 지원 병렬 프로그래밍에 적합하지 않음 부작용이 없다 부작용이 발생할 수 있다. 함수 호출 및 재귀를 사용하여 흐름 제어 루프와 조건문을 사용하서 흐름 제어 재귀를 사용한 반복 루프를 사용한 반복 실행순서가 중요하지 않다. 실행 순서가 매우 중요하다. 데이터 추상화, 동작 추상화 지원 데이터 추상화만 지원 이상으로 함수형 프로그래밍에 대한 대략적인 개요에 대해서만 우선 정리해 보았다. Reference tutorialspoint - Lean Functional Programming 함수형 프로그래밍 요약 함수형 프로그래밍 언어에 대한 고찰","link":"/2020/12/07/functional/intro/"},{"title":"함수형 프로그래밍 - 재귀","text":"앞서 확인한 개요에서 언급했는데 함수형 프로그래밍에선 반복을 재귀를 통해서 구현한다고 했는데 재귀와 꼬리재귀에 대해서 간단히 알아보자. 재귀 함수 본문에서 자기자신을 호출하는 방식을 재귀호출(recursive call)이라고 부른다. 재귀는 다른 명령어가 방지할 때까지 계속된다. 예제 꼬리 재귀 최적화 in python 재귀호출의 경우 호출 스택의 깊이가 얕은 경우엔 큰 상관이 없으나 깊이가 깊어지면 오버플로우가 발생하는 문제가 있다. 여담으로 실행하는 시스템에 따라서 조금씩 다를수 있지만 파이썬에서 호출가능한 스택의 최대 깊이는 보통 1000 정도에서 RecursionError가 발생한다. 이를 해결하기 위한 방법으로 제시되는 해결책 중 하나가 꼬리 재귀Tail recursion이다. 간단히 말하자면 함수에서 마지막으로 호출하는 함수가 자기 자신이고, 재귀 호출의 값을 반환받은후 추가적인 연산이 필요하지 않는 방식을 말한다. 꼬리 재귀 적용 예제 위의 예제에서 사용한 팩토리얼 함수를 보자. fact(n)을 호출했을 때 연산이 끝나지 않았는데 fact(n-1)을 호출하기 때문에 리턴 주소를 저장하기 위해서 시스템 콜스택을 사용하게 된다. 즉, 현재 함수(fact(n))에서 결과값을 반환하기 위해서는 현재 함수의 인자 값(n)을 스택에 가지고 있다가 그 다음 호출될 함수(fact(n-1))의 결과 값과 함께 연산을 해야 한다는 점이다. 이러한 방식은 꼬리 재귀를 만족하지 못한다고 본다. 예제를 꼬리 재귀로 바꾸려면 어떻게 해야할까? 재귀를 호출하는 부분에서 추가적인 연산이 필요없도록 만들면 되는데 이를 구현하기 위해선 return에서는 (언어 스펙에서 지정한 스택에 메모리를 쌓지 않는 연산자를 제외한) 연산자를 사용하면 안된다. 연산자의 사용없이 재귀 호출의 반환값을 그대로 return 해주면 된다. 한가지 주의할 점은 개발자가 꼬리재귀 구조로 코드를 짜더라도 사용하는 언어의 스펙에 따라서 꼬리재귀 최적화 보장여부가 다르기 때문에 확인이 필요하다. 요즘 python을 공부하고 싶어서 위 예시를 python으로 들었지만 python은 꼬리재귀 최적화를 보장하지 않는데 python의 창시자 귀도 반 로섬의 의견은 다음과 같다. 귀도 반 로섬의 TRE(Tail Recursion Elimination)에 대한 반론 콜 스택을 추적하기에 부적합하다(디버깅이 어렵다) 단순 최적화기 때문에 개별 파이썬 컴파일러 구현체에서 선택하게 둘 것 재귀가 모든 것의 기반이라는 접근은 이상적인 수학적인 접근일 뿐이다 파이썬 스타일의 개발자들은 재귀 대신 멋진(?) 문법들을 쓸 수 있다 not PYTHONIC 하다 Reference tutorialspoint - Learn Functional Programming 재귀,반복, Tail Recursion Tail Recursion Elimination","link":"/2020/12/07/functional/recursion/"},{"title":"기술면접 준비(Java)","text":"내용은 기초적인 부분이지만 막상 면접 전에 한번이라도 정리하지 않으면 용어가 바로바로 안나오는 경우도 많기 때문에 간단하게나마 정리해 보았다. OOP란 객체지향 프로그래밍(Object-Oriented Programming, OOP)은 컴퓨터 프로그램을 명령어의 목록으로 보는 시각에서 벗어나 여러 개의 독립된 단위, 즉 &quot;객체&quot;들의 모임으로 파악하고자 하는 것 입니다. 각각의 객체는 메시지를 주고받고, 데이터를 처리할 수 있으며 대표적인 특징으로 추상화, 상속, 캡슐화, 다형성이 있습니다. 직렬화(serialization)란? 객체 또는 데이터를 스트림을 통해 입출력하려면 바이트(byte)형태로 변환하는 것이 필요한데, 이를 직렬화 라고 합니다. 반대로 스트림을 통해 받은 직렬화된 객체를 원래 모양으로 만드는 과정을 역직렬화라고 합니다. 박싱과 언박싱이란? 기본 자료형(Primitive data type)을 Wrapper class로 바꾸어 주는 것을 박싱, **Wrapper class를 기본 자료형(Primitive data type)**으로 바꿔주는 것을 언박싱 이라고 합니다. Static에 대해서 static 키워드를 쓰면 객체를 생성하지 않아도 static 변수나 static 함수를 사용할 수 있습니다. 인스턴스를 생성하면 각 인스턴스는 서로 독립적이기 때문에 서로 다른 값을 유지합니다. 각 인스턴스들이 공통적으로 같은 값이 유지되어야 하는 경우 static을 붙입니다. static 이 붙은 메서드 에서는 인스턴스 변수를 사용할 수 없습니다. 메서드 내에서 인스턴스 변수를 쓰지 않는다면 가능하면 static을 붙이는게 호출 시간이 짧아지기 때문에 효율이 높아집니다. 클래스 설계시 static 사용 지침 클래스의 멤버변수 중 모든 인스턴스에 공통된 값을 유지해야 하는 것이 있으면 static 을 사용합니다. 메서드 중에서 인스턴스 변수를 사용하지 않는 메서드는 static 을 사용하는 것을 고려합니다. 접근 제어자에 대해서 public : 어디서든 접근 가능 protected : 동일 패키지 혹은 상속받은 외부 패키지 클래스에서 사용 (default) : 동일 패키지 내에서만 접근 가능 private : 해당 클래스 내에서만 접근 가능 String vs StringBuffer vs StringBuilder String immutable(불변) 객체를 한 번 할당할시 메모리 공간에 변동이 없습니다. 동기화를 신경쓰지 않아도 됩니다. 엄청나게 많은 문자열을 선언 및 연산할 시 성능저하를 고려해야합니다. StringBuffer mutable(가변) 멀티스레드 환경에서도 동기화를 지원(Thread-safe) StringBuilder mutable(가변) 동기화를 지원하지 않습니다. 정리하면 적은 양의 문자열의 선언 후 연산이 필요없다면 String을 사용하고 문자열의 선언이 많이 필요하거나 연산이 필요한데 싱글스레드 환경이라면 StringBuilder를, 멀티스레드 환경이라면 StringBuffer를 사용합니다. Java8 특징은? java에 함수형 프로그래밍이 처음으로 도입된 버전으로 새로 도입된 주요 기능으론Lamda 표현식, Stream API, Optional Class 같은 게 있습니다. 람다(Lamda) 표현식 익명함수로 이름과 식별자가 없는 함수를 말합니다. 불필요한 코드를 줄이고 가독성을 향상시키기 위함입니다. 12List&lt;Integer&gt; list = new ArrayList&lt;&gt;();list.forEach(num -&gt; System.out.println(&quot;number : &quot; + num)); StreamAPI 람다식을 적용해서 컬렉션같은 데이터 처리연산을 하기 위한 api로 멀티 스레드를 활용해서 병렬로 연산을 수행할 수 있고, 원본 데이터를 변경하지 않는 특징이 있고 코드가 매우 간단해진다는 것을 알 수 있습니다. Optional Class util 패키지에 속하며 NullPointException을 관리하기 위해 null이 될 수 있는 객체를 감싸고 있는 래퍼 클래스입니다. 명시적으로 해당 변수가 null일 수 있음을 표현하면서 null 체크를 직접하지 않아도 됩니다. 아래는 일반적으로 많이 사용되는 optional 예제코드 1234@Repositorypublic interface UserRepository extends JpaRepository&lt;UserEntity, Long&gt; { Optional&lt;UserEntity&gt; findByName(String Name);} 123String name = &quot;jay&quot;;UserEntity optUserEntity = userRepository.findByName(name) .orElseThrow(() -&gt; new EmptyDataException(&quot;해당 이름을 가진 유저가 없습니다.&quot;)); try-with-resource란? try-with-resources는 java7 이후 추가되었으며 자동으로 자원을 해제해주는 기능입니다. try 구문에 AutoCloseable 인터페이스를 구현하는 객체를 리소스로 선언하고 사용이 끝나면 자동으로 close 해주는 기능입니다. 123456try (Scanner scanner = new Scanner(new File(&quot;newFile.txt&quot;))){ System.out.println(scanner.nextLine());} catch (FileNotFoundException e) { e.printStackTrace();} 오버로딩 오버라이딩 차이점은? 자바에서 다형성을 지원하는 방법으로 오버로딩과 오버라이딩이 있습니다. (다형성(polymorphism) : 하나의 객체가 여러타입을 가지는 것) 오버로딩(Overloading)은 같은 이름의 메소드를 여러 개 가지면서 매개변수의 유형과 개수가 다르도록 작성하여 다양한 유형의 호출에 응답하도록 하는 것입니다. 프로그램의 가독성 ⬆ 오버라이딩(Overriding)은 상위클래스가 가진 메소드를 자식클래스에서 재정의 하여 사용하는 것을 말합니다. 하나의 코드로 여러 객체를 처리할 수 있는 장점이 있어서 사용 명칭 메소드명 매개변수 리턴타입 오버로딩 같음 다름 같아도되고 달라도됨 오버라이딩 같음 같음 같아야함 HashMap, LinkedHashMap, TreeMap 비교 HashMap 내부적으로 Entry의 array로 되어있고 해당 array에 index는 내부 해쉬 함수를 통해 계산합니다. hashing을 사용하기 때문에 많은 양의 데이터를 검색하는데 뛰어난 성능을 가지고 있지만 내부 해시값에 따라서 키순서가 정해져서 특정 순서나 규칙없이 출력됩니다. LinkedHashMap 내부적으로 LinkedList 형태로 저장되며 입력된 데이터의 순서를 유지할 수 있습니다. HashMap과 기본적으로 기능은 동일하지만 순서를 유지하는 기능이 추가되면서 메모리 사용량이 더 높습니다. TreeMap Key값에 따라서 자동으로 Sort가 되는 방식이며 동기화(synchronized) 처리가 되어있어 Thread-safe 합니다. HashMap과 다르게 키값으로 null을 허용하지 않고 내부적으로 red black tree 구조로 데이터를 저장합니다. red black tree 는 기존의 이진트리의 각 노드에 색깔을 저장하는 공간을 추가해서 색깔을 기준으로 균형을 맞추는 트리입니다. 키값에 대한 Compartor(콤퍼레이터) 구현으로 정렬 순서를 변경할 수 있습니다. 정리하자면 특별한 사유가 없다면 검색성능이 가장 좋은 HashMap 사용 순서를 보장하고 싶다면 LinkedHashMap 사용 키값을 일정하게 iterate 하고자한다면 TreeMap 사용 interface vs abstract 공통점 선언만 있고 구현 내용이 없습니다. 스스로 객체를 생성할 수 없지만 추상클래스를 extends 받거나, interface를 implements한 자식들만 객체를 생성할 수 있습니다. interface(인터페이스) 다중 상속 가능 추상 메서드, 상수만 선언 가능(java8부터 default메서드 사용 가능) 생성자, 일반 변수를 가질 수 없음 서로 관련성이 없는 클래스들이 인터페이스를 구현하게 되는 경우 특정 데이터 타입의 행동을 명시하고 싶은데, 어디서 구현되든지 상관없을 때 다중상속을 허용하고 싶을 때 abstract(추상 클래스) 다중 상속 불가 추상 메서드 1개 이상, 일반 변수, 일반 메서드 선언 가능 생성자, 일반 변수 가질 수 있음 관련성이 높은 클래스간에 코드를 공유하고 싶은 경우 상속받은 클래스들이 공통적으로 가지는 메서드와 필드가 많거나, public 이외의 접근제어자 사용이 필요한 경우 사용할 수 있습니다. 자바 메모리 구조 메서드(Method) 영역 클래스 변수의 이름, 타입, 접근 제어자 등과 같은 클래스와 관련된 정보를 저장하며 static 변수, 전역변수 등이 저장된다. 코드에서 사용되는 클래스들을 로더가 읽고 클래스 별로 분류해서 저장합니다. 스택(Stack) 영역 메서드가 실행되면 스택 영역에 메소드에 대한 영역이 1개 생기며 여기에 지역변수, 매개변수, 리턴값 등이 저장된다. LIFO(Last In First Out) 방식의 메모리입니다. 힙(Heap) 영역 new 연산자를 통해 생성된 객체와 배열의 인스턴스를 저장되고, 메모리는 가비지 컬렉터에 의해 관리됩니다.","link":"/2021/08/21/interview/technical-1/"},{"title":"Git-flow의 활용","text":"회사에서 내부서버에 Gitlab을 활용하여 Git Server를 구축하여 형상관리를 하고 있는데 특별한 관리나 체계없이 브랜치 관리를 하다보니 히스토리 파악하기도 힘들기도 하고 형상관리를 제대로 하고 있다는 느낌이 들지 않아 효율적인 형상관리 시스템의 사용을 위하여 방법을 찾던 중 우아한형제들 기술블로그의 “우린 Git-flow를 사용하고 있어요” 라는 글을 보고 해당 전략을 도입하면서 각 브랜치에 대해 간략하게 정리를 해보았다. Git-flow에는 5가지 종류의 브랜치가 존재한다. 항상 유지되는 메인 브랜치들(master, develop)과 일정 기간 동안만 유지되는 보조 브랜치들(feature, release, hotfix)이 있다. master : 제품으로 출시될 수 있는 브랜치 develop : 다음 출시 버전을 개발하는 브랜치 feature : 기능을 개발하는 브랜치 release : 이번 출시 버전을 준비하는 브랜치 hotfix : 출시 버전에서 발생한 버그를 수정 하는 브랜치 가장 중심이 되는 브랜치는 master랑 develop 브랜치이며, 이 두 개 브랜치는 무조건 있어야 한다. 이름은 바뀔 수 있다만 웬만해서는 변경하지 않고 진행하도록 하자. Git도 Production에서 사용하는 브랜치는 master를 사용하게 되니 관련된 부분을 변경하면 새로운 사람이 왔을때 스터디 커브가 존재할 수 있다. 병합된 feature, release, hotfix 브랜치는 삭제하도록 한다. (클라이언트 툴에서 git flow제공한다면 merge 하면 삭제하는 옵션을 제공한다.) Feature 브랜치 브랜치 나오는 곳 : develop 브랜치가 들어가는 곳 : develop 이름 지정 : master, develop, release-*, hotfix-*를 제외한 어떤 것이든 가능. 새로운 기능을 추가하는 브랜치이다.feature브랜치는 origin에는 반영하지 않고, 개발자의 reop애만 존재하도록 한다. 여기서 머지를 할 때, --no-ff 옵션을 이용하여 브랜치에서 머지가 되었음을 git 기록에 남겨두도록 한다. Release 브랜치 브랜치 나오는 곳 : develop 브랜치가 들어가는 곳 : develop, master 이름 지정 : release-* 새로운 Production 릴리즈를 위한 브랜치이다.지금까지 한 기능을 묶어 develop 브랜치에서 release 브랜치를 따내고, develop 브랜치에서는 다음번 릴리즈에서 사용할 기능을 추가한다.release 브랜치에서는 버그 픽스에 대한 부분만 커밋하고, 릴리즈가 준비되었다고 생각하면 master로 머지를 진행한다. (이때도 --no-ff 옵션을 이용하여 머지하였음을 남긴다.)master로 머지 후 tag 명령을 이용하여 릴리즈 버전에 대해 명시를 하고, -s 나 -u &lt;key&gt; 옵션을 이용하여 머지한 사람의 정보를 남겨두도록 한다. 그런 뒤 develop 브랜치로 머지하여, release 브랜치에서 수정된 내용이 develop 브랜치에 반영한다. Hotfix 브랜치 브랜치 나오는 곳 : master 브랜치가 들어가는 곳 : develop, master 이름 지정 : hotfix-* Production에서 발생한 버그들은 전부 여기로… 수정 끝나면, develop, master 브랜치에 반영하고, master에 다가는 tag 를 추가해준다.만약 release 브랜치가 존재한다면, release 브랜치에 hotfix 브랜치를 머지하여 릴리즈 될 때 반영이 될 수 있도록 한다. Reference 우아한형제들 기술블로그의 “우린 Git-flow를 사용하고 있어요”","link":"/2020/11/02/git/git-flow/"},{"title":"Spring Framework란 무엇인가","text":"기초가 부족한 관계로 기초부터 다시 정리도 하는 겸사겸사 첫주제는 spring framework로 정했다. Srping Framework란 스프링 프레임워크(Spring Framework)는 자바 플랫폼을 위한 오픈 소스 애플리케이션 프레임워크로 간단히 스프링(Spring)이라고도 한다. 동적인 웹 사이트를 개발하기 위한 여러 가지 서비스를 제공하고 있다. 대한민국 공공기관의 웹 서비스 개발 시 사용을 권장하고 있는 전자정부 표준프레임우커의 기반 기술로서 쓰이고 있다. 특징 크기와 부하의 측면에서 경량 컨테이너로서 자바 객체를 직접관리한다. 객체 생성, 소멸과 같은 라이프 사이클을 관리하며 스프링으로부터 필요한 객체를 얻어올 수 있다 제어 반전 혹은 제어 역행(IoC: Inversion of Control)을 지원한다. 컨트롤의 제어권이 사용자가 아닌 프레임워크에 있어서 필요에 따라 스프링에서 사용자의 코드를 호출한다. IoC는 DI와 DL에 의해 구현된다. DL(Dependency Lookup) : 의존성 검색 컨테이너에서는 객체들을 관리하기 위해 별도의 저장소에 빈을 저장하는데 저장소에 저장되어 있는 개발자들이 컨테이너에서 제공하는 API를 이용하여 사용하고자 하는 빈을 검색하는 방법 DI(Dependency Injection) : 의존성 주입 의존성 주입이란 객체가 서로 의존하는 관계가 되게 의존성을 주입하는 것으로 객체지향 프로그램에서 의존성이란 하나의 객체가 어떠한 다른 객체를 사용하는 것을 의미한다. IoC에선 각 클래스 사이에 필요로 하는 의존관계를 빈 설정 정보를 바탕으로 컨테이너가 자동으로 연결해 주는 것을 말한다. POJO(Plain Old Java Object) 방식의 프레임워크이다. 직역하면 오래된 방식의 간단한 자바 오프젝트라는 말이다. 일반적인 J2EE 프레임워크에 비해 구현을 위하여 특정한 인터페이스를 구현하거나 상속을 받을 필요가 없어 기존에 존재하는 라이브러리 등을 지원하기에 용이하고 객체가 가볍다. 쉽게 이야기하면 getter/setter 메소드로 이루어진 Java Benas를 생각하면 된다. 예를 들어 자바 서블릿 코드를 작성할 때는 보통 HttpServlet을 상속받아야 한다. 123public TestServlet extends HttpServlet { ...} 이처럼 서블릿 프로그래밍을 하는 것만으로 객체지향 프로그래밍의 가장 핵심적인 기능 중 하나인 상속을 할 수 없고 HttpServlet에서 제공하는 기능을 어떻게 재사용할 것인지 판단해야하는 부분도 생겼다. POJO는 이러한 제약이 없는 일반적인 객체를 말하는데 여기서 상속이나 인터페이스 구현을 사용하지 않는 객체가 아니라 어떠한 라이브러리나 프레임워크 등 자바 언어 사양 외에 어떠한 제한을 강제받지 않는 자바 오브젝트를 뜻하는 것이다. 관점 지향 프로그래밍(AOP: Aspect Oriented Programming)을 지원한다. 기존의 객체지향 프로그래밍(OOP: Object Oriented Programming) 에서는 객체의 재사용으로 인해 반복되는 코드의 양을 줄일 수 있었지만 여전히 많은 부분에서 중복된 코드가 발생한다. 예를 들어 로그, 권한 체크, 인증, 예외 처리와 같은 소스상에서 반복될 수 밖에 없는 필수적인 요소들로 코드의 가독성이나 유지보수적인 측면에서 좋지 않았다. AOP는 OOP를 대체하는 개념이 아닌 OOP를 좀 더 OOP처럼 사용하기 위하여 보완하는 개념으로 공통적으로 반드시 필요하지만 중복해서 작성해야하는 핵심 이외의 코드들을 외부로 분리하여 관리한다. 이렇게 외부에서 관리하는 공통기능을 핵심 로직에 영향을 끼치지 않게 잘 끼워넣어 개발하면 무분별하게 중복되는 코드를 제거하면서 공통기능의 수정을 통해 모든 핵심 로직의 공통기능을 수정하여 효율적인 유지보수가 가능해지면서 재활용성이 극대화된다. 즉, 위에서 예를 들었던 트랜잭션이나 로깅, 보안과 같이 여러 모듈에서 공통적으로 사용하는 기능의 경우 분리하여 관리할 수 있다는 것이다. MVC (Model2) MVC는 Model, View, Controller 를 뜻하며 사용자 인터페이스와 비즈니스 로직을 분리하여 개발하는 것으로 웹 프로그래밍 개발에선 거의 표준처럼 사용되고 있으며 일반적으로 Model2를 지칭한다. Model은 데이터를 처리하는 영역 View는 렌더링되서 실제로 보이는 화면 Controller는 사용자의 요청을 받고, 응답을 주는 로직을 담당 이처럼 소스를 분리하여 각 소스의 목적을 명확히하면 모듈화를 통해 재사용성을 늘리고 유지보수를 쉽게 할 수 있으며 확정성도 좋은 장점이 있다. 간단한 흐름을 살펴보면 요청 -&gt; 컨트롤러 -&gt; 모델 -&gt; 컨트롤러 -&gt; 뷰 의 흐름이라고 생각하면 된다. (MVC와 관련된 자세한 내용은 추후 기회가 된다면 별도로 포스트하겠다.) 이상으로 스프링 프레임워크의 특징에 대하여 대략적으로 정리를 해보았다. Reference 위키백과 POJO(Plain Old Java Object) 란? Spring 개념 정리","link":"/2020/11/03/spring/spring-1/"},{"title":"log4j2 설정","text":"사내 프로젝트의 여러가지 설정, 프로퍼티들은 dev, test, master로 분리되어있는데 살펴보다보니 log4j2-test.xml이 없고 test 프로파일로 빌드 시 log4j2-dev.xml을 바라보고 있었다. 시간도 좀 여유가 있던 시점이라 그냥 내가 만들어서 분리시켰는데 하는김에 SQL 관련 로그 설정을 변경했고 공부도 할겸 내용을 정리했다. 123456789101112131415161718192021222324252627282930&lt;!-- SQL Logger --&gt;&lt;logger name=&quot;jdbc.connection&quot; additivity=&quot;false&quot;&gt; &lt;level value=&quot;OFF&quot; /&gt; &lt;appender-ref ref=&quot;console&quot;/&gt; &lt;/logger&gt;&lt;logger name=&quot;jdbc.audit&quot; additivity=&quot;false&quot;&gt; &lt;level value=&quot;OFF&quot;/&gt; &lt;appender-ref ref=&quot;console&quot;/&gt; &lt;/logger&gt; &lt;logger name=&quot;jdbc.resultset&quot; additivity=&quot;false&quot;&gt; &lt;level value=&quot;OFF&quot; /&gt; &lt;appender-ref ref=&quot;console&quot;/&gt; &lt;/logger&gt;&lt;logger name=&quot;jdbc.sqlonly&quot; additivity=&quot;false&quot;&gt; &lt;level value=&quot;OFF&quot;/&gt; &lt;appender-ref ref=&quot;console&quot;/&gt; &lt;/logger&gt; &lt;logger name=&quot;jdbc.resultsettable&quot; additivity=&quot;false&quot;&gt; &lt;level value=&quot;info&quot;/&gt; &lt;appender-ref ref=&quot;console&quot;/&gt; &lt;/logger&gt; &lt;logger name=&quot;jdbc.sqltiming&quot; additivity=&quot;false&quot;&gt; &lt;level value=&quot;info&quot; /&gt; &lt;appender-ref ref=&quot;console&quot;/&gt; &lt;/logger&gt; 키 내용 jdbc.connection 열려있는 모든 번호와 연결 수립 및 해제 이벤트를 기록. 이는 연결 문제를 찾아내는데 매우 유용 (Connection Pool 설정) jdbc.audit ResultSet을 제외한 모든 JDBC 호출 정보를 로그로 남긴다. 많은 양의 로그가 생성되므로 특별히 JDBC 문제를 추적해야 할 필요가 있는 경우를 제외하고는 사용을 권장하지 않는다. jdbc.resultset ResultSet을 포함한 모든 JDBC 호출 정보를 로그로 남기므로 매우 방대한 양의 로그가 생성된다. jdbc.sqlonly SQL문만을 로그로 남기며, PreparedStatement일 경우 관련된 argument 값으로 대체된 SQL문이 보여진다. (원래는 ? 로 표시되는데, log4j 덕분에 완전한 쿼리로 보여지는 것임) jdbc.resultsettable SQL 결과 조회된 데이터의 table을 로그로 남긴다. jdbc.sqltiming SQL문과 해당 SQL을 실행시키는데 수행된 시간 정보(milliseconds)를 포함한다. SQL관련 sqlonly와 sqltiming은 sql로그를 찍는 공통적인 기능을 하며 sqltiming이 sqlonly를 포함하고 있습니다. #{id} 등 파라미터로 넣어준 값도 모두 치환하여 출력해줍니다. 둘다 찍도록 하면 sql문이 두번 찍히니 하나만 찍어줍니다. jdbc.sqlonly : SQL문을 로그로 남깁니다. jdbc.sqltiming : SQL문과 수행된 시간을 로그로 남깁니다. TABLE관련 결과를 table형식으로 만들어 출력해줍니다. jdbc.resultsettable : SQL 결과 조회된 데이터를 table 형식으로 만들어줍니다. JDBC호출 관련 아래 두개의 설정은 일반적으로는 킬 필요가 없는 설정입니다. 두개가 묶음으로 JDBC 호출정보를 출력해주는 기능을 하며 ResultSet을 포함하느냐 포함하지 않느냐의 차이입니다. jdbc.audit : ResultSet을 제외한 JDBC 호출 정보를 로그로 남긴다. jdbc.resultset : ResultSet을 포함한JDBC 호출 정보를 로그로 남깁니다. additivity : 중복된 로그를 남길 지 여부를 설정한다. 기본적으로 logger는 정의한 패키지의 조상으로부터 모든 appender를 상속받아서 만약 어떤 Logger가 같은 appender를 사용한다면 메시지가 2번 찍히게 된다. additivity=”false” 설정하면 이를 방지할 수 있다. Log Level 로그 레벨은 다음과 같다. (OFF &gt; FATAL &gt; ERROR &gt; WARN &gt; INFO &gt; DEBUG &gt; TRACE &gt; ALL) 레벨 범위 OFF 이벤트를 기록하지 않음 FATAL 아주 심각한 에러가 발생한 상태를 나타냄. 시스템적으로 심각한 문제가 발생해서 어플리케이션 작동이 불가능할 경우가 해당하는데, 일반적으로는 어플리케이션에서는 사용할 일이 없음. ERROR 요청을 처리하는중 문제가 발생한 상태를 나타냄. WARN 처리 가능한 문제이지만, 향후 시스템 에러의 원인이 될 수 있는 경고성 메시지를 나타냄. INFO 로그인, 상태변경과 같은 정보성 메시지를 나타냄. DEBUG 개발시 디버그 용도로 사용한 메시지를 나타냄. TRACE 디버그 레벨이 너무 광범위한 것을 해결하기 위해서 좀더 상세한 상태를 나타냄. ALL 모든 이벤트를 기록함","link":"/2023/04/17/spring/log4j-1/"},{"title":"JWT 개요 간단 정리","text":"1. JWT(Json Web Token)란 JSON 객체를 사용해서 토큰 자체에 정보들을 저장하고 있는 Web Token 으로 가벼운 인증으로 사용이 쉽다. 일반적으로 클라이언트와 서버, 서비스와 서비스 사이 통신을 할 때 권한 인가(Authorization)을 위해서 사용된다. 2. 구조 Header : Signature를 해싱하기 위한 알고리즘 정보 1234{ &quot;alg&quot; : &quot;HS256&quot;, &quot;typ&quot; : &quot;JWT&quot;} Payload : 서버와 클라이언트가 주고받는, 시스템에서 실제로 사용될 정보에 대한 내용들 1234{ &quot;name&quot; : &quot;jy&quot;, &quot;iat&quot; : 1422779638} Signature : 토큰의 유효성 검증을 위한 문자열로 헤더와 페이로드를 합친 문자열을 서명한 값이다. 헤더의 alg에 정의된 알고리즘과 secret을 이용해 해싱하고 이 값을 다시 base64 인코딩하여 생성한다. 12345HMAC-SHA256( secret, base64urlEncoding(header) + '.' + base64urlEncoding(payload)) 이 세 부분은 각각 Base64 인코딩을 사용하여 점을 사용해서 연결되면 JWT가 완성되며 주로 HTTP 통신 시 Authorization key의 value 로 사용된다. 1234{ //Header Payload Signature &quot;Authorization&quot;: &quot;Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJuYW1lIjoiankiLCJpYXQiOjE0MjI3Nzk2Mzh9.SNKm-Pcut8DUMBmeQXdIJlM_wFkh4jYd5YtRT369JdI&quot;,} 3. 장점과 단점 장점 중앙의 인증서버, 데이터 스토어에 대한 의존성이 없어서 시스템의 수평확장에 유리하다. Base64 URL Safe Encoding을 사용하여 URL, Cookie, Header 어디에서든 모두 사용 가능하다. 단점 Payload의 정보가 많아지면 네트워크 사용량이 증가하여 데이터 설계 고려 필요 토큰이 클라이언트에 저장되서 서버에서 클라이언트의 토큰을 조작할 수 없다.","link":"/2021/04/23/web/jwt-1/"},{"title":"2. 스프링을 이용한 리액티브 프로그래밍 - 기본 개념","text":"관찰자(Observer) 패턴 이벤트를 발생시키는 역할(주체Subject), 이벤트를 수신하는 역할(객체, 즉 관찰자Observer)의 두가지 핵심 요소가 존재 Observer는 Subject에 등록되고 Subject로부터 알림을 수신 Java의 내장 클래스인 Observable, Observer는 java9 부터 deprecated 되었다. interface가 아니라 class로 구현되어 있어서 이미 다른 클래스를 상속받은 클래스가 Observable을 상속할 수 없어서 재사용성에 제약이 생긴다. Observable의 핵심 메소드 중 하나인 setChanged() 메소드가 protected로 정의되어 있어서 사용하려면 상속받은 서브클래스만 해당 메소드를 호출할 수 있다. Observable의 알림은 순서를 보장할 수 없고 상태 변경 역시 1:1로 일치하지 않아서 멀티 스레드 환경에서 thread-safe 하지 않다. Serializable을 구현하지 않기 때문에 Observable을 상속받은 서브클래스도 직렬화할 수 없다. 발행-구독 패턴 스프링 프레임워크는 이벤트처리를 위해 @EventListener어노테이션과 이벤트 발행을 위한 ApplicationEventPublisher클래스를 제공한다. 관찰자 패턴과의 차이점은 게시자와 구독자 사이에 간접적인 이벤트 채널(=메시지 브로커 or 이벤트 버스)을 제공하여 구독자는 이벤트 채널은 알고 있지만 게시자가 누구인지는 신경쓰지 않는다. SseEmitter를 사용하면 스프링 프레임워크를 브로커를 사용하여 발행-구독 패턴을 구현할 수 있다. 다만 로직을 구현함에 있어 스프링의 내부 메커니즘을 사용했고 이는 프레임워크의 변경으로 인해 프로그램의 안정성을 보장할 수 없는 단점이 있다. 리액티브 프레임워크 RxJava 자바 플랫폼에서 리액티브 프로그래밍을 위한 표준 라이브러리는 RxJava 1.x 였고 현재(2021 기준)는 2.x를 지나 3.x까지 출시되었다. RxJava 라이브러리는 **Reactive Extensions(혹은 ReactiveX)**의 자바 구현체로 종종 관찰자 패턴, 반복자 패턴 및 함수형 프로그래밍의 조합으로 정의된다. RxJava의 기본적인 Observer 인터페이스는 아래와 같이 설계할 수 있다. 12345public interface RxObserver&lt;T&gt; { void onNext(T next); void onComplete(); void onError(Exception e);} RxJava에서 아래와 같이 주기적으로 비동기 이벤트 시퀀스를 생성할 경우 이벤트가 생성되는 것과 별개의 스레드에서 사용되기 때문에 메인 스레드가 종료되지 않도록 sleep()을 쓰거나 다른 방법으로 종료를 지연시킬 수 있다. 123Observable.interval(1, TimeUnit.SECONDS) .subscribe(e -&gt; System.out.println(&quot;Received: &quot; +e)); Thread.sleep(5000); 마블 다이어그램 RxJava는 연산자를 통해 스트림의 원소를 조정하거나 구조 자체를 변경할 수 있다. 연산자가 복잡한 변환을 수행할 경우 이를 시각적으로 표현하여 그 동작을 효과적으로 설명하기 위한 목적으로 마블 다이어그램(marble diagram)이 발명됐다. 위아래 실선(ㅡ&gt;) : Observable의 시간흐름(Timeline)을 의미한다. 각 도형(○,□) : Observable에서 발행하는 데이터로 발행될때마다 onNext 메서드가 호출된다. 파이프(|) : 데이터 발행을 모두 완료했다는 의미로 onCompleted 메서드가 호출된다. 위에서 아래로 점선(—&gt;) : 함수의 입력,출력을 의미한다. 가운데 박스 : 함수를 의미하며 입력된 값에 어떤 변환작업을 하는지 나타내고 있다. 엑스(X) : 함수에서 입력된 값을 처리하는 중 에러가 발생하거나 비정상적으로 종료되었음을 의미하며 onError 메서드가 호출된다. RxJava와 관련된 모든 연산자는 이러한 마블다이어그램으로 표현되고 있으니 익숙해질 필요가 있다.","link":"/2021/11/28/book/spring5-reactive/2/"},{"title":"1. 왜 리액티브 스프링인가?","text":"리액티브(반응형)이 필요한 이유 증가하는 요청, 부하에 대해서 높은 응답성을 유지해야 한다. 메시지 기반 통신을 바탕으로 탄력성과 복원력을 확보하하여 높은 응답성을 추구한다. 탄력성 자원을 비례적으로 추가하거나 제거하여 시스템의 처리량의 자동으로 증감 하는 것 복원력 시스템의 실패에도 반응성을 유지하는 것 시스템의 기능 요소를 격리해 모든 내부 장애를 격리하고 독립성을 확보함으로써 달성 메시지 기반 통신(Message-driven) 제한된 리소스의 활용도를 높이기 위해서 비동기 논블로킹 모델을 사용해야 한다. 메시지 브로커를 사용하면 대기열을 모니터링하여 시스템의 부하관리 및 탄력성을 제어할 수 있다. 리액티브 선언문(https://www.reactivemanifesto.org/ko/glossary)","link":"/2021/11/28/book/spring5-reactive/1/"},{"title":"4-2. 리액터 프로젝트 심화학습","text":"리액티브 스트림의 수명 주기 조립(assembling) 단계 처리 흐름에서 사용하는 연산자를 조합한 빌더 API처럼 보이지만 일반적인 빌더 패턴과 달리 리액터 API는 불변성(Immutability)을 제공한다.(적용된 각각의 연산자가 새로운 객체를 생성한다.) 스트림 구성을 조작하고 더나은 스트림 전달을 위한 다양한 기술을 적용할 수 있는 단계 구독 단계 특정 Publisher를 구독할 때 발생 조립 단계에서 일련의 Publisher 체인이 연결되었고 최상위 래퍼를 구독하면 해당 체인에 대한 구독 프로세스가 시작된다. 조립단계와 동일한 최적화를 수행할 수 있다. 리액터에서 멀티 스레딩을 지원하는 일부 연산자는 구독이 발생하는 작업자를 변경할 수 있다. 런타임 단계 게시자와 구독자 간에 실제 신호가 교환되는 단계 교환하는 처음 두 신호는 onSubscribe, request onSubscribe 메서드는 최상위 소스에서 호출 구독이 모든 구독자 체인을 통과하여 마지막 구독자가 구독 체인에 대한 정보를 수신하고 메시지 수신을 시작하려면 Subscription#request 메서드를 호출해 전송을 시작해야 한다. 런타임 중에도 request를 줄이기 위한 최적화를 적용할 수 있다. 리액터에서 스레드 스케줄링 모델 멀티스레딩 실행을 위해 제공하는 연산자 사이의 차이점에 대해서 확인 다른 워커로 실행을 전환할 수 있는 네 가지 연산자 publishOn 연산자 런타임 실행의 일부를 지정된 워커로 이동 Scheduler 인터페이스를 사용하여 현재 스트림에 대한 특정 워커를 선택할 수 있다. 내부적으로 전용 워커가 메시지를 하나씩 처리할 수 있도록 새로운 원소를 제공하는 큐를 가지고 있다. 리액티브 스트림의 모든 원소는 하나씩(동시에는 아니지만) 처리되므로 항상 모든 이벤트에 순서를 엄격하게 정의할 수 있다.(이 속성을 **직렬성(serializability)**라고 한다.) 병렬 처리를 할 수 없다는 말처럼 들리지만 병렬 처리도 가능한데 예를 들어 처리 단계 사이에 비동기 영역을 추가해서 독립적으로 작업해 비동기 처리를 할 수 있다. subscribeOn 연산자 구독체인에서 워커의 작업 위치를 변경 보통 호출 시점에서 상위 스트림에 해당하는 부분의 스레드를 설정 parallel 연산자 하위 스트림에 대한 플로 분할과 분할된 플로 간 균형 조정 역할 123456Flux.range(0, 10000) .parallel() .runOn(Schedulers.parallel()) .map() .filter() .subscribe() parallel연산자를 사용하면 ParallelFlux를 동작시킨다. 다수의 Flux를 추상화하여 Flux간에 데이터의 크기 균형을 이룬다. Scheduler Scheduler.schedule : Runnable 작업을 예약가능 Scheduler.createWorker : 동일한 방법으로 Runnable 작업 예약이 가능한 Worker 인터페이스의 인스턴스를 제공 Scheduler인터페이스 / Workder인터페이스의 차이점 : 워커 풀 / Thread 또는 리소스를 추상화한 것 리액터에서 제공하는 스케줄러 인터페이스의 3가지 주요 구현체 SingleScheduler : 모든 작업을 한 개의 전용 워커에 예약가능, 시간에 의존적 ParallelScheduler : 고정된 크기의 작업자 풀에서 작동(CPU 코어 수로 기본크기 제한) ElasticScheduler : 동적으로 작업자를 만들고 스레드 풀을 캐시, 생성된 스레드 풀의 최대 개수는 제한되지 않음 리액터 컨텍스트 Context는 스트림을 따라 전달되는 인터페이스 런타임 단계에서 필요한 컨텍스트 정보에 엑세스할 수 있도록 하는 것 멀티스레드 환경의 비동기 처리방식에서 ThreadLocal가 가지는 한계를 해결할 수 있다. 변수에 데이터를 넣은 후 publishOn 등을 통해 다른 워커에서 작업 플로를 수행하면 데이터를 쌓은 스레드와 작업 스레드가 달라서 데이터에 접근할 수 없다.","link":"/2021/12/03/book/spring5-reactive/4-2/"},{"title":"3. 스트림의 새로운 표준 - 리액티브 스트림","text":"API 불일치 문제 CompletableStage를 이용하는 자바 코어 라이브러리와 RxJava 같은 다양한 라이브러리가 있어서, 코드를 작성할 때 다양한 선택을 할 수 있지만 과도하게 많은 선택지는 시스템을 지나치게 복잡하게 만들 수 있다. 핵심적인 문제는 라이브러리 공급자가 일관된 API를 만들어낼 수 있는 표준화된 방법이 없다는 사실이다. 풀 방식과 푸시 방식 리액티브 초기 단계에서 모든 라이브러리의 데이터 흐름은 소스에서 구독자에게 푸시되는 방식이었다. 풀 방식으로 요소를 하나씩 요청할 경우 비동기 논블로킹 방식을 사용하더라도 각 요소에 대한 요청을 처리 하면서 대기시간이 발생하여 전체 처리시간 중 많은 시간을 유휴 상태로 있게 된다. 푸시 방식을 도입하면서 요청하는 횟수를 최소화하여 전체 처리 시간을 최적화할 수 있었다. 하지만 푸시 모델만 사용하는 것은 기술적 한계가 있는데 메시지 기반 통신의 본질은 요청에 응답하는 것인데 프로듀서가 컨슈머의 처리 능력을 무시하면 전반적인 시스템 안정성에 영향을 미칠 수 있기 때문이다. 흐름제어 느린 프로듀서와 빠른 컨슈머 순수한 푸시 모델은 동적으로 시스템의 처리량을 증가시키는 것이 불가능하다. 빠른 프로듀서와 느린 컨슈머 프로듀서는 컨슈머가 처리할 수 있는 것보다 더 많은 데이터를 전송할 수 있으며 이로 인해 부하를 받는 컴포넌트에 치명적인 오류가 발생할 수 있다. 이를 해결하기 위한 직관적인 방법은 큐에 수집하는 것인데 3가지 유형으로 구분할 수 있다. 무제한 큐: 메모리 한도에 도달하면 전체 시스템이 손상될 가능성이 있다.(복원력이 떨어짐) 크기가 제한된 드롭 큐: 메시지의 중요성이 낮을 때 사용되는 방법으로 큐가 가득 차면 메시지를 무시하는데 중요한건 데이터 세트가 변경된다는 점이다. 크기가 제한된 블로킹 큐: 가장 느린 컨슈머의 처리량에 의해 시스템의 전체 처리량이 제한된다. 시스템의 비동기 동작을 모두 무효화하여 절대 받아들일 수 없는 시나리오다. 이런 시스템 부하에 적절하게 대응하는 방법으로 배압 제어 메커니즘이 있다. 리액티브 스트림의 기본 스펙 리액티브 스트림 스펙에는 Publisher, Subscriber, Subscription, Processor의 네 가지 기본 인터페이스가 정의돼 있다. Publisher : Observable과 비교하면 Publisher와 Subscriber를 연결하기 위한 표준화된 진입점을 의미 Subscriber : Observer와 비슷한데 onSubscribe라는 추가 메서드를 제공하는데 Subscriber에게 구독이 성공했음을 알리는 API 메서드 Subscription : 원소 생성을 제어하기 위해 기본적인 사항을 제공 cancel() : 스트림에서 구독을 취소하거나 발행을 완전히 취소 가능 request(long n) : 요청하는 Publisher가 보내줘야 하는 데이터 크기를 알려줄 수 있음 ▶️ Publisher에서 유입되는 원소의 개수가 처리할 수 있는 제한을 초과하지 않을 것을 확신할 수 있다. 리액티브 스트림은 순수 푸시 모델과는 달리 배압을 적절하게 제어할 수 있는 하이브리드 푸시-풀 모델을 제공한다. 순수 푸시 모델을 사용하고 싶으면 최대 개수 요청 request(Long.MAX_VALUE) 순수 풀 모델을 사용하고 싶으면 onNext()가 호출될 때마다 요청 Processor : Publisher와 Subscriber의 혼합 형태로 Publisher와 Subscriber 사이에 몇가지 처리 단계를 추가하도록 설계됐다. 12public interface Processor&lt;T, R&gt; extends Subscriber&lt;T&gt;, Publisher&lt;R&gt; {} 리액티브 스트림 기술 호환성 키트(TCK) 모든 동작을 검증하고 반응 라이브러리를 표준화하여 서로 호환하는지 확인하는 공통 도구로 모든 리액티브 스트림 코드를 방어하고 지정된 규칙에 따라 구현을 테스트 한다. TCK github : https://github.com/reactive-streams/reactive-streams-jvm/tree/master/tck 리액티브 스트림을 활용한 비동기 및 병렬 리액티브 스트림 API는 Publisher가 생성하고 Subscriber가 소비한 모든 신호는 처리 중에 논블로킹이어야 하며 방해받지 않아야 한다고 규칙에 명시되어 있다. 모든 프로세서나 코어를 효율적으로 사용하려면 병렬처리가 필요하고 이는 일반적으로 onNext 메서드를 병렬로 호출하는 것을 뜻하지만 on*** 메서드의 호출은 스레드 안전성을 보장하는 방식으로 신호를 보내야 하며 다중 스레드에서 수행되는 경우 외부적인 동기화를 사용해야 한다. 즉, 스트림의 요소를 병렬 처리할 수 없다. 자원을 효율적으로 활용하기 위해 스트림 처리 파이프의 각 단계에 메시지를 비동기적으로 전달하는 것이다. 상황에 따라서 처리단계를 각각 별도의 스레드로 처리하고 각 스레드 사이에 큐와 같은 데이터 구조를 적용하여 메시지를 독립적으로 제공하고 사용하도록 할수 있다.","link":"/2021/11/28/book/spring5-reactive/3/"},{"title":"4. 리액터 프로젝트 - 리액티브 앱의 기초","text":"리액티브 스트림 스펙은 리액티브 라이브러리가 서로 호환할 수 있게 해주며 여러 중요한 개선 사항이 많았지만 API 및 규칙만 정의하고 일상적인 사용을 위한 라이브러리는 제공하지 않았다. 리액티브 프레임워크중에서 가장 유명한 라이브러리 중 하나인 리액터 프로젝트(Project Reactor)는 1.x 버전에서 리액터 패턴, 함수형 프로그래밍 및 리액티브 프로그래밍과 같은 메시지 처리에 대한 모범 사례를 통합하여 비동기 논블로킹 처리를 지원하도록 설계하였다. 이후 여러 부족한 부분들을 보완하면서 2.x를 거쳐 현재는 3.x 버전으로 릴리즈되어있다. 리액터 프로젝트 필수 요소 비동기 파이프라인을 구축할 때 콜백 지옥과 깊게 중첩된 코드를 생략 코드 가독성을 높이고 리액터 라이브러리에 의해 정의된 워크플로에 **조합성(composability)**을 추가 리액터 API는 연산자를 연결해서 사용하는 것을 권장하며 이를 통해 복잡하고 재사용 가능한 실행 그래프(execution graph)를 작성할 수 있다. 그래프는 실행 흐름만 정의하며 구독자가 실제 구독을 했을 때만 데이터 플로가 기동된다. 오류 발생 가능성이 있는 비동기 요청의 결과를 효율적으로 처리하여 유연하지만 복원력 있는 코드를 작성할 수 있다. 배압은 리액티브 스트림 스펙의 핵심 속성으로 리액터 역시 동일하다. 123(데이터 플로)--▶️ --▶️ --▶️ 게시자 연산자 연산자 구독자 ◀️-- ◀️-- ◀️--(요청) 배압 전파의 일반적인 모드를 모두 지원 푸시 전용 : subscription.request(Long.MAX_VALUE) 풀 전용 : subscription.request(1) 풀-푸시(혼합형) : 구독자가 수요를 실시간으로 제어할 수 있고 게시자가 데이터 소비 속도에 적응할 수 있는 경우 풀-푸시 모델을 지원하지 않는 이전 API를 적용할 때는 예전 스타일의 배압 메커니즘을 제공한다. Flux와 Mono 데이터를 기반으로 리액티브 스트림을 생성하는 팩토리 메서드를 제공 Mono는 Flux와 비슷하지만 하나의 요소를 대상으로 사용되는데 HTTP 요청이나 DB 쿼리와 같은 비동기 작업을 래핑하는데 매우 유용 Flux와 Mono는 구독 루틴을 단순화하는 subscribe() 메서드를 람다 기반으로 재정의한다. 12345678910111213Flux.just(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;) .subscribe( data -&gt; log.info(&quot;onNext: {}&quot;, data), err -&gt; { /* ignored */ }, () -&gt; log.info(&quot;onComplete&quot;) );/*onNext: AonNext: BonNext: ConComplete*/ 또한 subscription으로 구독을 직접 제어하거나 직접 Subscriber 인터페이스를 구현하여 스트림을 구독할 수 있다. 연산자를 이용해 리액티브 시퀀스 변환하기 연산자의 종류가 너무 많아서 적절한 연산자를 선택하는 가이드를 포함한 아래 링크를 참조 Which operator do I need? 원소 매핑 : map(1:1) … 필터링 : filter … 시퀀스 수집? 합치기? : collectList() … 원소 줄이기 : reduce, scan … 스트림 조합 : concat, merge, zip … 스트림 내의 원소 일괄 처리 buffer : List와 같은 컨테이너를 이용한 Buffering, Flux&lt;List&lt;T&gt;&gt; window : Flux&lt;Flux&lt;T&gt;&gt;와 같은 형태로 스트림을 스트림으로 Windowing groupBy : Flux&lt;GroupedFlux&lt;K, T&gt;&gt; 유형의 스트림으로 Grouping flatmap : 논리적으로 map과 flatten의 2가지 작업으로 구성 map파트는 들어오는 각 원소를 리액티브 스트림(T -&gt; Flux&lt;R&gt;)으로 변환 flatten파트는 생성된 모든 리액티브 시퀀스를 R 타입의 원소를 통과시키는 새로운 리액티브 시퀀스로 병합 샘플링 : sample 연산자를 사용하여 특정 기간 내 최근에 관찰된 값을 주기적으로 출력할 수 있다. 블로킹 구조로 전환 리액티브 애플리케이션에서 블로킹 처리를 해선 안되지만, 상위 API에서 필요로 하는 경우도 있음 blockFirst, blockLast, toIterable, toStream … Mono#toFuture 를 제외한 모든 메서드는 &quot;non-blocking only&quot;로 표시된 스케줄러에서 호출되면 UnsupportedOperatorException을 발생시킨다. 시퀀스 엿보기 doOnNext(Consumer &lt;T&gt;), doOnComplete(), doOnError(Throwable)… 최종 시퀀스를 수정하지 않고 프로세스 파이프라인의 중간에 있는 각 원소나 특정 시그널을 처리해야 하는 경우 Hot 스트림과 cold 스트림 1. 콜드 퍼블리셔(cold publisher) 구독자가 나타날 때마다 시퀀스 데이터가 생성되는 방식 구독자 없이는 데이터 생성 X 대표적으로 HTTP 요청이 이런식으로 동작한다. 2. 핫 퍼블리셔(hot publisher) 데이터 생성 시 구독자의 존재 여부에 의존하지 않는 방식 첫 구독자가 없더라도 원소를 만들어 낼 수 있다. 이때 구독자가 나타나면 이전 생성값 말고 새로운 값만 보낼 수도 있다. 리액터 라이브러리에 포함된 대부분은 Processor 인터페이스를 상속한다. 콜드 퍼블리셔를 리액티브 변환을 통해 핫 퍼블리셔로 전환할 수 있다.","link":"/2021/12/03/book/spring5-reactive/4/"},{"title":"1. 계층형 아키텍처의 문제는 무엇일까?","text":"계층형 아키텍처란? 계층(layer)으로 구성된 (웹) 애플리케이션은 전통적인 웹 애플리케이션 구조를 말한다. 크게보면 웹 → 도메인 → 영속성 으로 구성된 3계층으로 많이 표현되는데 웹 : 요청을 받아서 서비스로 요청을 보냄 도메인(비즈니스) : 필요한 비즈니스 로직을 수행하고 엔티티의 현재 상태를 조회하거나 변경하기 위해 영속성 계층의 컴포넌트 호출 영속성 : 엔티티, 리포지터리 등 데이터베이스 관련 컴포넌트 오랫동안 사용한만큼 견고한 아키텍처 패턴이 맞고 잘 이해하고 사용하면 각 계층에 독립적으로 로직을 작성하고 기존 기능에 영향없는 기능 추가도 가능하다. 다만, 계층형은 코드에 나쁜 습관들이 스며들기 쉽고 시간이 지날수록 유지보수가 힘들어지는 단점들이 있다. 계층형 아키텍처는 데이터베이스 주도 설계를 유도한다. 계층형의 토대는 데이터베이스라서 웹은 도메인을, 도메인은 영속성을 의존하다보니 모두 데이터베이스에 의존하게 된다. 보통 애플리케이션을 만들때 비즈니스를 관장하는 규칙이나 정책을 반영한 모델을 만드는데 이때 우리는 상태(state)가 아니라 행동(behavior)을 중심으로 모델링한다. 하지만 계층형의 설계는 보통 데이터베이스를 토대로 도메인 로직을 구현하는 방식이라서 아키텍처의 구현으로는 맞더라도 비즈니스 관점에선 다르다. 가장 중요한 도메인 로직을 먼저 만들어야 로직을 제대로 이해하는지 확인하고 이를 토대로 웹과 영속성 계층을 설계할 수 있기 때문이다. ORM(object-relational-mapping, 객체 관계매핑) 프레임워크(JPA, 하이버네이트 등)를 사용하면 비즈니스 규칙을 영속성 관점에 섞고 싶은 생각이 들게 된다. ORM에 의해 관리되는 엔티티들은 일반적으로 영속성 계층에 두고 도메인계층에선 엔티티에 접근가능한데 이러한 구조는 영속성 계층과 도메인 계층 사이에 강한 결합을 만들게 된다. 서비스에서 영속성 모델을 마치 비즈니스 모델처럼 사용하다보면 도메인 로직뿐만 아니라 영속성 계층과 관련된 작업들도 해줘야 한다. 영속성 코드가 사실상 도메인 코드에 녹아들면서 둘 중 하나만 바꾸는게 어려워져서 계층형의 목표와 대치되는 코드가 된다. 지름길을 택하기 쉬워진다. 계층형 아키텍처는 특정한 계층에서는 같은 계층에 있는 컴포넌트나 아래에 있는 계층에만 접근 가능하다는 규칙이 있다. 만약 상위 계층에 위치한 컴포넌트에 접근해야 한다면? 컴포넌트를 계층 아래로 내려버리면 된다. 한번은 괜찮을 수 있다. 근데 2번, 3번이 넘고 나 뿐만 아니라 다른 동료들도 그렇게 하게 되면? 유틸리티나 헬퍼 컴포넌트 등이 아래 계층으로 내려오게 되면 영속성 계층은 모든 것에 접근 가능하기 때문에 시간이 지날 수록 점점 비대해 질 것이다. 테스트하기 어려워진다. 계층형 아키텍처에서 계층을 건너뛰도록 하는 경우도 있다. 엔티티의 필드를 딱 하나만 조작하면 될 경우에 웹 계층에서 바로 영속성 계층에 접근하면 도메인 계층을 건너 뛰게 된다. 이런 경우 크게 두가지 문제가 발생하는데 도메인 로직을 웹 계층에 구현하게 된다. 만약 유스케이스가 확장된다면 더 많은 도메인 로직이 웹 계층에 추가되면서 애플리케이션 전반으로 책임이 섞이고 핵심 도메인 로직들이 퍼져나갈 수 있다. 웹 계층 테스트에서 도메인 계층뿐만 아니라 영속성 계층도 모킹(mocking)해야 한다. 이 경우 단위 테스트의 복잡도가 올라가고 이렇게 복잡한 설정을 할 시간이 없어서 테스트를 안하게 되는 시작이 된다. 유스케이스를 숨긴다. 기능을 추가하거나 변경할 적절한 위치를 찾는 일이 빈번하기 때문에 ㅐ아키텍처는 코드를 빠르게 탐색하는데 도움이 돼야 한다. 계층형 아키텍처에서는 도메인 로직이 여러 계층에 걸쳐 흩어지기 쉬운 환경이라 유스케이스가 “간단”해서 도메인 계층을 생략하면 웹 계층에 존재할 수도 있고, 도메인과 영속성 모두에 접근할 수 있도록 컴포넌트의 계층을 내리면 영속성 계층에 존재할 수도 있다. 이런 경우 새로운 기능을 추가할 적당한 위치를 찾기 어려워지고 여러 개의 유스케이스를 담당하는 아주 넓은 서비스가 만들어질 수도 있다. 넓은 서비스는 영속성 계층에 많은 의존성을 갖게되고, 웹 레이어의 많은 컴포넌트가 이 서비스에 의존하게 된다. 서비스는 점점 더 복잡해지고 테스트하기도 어려워진다. 동시 작업이 어려워진다. 새로운 기능을 추가하기 위해 3명의 개발자가 있을때 각 계층에 각각의 기능을 동시에 개발할 수 있을까? 계층형에선 영속성 계층 위에 모든 것이 만들어지기 때문에 영속성 계층을 먼저 개발ㄹ해야 하고, 그 다음에 도메인 계층, 웹 계층을 만들어야 한다. 동시에 한꺼번에가 아니라 한번에 한명의 개발자만 일할 수 있는 것이다. 또한 넓은 서비스가 있다면 서로 다른 기능을 동시에 작업하기 어려운데 병합 충돌(merge conflict)이나 롤백이 필요한 문제가 발생할 수 있다. 유지보수 가능한 소프트웨어를 만드는 데 어떻게 도움이 될까? 올바르게 구축하고 몇 가지 추가적인 규칙들을 잘 적용한다면 계층형 아키텍처는 유지보수하기 매우 쉬워지며 코드를 쉽게 변경하거나 추가할 수 있다. 하지만 잘못된 방향으로 흘러가기 쉽다보니 계층형 아키텍처로 만들든 다른 아키텍처 스타일로 만들든, 지름길을 택하지 않고 유지보수하기에 더 쉬운 솔루션을 만드는 데 도움이될 것이다.","link":"/2023/02/25/book/clean-architecture/clean-1/"},{"title":"11. 의식적으로 지름길 사용하기","text":"경제적인 관점에서 지름길을 사용하는게 더 효과적일 수 있다. 지름길을 사용하려면 일단 지름길 자체를 파악해야 한다. 유스케이스 간 모델 공유하기 기본적으로 유스케이스마다 다른 입출력 모델을 가져야 한다. 입력 파라미터의 타입과 반환값의 타입이 달라야 한다는 말이다. 만약 인커밍 포트 인터페이스의 입출력 모델이 같은 모델을 공유할 경우 공유한 모델이 변경될 경우 두 유스케이스 모두 영향을 받는다. 단일 책임 원칙에서 중요하게 생각하는 “변경할 이유”를 공유하게 되는 것이다. 만약 실제로 특정 세부사항을 변경할 경우 실제 두 유스케이스 모두에 영향을 주고 싶은 것이라면 괜찮다. 시작은 공유하더라도 어느 시점에서 유스케이스가 독립적으로 분리가 필요한 시점이라면 분리해야 한다. 도메인 엔티티를 입출력 모델로 사용하기 도메인 엔티티를 유스케이스의 입출력 모델로 사용하면 결합이 발생한다. 유스케이스가 단순히 데이터베이스 필드 몇개를 업데이트 하는 수준으로 간단하다면 괜찮을지도 모르지만 더 복잡한 도메인 로직을 구현해야 한다면 유스케이스 인터페이스에 대한 전용 입출력 모델을 만들어야 한다. 유스케이스의 변경이 도메인 엔티티까지 전파되는걸 바라지 않는다면 말이다. 인커밍 포트 건너뛰기 아웃고잉 포트는 의존성 역전(안쪽으로 흐르게 하기)에 필수 요소이지만 인커밍 포트는 그렇지 않다. 인커밍 어댑터에서 인커밍 포트 없이 애플리케이션 서비스에 직접 접근하도록 할 수 있다. 이 경우 두 계층 사이의 추상화 계층을 줄이면서 괜찮게 느껴질 수 있다. 하지만 인커밍 포트를 통해 애플리케이션 중심에 접근하는 진입점을 정의하지 않으면 특정 유스케이스를 구현하기 위해 어떤 서비스 메서드를 호출해야 하는지 알기 위해 애플리케이션 내부 동작에 대해 더 알아야 한다. 애플리케이션 서비스 건너뛰기 만약 간단한 CRUD 유스케이스에서 애플리케이션 서비스가 도메인 로직 없이 생성, 업데이ㅌ, 삭제 요청을 그대로 영속성 어댑터에 전달하기 때문에 건너뛰고 싶을 수도 있다. 하지만 이렇게 하려면 인커밍 어댑터와 아웃고잉 어댑터 사이에 모델을 공유해야 하는데 공유해야 하는 모델이 도메인 엔티티가 되면서 앞서 이야기한 도메인 엔티티를 입출력 모델로 사용하는 경우가 될 것이다. 또한 시간이 지나서 유스케이스가 점점 복잡해지면 도메인 로직을 그대로 아웃고잉 어댑터에 추가하고 싶은 생각이 들면서 도메인 로직이 흩어져서 찾고 유지보수 하는것이 어려워 진다.","link":"/2023/04/02/book/clean-architecture/clean-11/"},{"title":"10. 아키텍처 경계 강제하기","text":"일정 규모 이상의 프로젝트라면 계층 간의 경계가 약화되고, 코드는 점점 테스트하기 어려워질 것이다. 아키텍처의 경계를 강제한다는 것은 각 계층간의 의존성이 올바른 방향을 향하도록 강제하는 것을 말한다. 바깥쪽에서 안쪽으로, 어댑터 → 애플리케이션 → 도메인 으로 의존성이 흘러가야 한다. 의존성 규칙을 강제하는 가장 기본적인 방법은 접근 제한자를 사용하는 것이다. public, private, protected는 많이 알고 있지만 default에 대해서 깊게 생각하는 신입 개발자는 많지 않다. 자바 패키지를 통해 클래스들을 응집적인 모듈로 만들어주기 때문에 default 제한자는 중요하다. 모듈 내의 클래스들은 서로 접근이 가능하지만 패키지 바깥에선 접근할 수 없다. 그럼 모듈의 진입점으로 활용할 클래스들만 골라서 public으로 만들어준다면 의존성이 잘못된 방향을 가리키게 될 확률이 줄어든다. 각 계층의 어댑터, 서비스 클래스는 외부에서 접근할 필요가 없으니 default 제한자로 생성하고 다른 계층에서 접근해야 하는 domain 패키지는 public으로 만들 수 있다. 마찬가지로 웹어댑터와 영속성 어댑터에서 접근 가능해야 하는 애플리케이션 계층의 인커밍,아웃고잉 포트 역시 public으로 구현해야 한다. default 제한자는 몇 개 정도의 클래스로만 이뤄진 작은 모듈에서 가장 효과적이다. 패키지 내 클래스가 일정 수준을 넘어서게 되면 보통 하위 패키지를 만드는 방법을 선호하는데 이렇게 하면 자바는 하위 패키지를 다른 패키지로 취급하여 하위 패키지의 default 멤버에 접근할 수 없게 된다. 이렇게 되면 하위 패키지의 멤버는 public으로 만들어서 바깥으로 노출시켜야 하는데 의존성 규칙이 깨질 수 있는 환경이 될 수 있다. 클래스에 public 제한자를 사용하면 아키텍처의 의존성 방향이 잘못되더라도 컴파일러에서 이를 확인할 수 없다. 이때 컴파일 후 런타임에서 체크하는 방법을 도입할 수 있는데 ArchUnit은 Junit과 같은 단위 테스트 프레임워크 기반에서 아키텍처의 의존성 규칙 위반을 발견하면 예외를 던질 수 있는 API를 제공한다. ArchUnit API를 이용하면 헥사고날 아키텍처 내에서 관련된 모든 패키지를 명시할 수 있는 일종의 도메인 특화 언어(DSL)을 만들 수 있고 패키지 사이의 의존성 방향이 올바른지 자동으로 체크할 수 있다. 네이버에서 유닛테스트에 해당 라이브러리를 도입한 사례는 나중에 참고해보자 https://d2.naver.com/helloworld/9222129 Maven이나 Gradle은 자바에서 많이 사용하는 빌드 도구인데 주요한 기능 중 하나는 의존성 해결이다. 잘못된 의존성을 막기 위해 아키텍처를 여러 개의 빌드 아티팩트로 만들 수 있다. 각 모듈 혹은 계층에 대해 전용 코드베이스와 빌드 아티팩트로 분리된 빌드 모듈(JAR 파일)을 만들고 각 모듈의 빌트 스크립트에서는 아키텍처에서 허용하는 의존성만 지정하게 되면 클래스들이 클래스패스에 존재하지도 않아 컴파일 에러가 발생하여 잘못된 의존성을 만들수조차 없게 될 것이다. 또한 빌드 모듈로 아키텍처 경계를 구분하는 것을 패키지로 구분하는 방식에 비해 몇가지 장점이 더 있는데 빌드 도구는 순환 의존성(circular dependency)를 허용하지 않는다. 의존성을 해결하는 과정에서 무한 루프에 빠지기 때문에 빌드 도구를 사용하면 순환의존성이 없음을 확신할 수 있다. 빌드 모듈 방식은 다른 모듈을 고려하지 않고 특정 모듈의 코드를 격리한 채로 변경할 수 있다. 특정 어댑터에서 컴파일 에러가 발생하는 애플리케이션 계층을 리팩토링 할 경우 같은 빌드 모듈에 있다면 컴파일 에러를 고쳐야 애플리케이션 계층의 테스트가 실행되겠지만 서로 다른 빌드 모듈에 독립적으로 존재할 경우 어댑터의 컴파일 에러와 상관없이 애플리케이션 계층을 테스트할 수 있을 것이다. 모듈 간 의존성이 빌드 스크립트에 분명하게 선언돼 있기 때문에 새로운 의존성을 추가할 때 의식적으로 한번 더 생각해볼 수 있다.","link":"/2023/04/02/book/clean-architecture/clean-10/"},{"title":"2. 의존성 역전하기","text":"계층형 아키텍처의 대안을 이야기 하기전에 SOLID 원칙의 ‘S’와 ‘D’를 담당하는 아래 원칙들을 먼저 살펴보자. 단일 책임 원칙(Single Responsibility Principle, SRP) 하나의 컴포넌트는 오로지 한 가지 일만 해야 하고, 그것을 올바르게 수행해야 한다. 이 말은 실제 의도와는 조금 다른 오해가 발생할 여지가 있으니 아래 정의가 좀 더 정확하다고 볼 수 있다. 컴포넌트를 변경하는 이유는 오직 하나뿐이어야 한다. 만약 컴포넌트를 변경할 이유가 한 가지라면 우리가 “어떤 다른 이유로” 소프트웨어를 변경하더라도 이 컴포넌트에 대해선 전혀 신경 쓸 필요가 없다. 하지만 변경할 이유라는건 컴포넌트 간의 의존성을 통해 너무 쉽게 전파된다. A 컴포넌트는 B,C,D,E에 의존하고 E는 다른 의존성이 없다면 A는 다른 B,C,D,E가 바뀔 때 함께 바뀌어야 하지만 E는 E에 기능이 추가,변경될 때만 바뀌게 될 것이다. 의존성 역전 원칙(Dependency Inversion Principle, DIP) 계층형에서 계층 간 의존성은 항상 다음 계층인 아래 방향을 가리킨다. 단일 책임 원칙을 고수준에서 적용할 경우 상위 계층들이 하위 계층들에 비해 변경할 이유가 더 많다. 그러므로 영속성 계층에 대한 도메인 계층의 의존성 때문에 영속성 계층을 변경할 때마다 잠재적으로 도메인 계층도 변경해야 한다. 하지만 도메인 코드는 애플리케이션에서 가장 중요한 코드인데 영속성 코드가 바뀐다고 도메인 코드까지 바꾸는게 맞을까? 이 의존성은 어떻게 제거할 수 있을까? 의존성 역전 원칙은 말 그대로의 의미이다. 코드 상의 어떤 의존성이든 그 방향을 바꿀 수(역전시킬 수) 있다. 단, 서드파티 라이브러리처럼 제어할 수 없는 코드에 의존성을 가지고 있다면 역전이 불가능하다. 일반적으로 보기 쉬운 계층형 구조의 서비스가 있다. 도메인 계층의 서비스는 영속성 계층의 엔티티와 리포지토리와 상호작용한다. 엔티티는 도메인 객체를 표현하고 도메인 코드는 이러한 엔티티의 상태를 변경하는 일을 중심으로 하니까 일단 엔티티를 도메인 계층으로 올려보면 영속성의 리포지토리가 도메인의 엔티티를 의존하는 순환의존성이 생기게 된다. 여기서 DIP를 적용하면 도메인 계층에 리포지토리에 대한 인터페이스를 만들고, 실제 리포지토리는 영속성 계층에서 구현하게 하는 것이다. 이제 도메인 계층에 인터페이스를 도입함으로써 의존성을 역전시켜서 도메인 로직은 영속성 코드에 의존하지 않고 영속성 계층이 도메인 계층에 의존하게 된다. 클린 아키텍처 도메인 코드가 바깥으로 향하는 어떤 의존성도 없어야 함을 의미한다. 대신 의존성 역전 원칙의 도움으로 모든 의존성이 도메인 코드를 향하고 있다. 클린 아키텍처의 코어에는 주변 유스케이스에서 접근하는 도메인 엔티티들이 있다. 유스케이스는 서비스를 의미하는데 단일 책임을 갖기 위해 좀 더 세분화 시켜서 넓은 서비스 문제를 피한다. 도메인 코드에선 어떤 영속성 프레임워크나 UI 프레임워크가 사용되는지 알 수 없기 때문에 특정 프레임워크에 특화된 코드를 가질 수 없고 비즈니스 규칙에 집중할 수 있어서 자유롭게 모델링할 수 있다. 다만, 도메인 계층이 영속성이나 UI 같은 외부 계층과 철저하게 분리돼야 하므로 애플리케이션의 엔티티에 대한 모델을 각 계층에서 유지보수 해야 한다. 영속성에서 ORM을 사용하는 경우, 도메인 계층과 영속성 계층이 데이터를 주고받을 때, 두 계층에 각각 엔티티 클래스를 만들어서 서로 변환해야 하는데 이는 바람직한 방향이다. 특정 프레임워크에 특화된 문제로부터 해방시키고자 했던, 결합이 제거된 상태이다. 클린 아키텍처는 약간 추상적인 느낌이 강해서 이 원칙들을 좀 더 구체적으로 만들어주는 ‘육각형 아키텍처(헥사고날 아키텍처)’에 대해서 살펴보자. 육각형 아키텍처(헥사고날 아키텍처) 애플리케이션 코어가 각 어댑터와 상호작용하기 위해 특정 포트를 제공하기 때문에 ‘포트와 어댑터 아키텍처라고도 불린다. 꼭 육각형의 모양이 중요한건 아니고 팔각형이어도 상관없다. 육각형 안에는 도메인 엔티티와 이와 상호작용하는 유스케이스가 있다. 외부로 향하는 의존성이 없고 모든 의존성은 코어를 향한다. 육각형 바깥에는 웹 브라우저와 상호작용하는 웹 어댑터, 데이터베이스와 상호작용하는 영속성 어댑터, 외부 시스템와 상호작용하는 어댑터 등 애플리케이션과 상호작용하는 다양한 어댑터들이 있다. 코어와 어댑터들 간의 통신이 가능하려면 애플리케이션 코어가 각각의 포트를 제공해야 한다. 주도하는 어댑터에게는 포트가 코어에 있는 유스케이스 클래스들에 의해 구현되고 호출되는 인터페이스가 될 것이고, 주도되는 어댑터에는 어댑터에 의해 구현되고 코어에 의해 호출되는 인터페이스가 될 것이다. 이러한 아키텍처의 목적은 결국 도메인 코드가 바깥쪽 코드에 의존하지 않게 함으로써 영속성과 UI에 특화된 모든 문제로부터 도메인 로직의 결합을 제거하고 코드를 변경할 이유의 수를 줄이는 효과가 있다.","link":"/2023/03/01/book/clean-architecture/clean-2/"},{"title":"3. 코드 구성하기","text":"코드만 보더라도 어떤 아키텍처인지 알 수 있다면 좋지 않을까? 보통 새로운 프로젝트를 시작할 때 가장 먼저 패키지 구조를 설계하게 된다. 계층으로 구성하기 기본적인 계층형으로 프로젝트를 생성한다면 아래와 같은 구조가 될 것이다. 12345678910buckpal ├─domain │ ├─Account │ ├─Activity │ ├─AccountRepository │ └─AccountService ├─persistence │ └─AccountRepositoryImpl └─web └─AccountController 도메인, 웹, 영속성 계층별로 패키지를 만들었고 앞서 나왔던 의존성 역전을 사용하여 domain 패키지에 AccountRepository 인터페이스를 두고 persistence 패키지에 구현체를 둬서 의존성이 도메인을 바라보도록 구성되어있다. 다만 몇가지 단점이 보이는데 기능이나 특성을 구분짓는 패키지의 경계가 없다. 새로운 기능(ex) 사용자관리)을 추가하려면 각 계층 패키지에 UserController, UserService, User 등을 추가하게 되는데 다른 기능과 섞이게 되면 예상치 못한 부수효과가 발생할 수 있다. 애플리케이션이 어떤 유스케이스를 제공하는지 파악하기 어렵다. AccountController와 AccountService가 구체적으로 어떤 기능을 제공하는지 파악하려면 내부 구현 메서드를 살펴봐야 한다. 패키지만 봐서 의도하는 아키텍처를 짐작하기 어렵다. 육각형 아키텍처라고 추측하고 웹 어댑터와 영속성 어댑터를 찾기 위해 web, persistence 패키지를 조사해볼 순 있지만 어떤 기능이 웹어댑터에서 호출되는지, 영속성 어댑터가 도메인 계층에 어떤 기능을 제공하는지 한눈에 알 수 없다. 인커밍 포트와 아웃고잉 포트가 코드 속에 숨겨져 있다. 기능으로 구성하기 1234567buckpal └─account ├─Account ├─AccountController ├─AccountRepository ├─AccountRepositoryImpl └─SendMoneyService 계좌 관련 기능을 모두 account 라는 패키지에 모았고 AccountService도 책임을 좁히기 위해서 SendMoneyService로 변경하였다. 이렇게 되면 ‘송금하기’ 유스케이스를 구현한 코드는 클래스명만 봐도 바로 찾을 수 있다. 하지만 기능을 기준으로 코드를 구성하면 기반 아키텍처가 명확하게 보이지 않아서 가시성이 많이 떨어진다는 큰 단점이 있다. 아키텍처적으로 표현력 있는 패키지 구조 육각형 아키텍처에서 구조적으로 핵심적인 요소는 엔티티, 유스케이스, 인커밍/아웃고잉 포트, 인커밍/아웃고잉(혹은 주도하거나 주도되는) 어댑터이다. 123456789101112131415161718192021buckpal └─account ├─adapter │ ├─in │ │ └─web │ │ └─AccountController │ ├─out │ └─persistence │ ├─AccountPersistenceAdapter │ └─SpringDataAccountRepository ├─domain │ ├─Account │ └─Activity └─application ├─SendMoneyService └─port ├─in │ └─SendMoneyUseCase └─out ├─LoadAccountPort └─UpdateAccountStatePort 도메인 모델이 속한 domain 패키지와 도메인 모델을 둘러싼 서비스 계층을 포함하는 application패키지가 있다. SendMoneySerivce는 인커밍 포트 인터페이스인 SendMoneyUseCase를 구현 아웃고잉 포트 인터페이스이자 영속성 어댑터에 의해 구현된 LoadAccountPort와 UpdateAccountStatePort를 사용한다. adapter 패키지는 애플리케이션 계층의 인커밍 포트를 호출하는 인커밍 어댑터와 애플리케이션 계층의 아웃고잉 포트에 대한 구현을 제공하는 아웃고잉 어댑터를 포함한다. 책을 읽다가 곰곰히 생각해 봤지만 패키지의 구조가 표현력이 있긴한데 아직 익숙치 않아서 한눈에 들어오진 않는다. 다만, 팀원들과 이러한 아키텍처에 대한 논의가 충분히 되고 합의된 상태에서 구조를 잡는다면 코드와 아키텍처가 직접적으로 매핑되면서 추상적이던 아키텍처가 좀 더 구체적으로 파악이 가능해진거 같기도 하다.","link":"/2023/03/01/book/clean-architecture/clean-3/"},{"title":"4. 유스케이스 구현하기","text":"앞서 설계한 패키지대로 코드를 작성하면 애플리케이션, 웹, 영속성 계층이 아주 느슨하게 결합돼 있기 때문에 필요한 대로 도메인 코드를 자유롭게 모델링할 수 있다. 육각형 아키텍처는 도메인 중심의 아키텍처에 적합하기 때문에 도메인 엔티티를 만드는 것으로 시작한 후 해당 도메인 엔티티를 중심으로 유스케이스를 구현한다. 유스케이스 둘러보기 유스케이스는 일반적으로 아래와 같은 단계를 따른다. 입력을 받는다 비즈니스 규칙을 검증한다 모델 상태를 조작한다 출력을 반환한다. 유스케이스는 인커밍 어댑터로부터 입력을 받는데 유스케이스 코드는 도메인 로직에만 집중하고 ‘입력 유효성 검증’은 다른 곳에서 처리하는게 좋다. 그러나 유스케이스는 비즈니스 규칙을 검증할 책임이 있고 도메인 엔티티와 이 책임을 공유한다. 입력 유효성 검증 입력 유효성 검증이 유스케이스의 책임이 아니라도 애플리케이션 계층의 책임은 맞다. 유효성 검증을 하지 않으면 애플리케이션 코어의 바깥쪽으로부터 유효하지 않은 입력값을 받게 되고, 모델의 상태를 해칠 수 있다. 입력 모델(input model)에서 이러한 검증을 담당해보자. 유스케이스에서 입력에 사용할 모델 클래스의 생성자 내에서 특정 조건에 위배될 경우 객체 생성 자체를 막고 예외를 던지면 될 것이다. 또한 필드에 final을 붙여서 불변 필드로 만들어 생성에 성공하면 유효한 상태를 유지하고 잘못된 상태로 변경할 수 없다는 사실을 보장할 수 있다. 사실 이런 기능들은 Bean Validation API를 사용하면 편하게 사용할 수 있다. 1234567891011@Getterpublic class SendMoneyCommand extends SelfValidating&lt;SendMoneyCommand&gt; { @NotNull private final Money; public SendMoneyCommand(Money money){ this.money = money; requiredGreaterThan(money, 0); this.validateSelf(); }} SelfValidating 추상 클래스는 validateSelf() 메서드를 제공하여 호출하면 필드에 지정된 Bean Validation 어노테이션(@NotNull 같은)을 검증하고 유효성 검증 규칙을 위반한 경우 예외를 던진다. 생성자의 힘 필드가 많아질 경우 빌더패턴을 도입해서 생성자를 private으로 만들고 빌더의 build() 메서드 내부에 생성자 호출을 숨길 수 있다. 다만, 만약 빌더와 생성자에 새로운 필드를 추가하고 빌더를 호출하는 코드에 새로운 필드를 추가하는 걸 깜빡하더라도 컴파일러는 이러한 시도에 대해서 경고해주지 못할 수 있다. 빌더 뒤에 숨기지 말고 생성자를 직접 사용했다면 컴파일 에러에 따라 나머지 코드에 변경사항을 반영할 수 있을 것이다. 난 저자랑 생각이 다름 저자가 예시에서 파라미터가 20개인 생성자를 호출하는 대신 빌더를 사용하면 이러이러 하다 라고 말했는데 코드의 가독성 측면에서 빌더패턴이 훨씬 보기 좋다고 생각한다. 빌더 호출 코드에 깜빡할 경우가 얼마나 될것이며 단위테스트 과정에서 어느정도 걸러낼 수 있다고 생각된다. 오히려 생성자의 파라미터 20개가 모두 String 타입일 경우 지저분한 코드를 보는게 더 스트레스 받을거 같다. 유스케이스마다 다른 입력 모델 각기 다른 유스케이스에 동일한 모델을 사용하고 싶은 경우가 있다. 계좌 등록하기와 계좌 업데이트 하기라는 두개의 기능을 구현할 때 계좌에 대한 등록시점에선 소유권을 체크하기 위해 계좌의 소유자 ID 필드가 무조건 필요하지만 업데이트는 계좌번호만 체크한다고 했을때 ID 필드는 null을 허용하도록 만들수 있다. 일단 불변 도메인 모델에 null을 허용하는 것부터 일단 코드 스멜이 난다고 볼수 있다.(잠재적으로 side-Effect가 발생할수도 있는 코드를 말한다.) 단일책임원칙을 고려했을 때 각 유스케이스의 전용 입력모델을 사용하는게 결합도도 낮추고 부수효과도 줄일 수 있는 방법이다. 다만, 모든 입력 데이터에 대해서 각 유스케이스 별 모델 매핑을 해줘야하는 비용이 있지만 매핑 전략에 대해선 후술 한다. 비즈니스 규칙 검증하기 검증은 크게 두가지를 생각해볼 수 있다. 입력 유효성 검증 비즈니스 규칙 검증 둘을 구분하는 가장 실용적인 방법은 특정 검증이 도메인의 상태에 접근이 필요한지? **를 생각해보면 된다. 가령 단순히 입력 데이터의 유효성 체크는 도메인의 상태를 체크할 필요 없이 선언적으로 검증이 가능하지만 비즈니스 규칙은 좀 더 맥락을 이해하고 구현할 필요가 있다. “출금계좌는 초과출금될 수 없다.” 라는 규칙을 검증할땐 출금계좌의 존재여부 부터 체크하는 등 도메인의 상태에 접근해야 하지만 ”출금금액은 0보다 커야한다.” 라는 규칙은 도메인의 상태에 접근없이 입력 데이터를 단순히 체크하기만 하면 된다 비즈니스 규칙 검증은 보통 도메인 엔티티 내부에 직접 구현하는게 가장 좋다. 123456789public class Account { //... public boolean withDraw(Money money, AccountId targetAccountId) { if(!mayWithDraw(money)){ return false; } //... }} 이렇게 하면 지켜야하는 비즈니스 로직 옆에 있기 때문에 위치를 정하기도, 추론하기도 쉽다. 만약 엔티티 내부에 위치하기 어렵다면 유스케이스에서 도메인을 사용하기 전에 검증하는 방식도 있을 것이다. 1234567891011@RequriedArgsConstruct@Transactionalpublic class SendMoneyService implements SendMoneyUseCase { //... @Override public void SendMoney(SendMoneyCommand command){ requriedSourceAccount(command.getSourceAccountId()); requriedTargetAccount(command.getTargetAccountId()); //... }} 유효성 검증 코드를 실행 후 오류가 발생하면 유효성 전용 예외처리를 통해 사용자 풍부한 도메인 모델 vs 빈약한 도메인 모델 풍부한 도메인 모델 : 애플리케이션의 코어에 있는 도메인 엔티티에 최대한 많은 로직이 담겨있다. 도메인의 상태를 변경하는 메서드를 제공하고 비즈니스 규칙 검증에 유효한 값만 허용한다. 빈약한 도메인 모델 : 도메인 엔티티는 최대한 적은 로직을 가지고 있다. 보통 getter, setter를 제외한 다른로직은 모두 유스케이스에 구현한다. 결국 복잡한 비즈니스 모델이 어디에 있냐의 차이로 스타일의 차이라고 봐도 무방할 거 같다. 유스케이스마다 다른 출력모델 입력에 대한 처리가 끝나면 출력을 해야하는데 이때도 동일하게 각 유스케이스마다 다른 출력모델을 사용하는게 좋다. 어떤 출력에선 Account 모델 자체를 받고 싶어할 수도 있고, 단순히 성공실패 여부 등 boolean 값만을 받고 싶을 수도 있다. 정답은 없지만 명확한 규칙이 없다면 최대한 작은 데이터를 반환하는게 좋다. 모델 클래스를 통째로 반환하면 강한 결합이 일어나는데 한 유스케이스의 출력 모델에 새로운 필드가 추가될 경우 동일한 모델을 공유하는 다른 유스케이스에서도 해당 필드를 처리해야 하는 것처럼 영향이 있기 때문에 모델은 구체적이고 작게 결합은 약하게 하는게 좋다. 같은 이유로 도메인 엔티티 자체를 출력모델로 사용하는 것도 최대한 자제해야한다. 읽기 전용 유스케이스는 어떨까? 상태를 변경할 필요 없이 단순히 DB를 조회해서 값을 반환하기만 하는 읽기 전용 유스케이스를 구현해야 할때 상태변경 유스케이스와 동일한 형식으로 만들게 되면 간단한 기능에 비해 구현해야하는 것들이 많아질 수 있다. 간단히 쿼리만 조회해야 한다면 쿼리 서비스를 만들 수 있다. 인커밍 전용 포트를 만들어서 쿼리 서비스에서 구현하는 것이다. 12345678910@RequiredArgsConstructorpublic class GetAccountQueryService implements GetAccountBalanceService { private final LoadAccountPort loadAccountPort; @Override public Money getAccountBalance(AccountId accountId) { return loadAccountPort.loadAccount(accountId, LocalDateTime.now()) .calculateBalance(); }} 쿼리서비스는 유스케이스 서비스와 동일한 방식으로 동작하는데 GetAccountBalanceQuery 라는 인커밍 포트를 구현하고, 데이터베이스로부터 실제로 데이터를 로드하기 위해 LoadAccountPort라는 아웃고잉 포트를 호출한다. 이처럼 읽기 전용 쿼리는 쓰기가 가능한 유스케이스(or 커맨드)와 코드 상에서 명확히 구분되는데 이런 방식은 CQS(Command-Query Separation)나 CQRS(Command-Query Responsibility Segregation) 같은 개념과 잘 맞는다.","link":"/2023/03/05/book/clean-architecture/clean-4/"},{"title":"5. 웹 어댑터 구현하기","text":"의존성 역전 웹 어댑터는 인커밍 어댑터로 외부로부터 요청을 받아 애플리케이션 코어를 호출하여 어떤 일을 해야할지 알려준다. 이때 제어의 흐름은 웹 어댑터 계층의 컨트롤러에서 애플리케이션 계층의 서비스로 흐른다. 애플리케이션 계층은 서비스와 통신할 수 있는 특정 포트를 중간 계층으로 구현하고 웹 어댑터는 이러한 포트를 호출할 수 있다. addapter.in.web → application.port.in ← application.service 컨트롤러 → 포트 ← 서비스 여기서 가만히 보면 의존성 역전 원칙이 적용된 걸 알수 있는데 제어의 흐름이 왼쪽에서 오른쪽 즉, 웹 어댑터가 유스케이스를 직접 호출할수도 있다. addapter.in.web → application.service 컨트롤러 → 서비스 컨트롤러에서 서비스를 직접 호출하지 않고 포트를 두는 이유는 애플리케이션 코어가 외부와 통신할 수 있는 곳의명세가 바로 포트이기 때문이다. 포트를 적절한 위치에 구현하게 되면 외부와 어떤 통신을 하는지 명확하게 알기 쉬워 유지보수에 큰 도움이 된다. 웹 어댑터의 책임 일반적으로 웹 어댑터는 다음과 같은 일을 한다. HTTP 요청을 자바 객체로 매핑 권한 검사 입력 유효성 검증 입력을 유스케이스의 입력 모델로 매핑 유스케이스 호출 유스케이스의 출력을 HTTP로 매핑 HTTP 응답을 반환 이 과정에서 한가지라도 예외가 발생하면 웹 어댑터는 오류를 반환단다. 웹 어댑터에 많은 기능과 책임이 들어간거 같지만 이것들은 모두 애플리케이션 계층에서 신경쓰면 안되는 것들이다. HTTP와 관련된 로직을 애플리케이션 계층에서 알게 된다면 HTTP가 아닌 다른 통신방식을 사용하는 인커밍 포트를 구현할 때 동일한 도메인 로직을 수행할 수 없게 된다. 컨트롤러 나누기 스프링 MVC 프레임워크에서는 위에서 언급한 책임들을 수행할 컨트롤러 클래스를 구현할 수 있다. 웹 어댑터를 구성할땐 하나의 클래스에 많은 기능을 담기보단 가능하면 최대한 기능을 작게 분할하여 여러 개의 어댑터로 구성하는 편이 좋다. 계좌와 관련된 REST API를 설계한다고 할때 보통 AccountController 클래스를 만들게 된다. 12345678910111213141516171819202122@RestController@RequiredArgsConstructorpublic class AccountController{ //... @GetMapping(&quot;/accounts&quot;) List&lt;AccountResource&gt; listAccounts() {} @GetMapping(&quot;/accounts/{id}&quot;) AccountResource getAccount(@PathVariable(&quot;accountId&quot;) Long accountId) {} @GetMapping(&quot;/accounts/{id}/balance&quot;) long getAccountBalance(@PathVariable(&quot;accountId&quot;) Long accountId) {} @PostMapping(&quot;/accounts&quot;) AccountResource createAccount(@RequestBody AccountResource account) {} @PostMapping(&quot;/accounts/send/{sourceAccountId}/{targetAccountId}/{amount}&quot;) void sendMoney( @PathVariable(&quot;sourceAccountId&quot;) Long sourceAccountId; @PathVariable(&quot;targetAccountId&quot;) Long targetAccountId; @PathVariable(&quot;amount&quot;) Long amount) {} } Account와 관련된 모든 기능이 하나의 컨트롤러에서 처리되서 효율적이고 괜찮게 보일 수 있지만 몇가지 단점이 있다. 클래스의 코드는 적을수록 좋다. 하나의 클래스에 너무 많은 코드가 있다면 시간이 지나면서 늘어나는 코드 그 이상으로 파악하는 것에 난이도가 높아진다. 이것은 메서드를 아무리 깔끔하게 분리해놔도 쉽지 않다. 또한 테스트 코드를 작성하더라도 프로덕션 코드에 비해 더 추상적인 테스트 코드의 특성상 클래스 단위를 작게 해야 찾기 쉬워진다. 가장 중요한 부분은 모델의 재사용을 촉진한다는 점이다. 위 코드는 AccountResource 라는 단일 모델 클래스를 공유하는데 여기에 연산에 필요한 모든 데이터를 담다보면 특정 연산에선 필요없는 필드도 생길수 밖에 없다. 새로운 기능을 추가하거나 기존 기능을 수정할때 불필요한 필드에 대한 고민이 들어가면서 난이도만 늘어날뿐이다. 따라서 가급적 별도의 패키지 안에 별도의 컨트롤러를 만드는 방식이 좋다. 123456789101112131415@RestController@RequiredArgsConstructorpublic class SendMoneyController { private final SendMoneyUseCase; @PostMapping(&quot;/accounts/send/{sourceAccountId}/{targetAccountId}/{amount}&quot;) void sendMoney( @PathVariable(&quot;sourceAccountId&quot;) Long sourceAccountId, @PathVariable(&quot;targetAccountId&quot;) Long targetAccountId, @PathVaribale(&quot;amount&quot;) Long amount) { SendMoneyCommand command = new SendMoneyCommand(new AccountId(sourceAccountId), new AccountId(targetAccountId), Money.of(amount)); sendMoneyUseCase.sendMoney(command); }} 각 컨트롤러는 Create, Update와 같은 컨트롤러 자체의 모델을 가지고 있거나, 원시값을 받아도 된다. 이러한 전용모델 클래스는 컨트롤러의 패키지에 private 으로 선언할 수 있어서 다른 곳에서 재사용될 일이 없고 다른 컨트롤러에서 사용할때도 한번 더 생각해볼 수 있게 된다. 웹어댑터를 구현할 땐 HTTP요청이 어플리케이션의 유스케이스 호출로 변환하고 결과를 다시 HTTP로 변환하여 어떠한 도메인 로직도 수행하지 않도록 구현해야 한다.","link":"/2023/03/15/book/clean-architecture/clean-5/"},{"title":"6. 영속성 어댑터 구현하기","text":"전통적인 계층형 아키텍처에서는 결국 모든 것이 영속성 계층에 의존하게 되어 ‘데이터베이스 주도 설계’가 되는 문제가 있다. 이러한 의존성을 역전시키기 위해 영속성 계층을 애플리케이션 계층의 플러그인으로 만드는 방식으로 구현해야 한다. 의존성 역전 애플리케이션 서비스에서는 영속성 기능을 사용하기 위해 포트 인터페이스를 호출한다. 육각형 아키텍처에서 영속성 어댑터는 아웃고잉 어댑터이다. 애플리케이션에 의해 호출뿐, 애플리케이션을 호출하진 않기 때문이다. 포트는 애플리케이션 서비스와 영속성 코드 사이의 간접적인 계층으로 영속성 계층에 대한 코드 의존성을 없애기 위해 이러한 간접 계층이 추가되었다. 이제 영속성 코드를 리팩토링하더라도 코어 코드를 변경하는 결과로 이어지진 않는다. 영속성 어댑터의 책임 영속성 어댑터가 하는 일은 보통 아래와 같다. 입력을 받는다 데이터베이스 포맷으로 매핑 입력을 데이터베이스로 보낸다 데이터베이스 출력을 애플리케이션 포맷으로 매핑 출력을 반환한다. 중요한건 영속성 어댑터의 입력 모델이 영속성 어댑터 내부가 아닌 애플리케이션 코어에 있기 때문에 영속성 어댑터 내부를 변경하는 것이 코어에 영향을 미치지 않는다는 점이다. 출력 모델도 동일하게 애플리케이션 코어에 위치해야 한다. 포트 인터페이스 나누기 보통 특정 엔티티가 필요로 하는 모든 데이터베이스 연산을 하나의 리포지토리 인터페이스에 넣는 식으로 구현한다. 하지만 이러한 방식은 코드에 불필요한 의존성이 생기게 되는데 데이터베이스 연산에 의존하는 각 서비스는 인터페이스에 단 하나의 메서드나 사용하더라도 ‘넓은’ 포트 인터페이스에 의존성을 갖게 되는 문제가 발생한다. 맥락 상 필요하지 않는 메서드에 생긴 의존성은 코드를 이해하고 테스트하기 어렵게 만든다. 인터페이스 분리 원칙(Interface Segregation Principle, ISP)을 적용하여 클라이언트가 오직 자신이 필요로 하는 메서드만 알게 만들어 각각의 특화된 인터페이스로 분리해야 한다. 이렇게 되면 각 서비스는 실제로 필요한 메서드에만 의존하고, 포트의 이름은 역할을 명확하게 표현할 수 있으며 서비스 코드를 짤 때 필요한 포트에 연결만 하면 된다. 영속성 어댑터 나누기 위에서 나눈 포트 인터페이스처럼 영속성 어댑터도 한개만 만들라는 규칙은 없다. 예를 들면 영속성 연산이 필요한 도메인 클래스(DDD의 애그리거트) 하나당 하나의 영속성 어댑터를 구현할 수도 있다. 도메인 코드는 영속성 포트에 의해 정의된 명세를 어떤 클래스가 충족시키는지에 관심이 없다. 모든 포트가 구현돼 있기만 한다면 영속성 계층에서 하고 싶은 어떤 작업이든 해도 된다. 애그리거트당 하나의 영속성 어댑터 접근 방식 또한 나중에 여러 개의 바운디드 컨텍스트(bounded context)의 영속성 요구사항을 분리하기 위한 좋은 토대가 된다. 바운디드 컨텍스트 간의 경계를 명확하게 구분하고 싶다면 각 바운디드 컨텍스트가 영속성 어댑터를 하나씩 가지고 있어야 한다. account와 관련된 서비스가 billing과 관련된 영속성 어댑터에 접근하지 않아야 한다. 경계 너머의 다른 무언가가 필요하다면 전용 인커밍 포트를 통해 접근해야 한다. 스프링 데이터 JPA 예제 앞서 살펴본 Account 클래스는 유효한 상태의 Account 엔티티만 생성할 수 있는 팩터리 메서드를 제공하고 계좌 잔고 확인 등 유효성 검증을 모든 상태 변경 메서드에서 수행하기 때문에 유효하지 않은 도메인 모델을 생성할 수 없다. 즉, 최대한 불변성을 유지하려고 한다. 123456789101112131415161718192021222324252627package buckpal.domain;@AllArgsConstructor(access = AccessLevel.PRIVATE)public class Account{ @Getter private final AccountId id; @Getter private final ActivityWindow activityWindow; private final Money baselineBalance; public static Account withoutId(Money baselineBalance, ActivityWindow activityWindow) { return new Account(null, baselineBalance, activityWindow); } public static Account withId(AccountId accountId, Money baselineBalance, ActivityWindow activityWindow) { return new Account(accountId, baselineBalance, activityWindow); } public Money calculateBalance() { //... } public boolean withdraw(Money money, AccountId targetAccountId){ //... } public boolean deposit(Money money, AccountId sourceAccountId){ //... }} JPA를 사용하려면 계좌의 데이터베이스 상태를 표현하는 @Entity 어노테이션이 추가된 클래스가 필요하다. 123456789101112package buckpal.adapter.persistence;@Entity@Table(name = &quot;account&quot;)@Data@AllArgsConstructor@NoArgsConstructorclass AccountJpaEntity { @Id @GeneratedValue private Long id;} 123456789101112131415161718package buckpal.adapter.persistence;@Entity@Table(name = &quot;activity&quot;)@Data@AllArgsConstructor@NoArgsConstructorclass ActivityJpaEntity { @Id @GeneratedValue private Long id; @Column private LocalDataTime timestamp; @Column private Long ownerAccountId; @Column private Long sourceAccountId; @Column private Long targetAccountId; @Column private Long amount;} JPA의 @ManyToOne이나 @OneToMany 어노테이션을 이용해서 ActivityJpaEntity와 AccountJpaEntity를 연결해서 관계를 표현할 수도 있었지만 데이터베이스 쿼리에 부수효과가 생길 수 있어서 일단 제외했다. 1234567891011121314151617181920212223242526interface ActivityRepository extends JpaRepository&lt;ActivityJpaEntity, Long&gt; { @Query(&quot;select a from ActivityJpaEntity a &quot; + &quot;where a.ownerAccountId = :ownerAccountId &quot; + &quot;and a.timestamp &gt;= :since&quot;) List&lt;ActivityJpaEntity&gt; findByOwnerSince( @Param(&quot;ownerAccountId&quot;) Long ownerAccountId, @Param(&quot;since&quot;) LocalDateTime since); @Query(&quot;select sum(a.amount) from ActivityJpaEntity a &quot; + &quot;where a.targetAccountId = :accountId &quot; + &quot;and a.ownerAccountId = :accountId &quot; + &quot;and a.timestamp &lt; :until&quot;) Long getDepositBalanceUntil( @Param(&quot;accountId&quot;) Long accountId, @Param(&quot;until&quot;) LocalDateTime until); @Query(&quot;select sum(a.amount) from ActivityJpaEntity a &quot; + &quot;where a.sourceAccountId = :accountId &quot; + &quot;and a.ownerAccountId = :accountId &quot; + &quot;and a.timestamp &lt; :until&quot;) Long getWithdrawalBalanceUntil( @Param(&quot;accountId&quot;) Long accountId, @Param(&quot;until&quot;) LocalDateTime until);} 스프링 부트는 이 리포지토리를 자동으로 찾고 스프링 데이터는 실제 데이터베이스와 통신하는 인터페이스 구현체를 제공한다. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556@RequiredArgsConstructor@PersistenceAdapterclass AccountPersistenceAdapter implements LoadAccountPort, UpdateAccountStatePort { private final SpringDataAccountRepository accountRepository; private final ActivityRepository activityRepository; private final AccountMapper accountMapper; @Override public Account loadAccount( AccountId accountId, LocalDateTime baselineDate) { AccountJpaEntity account = accountRepository.findById(accountId.getValue()) .orElseThrow(EntityNotFoundException::new); List&lt;ActivityJpaEntity&gt; activities = activityRepository.findByOwnerSince( accountId.getValue(), baselineDate); Long withdrawalBalance = orZero(activityRepository .getWithdrawalBalanceUntil( accountId.getValue(), baselineDate)); Long depositBalance = orZero(activityRepository .getDepositBalanceUntil( accountId.getValue(), baselineDate)); return accountMapper.mapToDomainEntity( account, activities, withdrawalBalance, depositBalance); } private Long orZero(Long value){ return value == null ? 0L : value; } @Override public void updateActivities(Account account) { for (Activity activity : account.getActivityWindow().getActivities()) { if (activity.getId() == null) { activityRepository.save(accountMapper.mapToJpaEntity(activity)); } } }} 영속성 측면과의 타협 없이 풍부한 도메인 모델을 생성하고 싶다면 도메인 모델과 영속성 모델을 매핑하는 것이 좋다. 트랜잭션 경계는 어디에 위치해야 할까? 트랜잭션은 하나의 유스케이스에 대해서 일어나는 모든 쓰기 작업에 걸쳐 있어야 한다. 가장 쉬운건 @Transactional 어노테이션을 서비스 클래스에 붙여서 모든 public 메서드를 트랜잭션으로 감싸게 하는 것이다 도메인 코드에 플러그인처럼 동작하는 영속성 어댑터를 만들면 서로 분리되서 풍부한 도메인 모델을 만들 수 있고 포트의 명세만 지켜진다면 영속성 계층 전체를 교체할 수도 있다.","link":"/2023/03/25/book/clean-architecture/clean-6/"},{"title":"7. 아키텍처 요소 테스트하기","text":"기본 전제 만드는 비용이 적고 유지보수 하기 쉬워야 하며 빨리 실행되고 안정적인 작은 크기의 테스트들에 대해 높은 커버리지를 유지해야 한다. 테스트 피라미드 시스템 테스트 : 애플리케이션을 구성하는 모든 객체 네트워크를 가동시켜 특정 유스케이스가 전 계층에서 잘 동작하는지 검증 통합테스트: 연결된 여러 유닛을 인스턴스화하고 시작점이 되는 클래스의 인터페이스로 데이터를 보낸 후 유닛들의 네트워크가 기대한대로 잘 동작하는지 검증 단위테스트 : 하나의 클래스를 인스턴스화하고 해당 클래스의 인터페이스를 통해 기능들을 테스트 클린 아키텍처 기준 단위 테스트 : 특정 코드의 중요한 로직들이 의도한대로 별도의 외부 의존성 없이 제대로 돌아가는지 검증이 필요한 경우, 도메인 엔티티 테스트, 유스케이스 테스트 통합 테스트 : 웹 어댑터, 영속성 어댑터와 같이 외부 클라이언트와 상호작용을 통해 검증이 필요한 경우 시스템 테스트 : 전체 애플리케이션을 띄우고 API를 통해 요청을 보내고 모든 계층이 조화롭게 잘 동작하는지 검증 앞서 단위, 통합 테스트를 잘 구현했다면 시스템 테스트는 일부 겹치는 로직도 있지만 단위,통합만으로 알아차리지 못한 계층 간 매핑 버그 같은건 시스템 테스트를 통해서 알게되는 경우도 있다. 시스템 테스트를 통해 중요한 시나리오들을 모두 커버하면 배포할 준비가 된것이다. 단순히 라인 커버리지를 100%로 만드는 것을 목표로 테스트하는건 잘못된 지표이다. 처음 몇번의 배포는 믿음의 도약을 하고 이후 버그를 수정하고 이로부터 배우는 것을 목표로 삼는 다면 제대로 가는 것이다. “테스트가 이 버그를 왜 잡지 못했을까?” 를 생각하고 이에 대한 답변을 기록하고, 이 케이스를 커버할 수 있는 테스트를 추가해야 한다. 새로운 필드를 추가할때마다 테스트를 고치는데 한 시간을 써야 한다면 뭔가 잘못된것이다. 테스트가 구조적 변경에 너무 취약하여 리팩토링할 때마다 테스트 코드도 변경해야 한다면 테스트로서의 가치를 잃는다. 헥사고날 아키텍처는 도메인 로직과 바깥으로 향한 어댑터를 깔끔하게 분리하여 핵심 도메인 로직은 단위 테스트로, 어댑터는 통합 테스트로 처리하는 명확한 테스트 전략을 정의할 수 있다.","link":"/2023/04/02/book/clean-architecture/clean-7/"},{"title":"8. 경계 간 매핑하기","text":"앞서 웹, 애플리케이션, 도메인, 영속성 계층의 역할에 대해선 이야기 했으니 각 계층의 모델을 매핑하는 것에 대해서 이야기해보자. 매핑을 하지 않으면 양 계층에서 같은 모델을 사용하게 되면서 두 계층이 강하게 결합된다. 하지만 보일러플레이트 코드를 너무 많이 만들게 되는 단점이 발생하기도 한다. 매핑하기는 크게 4가지 전략이 있다. 매핑하지 않기 전략 계층들을 연결하는 포트 인터페이스가 도메인 모델을 입출력 모델로 사용하여 모든 계층이 같은 모델을 사용한다면 계층간 매핑이 전혀 필요없다. 하지만 이러한 전략은 특정 계층에 특별한 요구사항이 발생할 경우 그와 상관없는 모든 계층의 모델에 새로운 필드나 어노테이션이 추가될 수 있다. 도메인 모델이 다른 계층들과 관련된 이유로 변경되야 하므로 단일 책임 원칙을 위반한다. 하지만 간단한 CRUD 유스케이스같은 경우는 같은 필드를 가진 웹 모델을 도메인 모델로, 혹은 도메인 모델을 영속성 모델로 매핑할 필요가 없을 것이다. 모든 계층이 정확히 같은 구조의, 정확히 같은 정보를 필요로 한다면 매핑하지 않기 전략은 좋은 선택지가 될 수 있다. 양방향 매핑 전략 각 계층이 전용 모델을 가진 매핑 전략을 말한다. 각 어댑터가 전용 모델을 가지고 있어서 해당 모델을 도메인 모델로, 도메인 모델을 해당 모델로 매핑할 책임을 가지고 있다. 웹 계층의 컨트롤러는 웹 모델을 서비스와 연결되는 포트 인터페이스에서 도메인 모델로 매핑하고 반환된 도메인 객체를 다시 웹 모델로 매핑한다. 영속성 계층도 이와 유사하게 구현한다. 각 계층은 전용 모델을 가지고 있으므로 각 계층이 전용 모델을 변경하더라도 다른 계층에는 영향이 없다. 각 모델은 계층에 필요한 데이터, 유스케이스를 제일 잘 구현할 수 있고 도메인 모델은 웹이나 영속성의 관심사로 오염되지 않는 깨끗한 도메인 모델로 이어져서 단일 책임 원칙을 만족한다. 개념적으로 매핑하지 않기 다음으로 간단하여 매핑 책임이 명확한데 매핑보다 도메인 로직에 집중할 수 있다. 다만, 너무 많은 보일러플레이트 코드가 생기는데 두 모델 간 매핑을 구현하는 데 꽤 시간이 들고 매핑 프레임워크를 사용하여 내부 동작을 제네릭과 리플렉션 뒤로 숨길 경우 디버깅하기도 쉽지 않다. 또다른 단점으로 도메인 모델이 계층 경계를 넘어서 통신하는데 사용된다는 것이다. 인커밍, 아웃고잉 포트는 도메인 객체를 입력 파라미터와 반환값으로 사용하면서 바깥쪽 계층의 요구에 따라 변경에 취약해질 수 있다. 완전 매핑 전략 각 연산마다 모두 별도의 입출력 모델을 사용할 수 있다. 계층 경계를 넘어 통신할 때 도메인 모델을 사용하는게 아니라 각 작업에 특화된 전용 모델을 만드는 것이다. 당연히 한 계층을 다른 여러 개의 커맨드로 연결하게 되면 하나의 웹 모델과 도메인 모델 간의 매핑보다 더 많은 코드가 필요하지만 여러 유스케이스의 요구사항을 함께 다뤄야 하는 매핑에 비해 구현하고 유지보수 하기가 훨씬 쉽다. 완전 매핑 전략은 전역적으로 적용하기 보다는 웹 계층과 애플리케이션 계층 사이에 상태 변경 유스케이스의 경계를 명확하게 할 때 가장 좋다. 애플리케이션 계층과 영속성 계층 사이에는 매핑 오버헤드 때문에 추천하지 않는다. 단방향 매핑 전략 모든 계층의 모델들이 같은 인터페이스를 구현한다. 이 인터페이스는 관련있는 모든 필드에 대한 getter 메서드를 제공하여 도메인 모델의 상태를 캡슐화 한다. 웹, 영속성 계층으로 도메인 객체를 전달하고 싶으면 별도의 매핑 없이 할수 있다. 모든 계층이 바라보는 상태 인터페이스를 사용할지, 전용 모델로 매핑할지는 바깥 계층에서 정할 수 있다. 행동을 변경하는 것이 상태 인터페이스에 의해 노출돼 있지 않기 때문에 실수로 도메인 객체의 상태를 변경하는 일은 발생하지 않는다. 각 매핑 전략은 저마다의 장단점이 있때문에 한 전략을 전역적으로 사용하기 보다는 그때그때 상황에 맞는 전략을 구사해야 한다.","link":"/2023/04/02/book/clean-architecture/clean-8/"},{"title":"9. 애플리케이션 조립하기","text":"유스케이스, 웹 어댑터, 영속성 어댑터를 구현했으니 이것들이 동작하는 애플리케이션으로 조립해보자. 각 클래스를 그냥 필요할때 인스턴스화 하지 않는 이유는 코드의 의존성이 올바른 방향을 가리키게 하기 위해서다. 모든 의존성은 안쪽, 애플리케이션의 도메인 코드 방향으로 향해야 도메인 코드가 바깥 계층의 변경으로부터 안전하다. 헥사고날 아키텍처 스타일은 하나의 코드를 훨씬 더 테스트하기 쉽다. 한 클래스가 필요로 하는 모든 객체를 생성자로 전달할 수 있다면 실제 객체 대신 목으로 전달할 수 있고, 이렇게 되면 격리된 단위 테스트를 생성하기 쉬워진다. 가장 추천하는 건 객체 인스턴스를 생성할 책임을 모든 클래스에 대한 의존성을 가지는 설정 컴포넌트를 만드는 것이다. 각 계층의 어댑터 인스턴스 생성 HTTP요청, 데이터베이스 접근 등 외부 접근에 대한 전달을 보장 각 어댑터에 유스케이스 인스턴스 제공 그 외 설정 파일이나 설정 파라미터의 소스에도 접근해야 한다. 단일 책임 원칙을 위반하게 되지만 나머지 부분을 깔끔하게 유지하기 위해서 구성요소들을 연결하는 바깥쪽 컴포넌트가 필요하다. 스프링 프레임워크를 이용해서 애플리케이션을 조립한 결과물을 애플리케이션 컨텍스트(application context)라고 한다. 애플리케이션 컨텍스트는 애플리케이션을 구성하는 모든 객체(bean)을 포함한다. 일반적으로는 스프링의 클래스패스 스캐닝으로 조립하는데 클래스패스에서 접근 가능한 모든 클래스를 확인해서 @Component 어노테이션이 붙은 클래스를 찾아서 각 클래스의 객체를 생성한다. 이때 필요한 모든 필드를 인자로 받는 생성자가 필요한데 Lombok 라이브러리의 @RequiredArgsConstructor 어노테이션을 사용하면 모든 final 필드를 인자로 받는 생성자를 자동으로 만들수도 있다. 각 클래스들의 인스턴스를 만들어 애플리케이션 컨텍스트에 추가하게 되는데 어노테이션 기반이라 필요한 곳에 작성만 잘하면 손쉽게 애플리케이션을 조립할 수 있지만 몇가지 단점도 존재한다. 클래스에 프레임워크에 특화된 어노테이션을 붙여야 한다는 점에서 침투적이다. 일반적인 애플리케이션 개발에선 필요한 경우 한두개 정도는 용인되더라도 다른 개발자들이 사용할 라이브러리나 프레임워크를 만드는 입장이라면 사용자가 스프링 프레임워크의 의존성에 엮이게 되서 쓰지 말아야 한다. 스프링 전문가가 아니라면 문제가 발생했을 때 원인을 찾는데 많은 시간이 소요될 수 있다. 이는 클래스패스 스캐닝이 단순히 스프링에게 부모 패키지를 알려 준 후 이 패키지 안에서 @Compoment가 붙은 모든 클래스를 찾는 방법이기 때문이다. 애플리케이션 내 모든 클래스를 하나하나 다 알기 어렵다. 좀 더 제어하기 쉬운 대안으로 스프링의 Java Config로 조립하는 방법이 있다. 이 방식은 애플리케이션 컨텍스트에 추가할 빈을 생성하는 설정 클래스를 만든다. @Configuration 어노테이션을 사용하여 설정 클래스임을 표시하여 클래스패스 스캐닝을 사용하는 방식은 모든 빈을 가져오는 대신 설정 클래스만 선택하기 때문에 제어가 한결 쉽다. 빈 자체는 설정 클래스 내의 @Bean 어노테이션이 붙은 팩토리 메서드를 통해 생성한다. @EnabledJpaRepositories 어노테이션을 사용하여 스프링 부트가 정의된 모든 스프링 데이터 리포지토리 인터페이스의 구현체를 제공할 것이다. 이 어노테이션은 설정 클래스가 아니라 메인 애플리케이션에도 붙일 수 있지만 그럴 경우 애플리케이션이 시작할 때마다 JPA를 활성화해서 영속성이 실질적으로 필요없는 테스트에서 애플리케이션을 실행할 때도 JPA 리포지토리들을 활성화 할 것이다. 따라서 이러한 기능 어노테이션은 별도의 설정 모듈에 있는 것이 한꺼번에 모든 것이 시작할 필요가 없어져서 애플리케이션을 더 유연하게 만들어 준다. 이러한 방식은 일반적인 클래스패스 스캐닝 방식에 비해 @Component 어노테이션을 코드 여기저기에 붙이도록 강제하지 않아서 애플리케이션 계층을 스프링 프레임워크에 대한 의존성 없이 깔끔하게 유지할 수 있다. 다만, 설정 클래스가 생성하는 빈이 설정 클래스와 같은 패키지에 존재하지 않는다면 이 빈들을 public 으로 만들어야 한다.","link":"/2023/04/02/book/clean-architecture/clean-9/"},{"title":"1.SpringBoot 시작하기","text":"솔루션 회사를 몇년 다니다보니 내가 생각하는 웹서비스 환경의 경험이 적다는 생각이 들어서 A to Z까지 천천히 따라가면서 조금이나마 경험을 늘릴 수 있는 계기가 되지 않을까 싶어서 서적을 하나 구매해보았다. 이하 포스팅할 내용은 모두 스프링 부트와 AWS로 혼자 구현하는 웹 서비스라는 책을 읽고 작성한 내용으로 학습한 내용을 정리하기 위함이다. 개발환경은 책과 좀 다르다. 시간도 지났고 기존에 사용하던 환경이 있으므로 openJDK 11 Gradle 6.7 IntelliJ 유료버전 1. 인텔리제이로 스프링 부트 시작하기 이미 인텔리제이는 사용하고 있지만 책에서 언급한 이클립스에 비해 인텔리제이가 가진 장점은 다음과 같다. 강력한 추천 기능(Smart Completion) 훨씬 더 다양한 리팩토링과 디버깅 기능 이클립스의 깃(Git)에 비해 훨씬 높은 자유도 프로젝트 시작할 때 인덱싱을 하여 파일을 비롯한 자원들에 대한 빠른 검색 속도 HTML과 CSS, JS, XML에 대한 강력한 기능 지원 자바, 스프링 부트 버전업에 맞춘 빠른 업데이트 인텔리제이는 무료버전과 유료버전이 모두 존재하지만 커뮤니티(무료) 버전만 사용하더라도 개발에 큰 지장은 없다. 자바 개발에 대한 모든 기능 및 Maven, Gradle과 같은 빌드 도구도 모두 지원한다. 여담으로 둘다 써본 경험에서 불편했던 점은 딱 한가지였는데 임베디드 톰캣이 아닌 외부 톰캣과 연동하는 경우 커뮤니티 버전에선 공식적으로 지원하지 않아서 별도의 플러그인을 설치해서 사용했는데 그게 좀 귀찮았던 기억이 있다. 그리고 이클립스를 쓰다가 인텔리제이로 넘어오면 가장 당황하는 것이 워크스페이스가 없이 프로젝트와 모듈의 개념만 있다는 점이다. 이 말은 인텔리제이는 한번에 하나의 프로젝트만 열린다는 점이다. Gradle로 프로젝트 생성 ArtifactId는 프로젝트의 이름이 된다 그동안 Maven만 사용해봤는데 Gradle이 가진 장점과 단점은 무엇인지 추후에 찾아봐서 포스팅 해봐야겠다. 시간이 지나면서 버전이 바뀐 영향인지 프로젝트 생성부터 책과 약간 다르게 진행이 되긴 하는데 또 그래야 더 찾아보고 공부가 되지 않을까 하는 생각도 들었다. Gradle 프로젝트 생성 완료 Gradle 프로젝트를 springBoot 프로젝트로 변경하기 초기 build.gradle 파일 12345678910111213141516171819plugins { id 'java'}group 'com.springboot.service'version '1.0-SNAPSHOT'repositories { mavenCentral()}dependencies { testImplementation 'org.junit.jupiter:junit-jupiter-api:5.6.0' testRuntimeOnly 'org.junit.jupiter:junit-jupiter-engine'}test { useJUnitPlatform()} 음… 여기서부터 책과 벌써 다르다. 아무래도 책의 출판시점에서 2년이나 지났으니 각종 라이브러리, 도구들의 버전업이 일어나면서 여러 내용들이 바뀐듯 하다. 구글링을 해보니 저자분이 [2020.12.16] 기준으로 최신 라이브러리로 버전업한 내용에 대해 정리해놓은 글이 있어서 해당 내용을 참고하였다. 참고링크 : 스프링 부트와 AWS로 혼자 구현하는 웹 서비스 (2020.12.16) 변경된 도구들의 버전은 다음과 같다. 라이브러리,도구명 출판버전 웹버전 Spring Boot 2.1.7 2.4.1 Gradle 4.8~4.10 6.7.1 JUnit 4 5","link":"/2021/04/21/book/springboot-aws-webservice/1/"},{"title":"2.테스트코드 작성하기","text":"JUnit4 -&gt; 5 변경점 12345678910111213141516171819202122232425262728293031323334353637383940414243junit4 -&gt; 5 1. @Test 패키지 위치 변경org.junit.Test-&gt; org.junit.jupiter.api.Test 2. @RunWith Junit5에서 @ExtendWith 로 변경되서 어노테이션명과 패키지위치 변경org.junit.runner.RunWith-&gt; org.junit.jupiter.api.extension.ExtendWith@RunWith-&gt; @ExtendWith 3. SpringRunner SpringExtension 으로 변경되서 클래스명과 패키지위치 변경SpringRunner-&gt; SpringExtensionorg.springframework.test.context.junit4.SpringRunner-&gt; org.springframework.test.context.junit.jupiter.SpringExtension 4. @After 테스트 메소드가 끝날때마다 수행되는 @After 도 Junit5에서 @AfterEach 로 변경되었기 때문에 어노테이션과 패키지위치 변경@After-&gt; @AfterEachorg.junit.After-&gt; org.junit.jupiter.api.AfterEach 5. @Before 마찬가지로 @BeforeEach 로 변경되서 어노테이션과 패키지위치 변경@Before-&gt; @BeforeEachorg.junit.Before-&gt; org.junit.jupiter.api.BeforeEach 샘플 컨트롤러 코드 12345678910111213141516package com.jojoldu.book.springboot.web;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.RestController;//컨트롤러를 JSON을 반환하는 컨트롤러로 만들어준다.@RestControllerpublic class HelloController { //HTTP Method인 Get의 요청을 받을 수 있는 API를 만들어 준다. @GetMapping(&quot;/hello&quot;) public String hello() { return &quot;hello&quot;; }} 샘플 단위 테스트 코드 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768package com.jojoldu.book.springboot.web;import org.junit.jupiter.api.Test;import org.junit.jupiter.api.extension.ExtendWith;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.boot.test.autoconfigure.web.servlet.WebMvcTest;import org.springframework.test.context.junit.jupiter.SpringExtension;import org.springframework.test.web.servlet.MockMvc;import static org.springframework.test.web.servlet.request.MockMvcRequestBuilders.get;import static org.springframework.test.web.servlet.result.MockMvcResultMatchers.content;import static org.springframework.test.web.servlet.result.MockMvcResultMatchers.status;/*1. @ExtendWith(SpringExtension.class)- 테스트를 진행할 때 JUnit에 내장된 실행자 외에 다른 실행자를 실행시킨다.- 여기서는 SpringExtension 이라는 스프링 실행자를 사용한다.- 스프링 부트 테스트와 JUnit 사이에 연결자 역할- JUnit4 -&gt; 5로 넘어오면서 사용하는 어노테이션과 클래스가 각각 @RunWith -&gt; @ExtendWith 로 SpringRunner -&gt; SpringExtension 으로 변경되었다.2. @WebMvcTest- 여러 스프링 테스트 어노테이션 중, Web(Spring MVC)에 집중할 수 있는 어노테이션- 선언할 경우 @Controller, @ControllerAdvice 등을 사용할 수 있다.- 단, @Service, @Component, @Repository 등은 사용할 수 없다.*/@ExtendWith(SpringExtension.class)//1@WebMvcTest(controllers = HelloController.class)//2public class HelloControllerTest { /* 3. AutoWired - 스프링이 관리하는 빈(Bean)을 주입 받는다. 4. private MockMvc mvc - 웹 API를 테스트할 때 사용한다. - 스프링 MVC 테스트의 시작점 - 이 클래스를 통해 HTTP GET, POST 등에 대한 API 테스트를 할 수 있다. */ @Autowired//3 private MockMvc mvc;//4 @Test public void hello가_리턴된다() throws Exception { String hello = &quot;hello&quot;; /* 5. mvc.perform(get(&quot;/hello&quot;)) - MockMvc를 통해 /hello 주소로 HTTP GET 요청을 한다. - 체이닝이 지원되어 아래와 같이 여러 검증 기능을 이어서 선언할 수 있다. 6. .andExpect(status().isOk()) - mvc.perform의 결과를 검증한다. - HTTP Header의 Status를 검증한다. - 우리가 흔히 알고 있는 200, 404, 500 emddml 상태를 검증한다. - 여기선 OK 즉, 200인지 아닌지를 검증한다. 7. .andExpect(content().string(hello)) - mvc.perform의 결과를 검증한다. - 응답 본문의 내용을 검증한다. - Controller에서 &quot;hello&quot;를 리턴하기 때문에 이 값이 맞는지 검증한다. */ mvc.perform(get(&quot;/hello&quot;))//5 .andExpect(status().isOk())//6 .andExpect(content().string(hello));//7 }} 2.3 롬북(Lombok) 자바 개발 시 자주 사용하는 코드 Getteer, Setter, 기본생성자, toString 등을 어노테이션으로 자동 생성해준다. 1234dependencies { // lombok implementation('org.projectlombok:lombok')}","link":"/2021/04/21/book/springboot-aws-webservice/2/"},{"title":"3.JPA로 데이터베이스 다루기","text":"3 JPA Posts 클래스는 실제 DB의 테이블과 매칭될 클래스로 Entity 클래스라고 부른다. DB데이터에 작업할 경우 실제 쿼리를 날리기 보단 이 Entity 클래스의 수정을 통해 작업 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465package com.jojoldu.book.springboot.domain.posts;import lombok.Builder;import lombok.Getter;import lombok.NoArgsConstructor;import javax.persistence.*;/*1. @Entity- 테이블과 링크될 클래스- 기본값으로 클래스의 카멜케이스 이름을 언더스코어 네이밍(_)으로 테이블 이름을 매칭한다.- ex) SalesManager.java -&gt; sales_manager table아래 3개는 lombok 어노테이션2. @NoArgsConstructor- 기본생성자 자동추가- public Posts() {} 와 같음3. @Getter- 클래스 내 모든 필드의 Getter 메소드 자동생성4. @Builder- 해당 클래스의 빌더 패턴 클래스를 생성- 생성자 상단에 선언 시 생성자에 포함된 필드만 빌더에 포함*/@Getter@NoArgsConstructor@Entitypublic class Posts { /* 1. @Id - PK 필드 2. @GeneratedValue - PK 생성규칙 - GenerationType.IDENTITY 옵션을 추가해야 auto_increment 된다. */ @Id @GeneratedValue(strategy = GenerationType.IDENTITY) private Long id; /* 1. @Column - 선언하지 않더라도 해당 클래스의 필드는 모두 컬럼이 된다. - 옵션을 추가할 때만 선언해도 된다. */ @Column(length = 500, nullable = false) private String title; @Column(columnDefinition = &quot;TEXT&quot;, nullable = false) private String content; private String author; @Builder public Posts(String title, String content, String author){ this.title = title; this.content = content; this.author = author; }} Entity 클래스는 Setter 메소드를 만들지 않고 해당 필드의 값 변경이 필요하면 명확히 그 목적과 의도를 나타낼 수 있는 메소드를 추가해야 한다. 12345678public class Order{ public void cancelOrder(){ this.status = false; }}public void 주문서비스의_취소이벤트() { order.cancelOrder();} 기본적인 구조는 생성자를 통해 최종값을 채운 후 DB에 삽입하는 것이고 값 변경이 필요하면 해당 이벤트에 맞는 public 메소드를 호출하여 변경하는 것을 전제로 한다. 생성시점에 값을 넣는 방법으로 생성자와 @Builder를 통해 제공되는 빌더 클래스를 사용할 수 있는데 차이점이 있다면 생성자는 new Example(b,a)처럼 파라미터의 위치를 바꾸더라도 실제 실행 전까지 문제를 찾기 어렵다. 1234public Example(String a, String b){ this.a = a; this.b = b;} 빌더를 사용한다면 어느 필드에 어떤 값을 채워야할 지 명확하게 인지할 수 있다. 1234Example.builder() .a(a) .b(b) .build(); Posts클래스(Entity) 생성이 끝나면 해당 클래스로 Database를 접근하게 해줄 JpaRepository를 생성한다. src/main/java/com/jojoldu/book/springboot/domain/posts/PostsRepository 1234import org.springframework.data.jpa.repository.JpaRepository;public interface PostsRepository extends JpaRepository&lt;Posts, Long&gt; {} 보통 ibatis나 MyBatis 등에서 Dao라고 불리는 DB Layer 접근자를 JPA에선 Repository 라고 부르며 인터페이스로 생성한다. 생성 후, JpaRepository&lt;Entity 클래스, PK 타입&gt; 를 상속하면 기본적인 CRUD 메소드가 자동으로 생성된다. @Repositry 어노테이션을 추가할 필요는 없지만 Entity 클래스와 기본 Entity Repository는 함께 위치해야 한다. 생성한 Repository 를 테스트하기 위해 아래 코드를 작성하며 테스트할 기능은 save, findAll 기능이다. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465package com.jojoldu.book.springboot.domain.posts;import org.junit.jupiter.api.AfterEach;import org.junit.jupiter.api.Test;import org.junit.jupiter.api.extension.ExtendWith;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.boot.test.context.SpringBootTest;import org.springframework.test.context.junit.jupiter.SpringExtension;import java.util.List;import static org.assertj.core.api.Assertions.assertThat;/*별다른 설정없이 @SpringBootTest를 사용할 경우 H2 데이터베이스를 자동으로 실행한다.*/@ExtendWith(SpringExtension.class)@SpringBootTestpublic class PostsRepositoryTest { @Autowired PostsRepository postsRepository; /* 1. @AfterEach - Junit에서 단위테스트가 끝날 때마다 수행되는 메소드를 지정한다. - 보통은 배포 전 전체 테스트를 수행할 때 테스트간 데이터 침범을 막기 위해 사용된다. - Junit4 -&gt; 5로 넘어가면서 After -&gt; AfterEach 로 변경되었다. */ @AfterEach public void cleanup() { postsRepository.deleteAll(); } @Test public void 게시글저장_불러오기() { //given String title = &quot;테스트 게시글&quot;; String content = &quot;테스트 본문&quot;; /* 2. postsRepository.save - 테이블 posts에 insert/update 쿼리를 실행한다. - id값이 있으면 update, 없다면 insert 쿼리가 실행된다. */ postsRepository.save(Posts.builder() .title(title) .content(content) .author(&quot;jojoldu@gmail.com&quot;) .build()); /* 3. postsRepository.findAll - 테이블 posts에 있는 모든 데이터를 조회해오는 메소드 */ //when List&lt;Posts&gt; postsList = postsRepository.findAll(); //then Posts posts = postsList.get(0); assertThat(posts.getTitle()).isEqualTo(title); assertThat(posts.getContent()).isEqualTo(content); }} 3.4 등록/수정/조회 API 만들기 API를 만들기 위해 총 3개의 클래스가 필요하다. Request 데이터를 받을 Dto API 요청을 받을 Controller 트랜잭션, 도메인 기능 간의 순서를 보장하는 Service 여기서 Service는 비지니스 로직을 처리하는 것이 아니라 트랜잭션, 도메인 간 순서 보장의 역할만 한다. Web, Service, Repository, Dto, Domain 이 5가지 레이어에서 비지니스 처리를 담당해야 할 곳은 **Domain**이다. 기존에 서비스로 처리하던 방식을 트랜잭션 스크립트라고 한다. 1234567891011121314151617181920212223242526@Transactionalpublic Order cancelOrder(int orderId){ //1) 데이터베이스로부터 주문정보, 결제정보, 배송정보 조회 OrdersDto order = ordersDao.selectOrders(orderId); BillingDto billing = billingDao.selectBilling(orderId); DeliveryDto delivery = deliveryDao.selectDelivery(orderId); //2) 배송 취소를 해야하는지 상태값 확인 String deliveryStatus = delivery.getStatus(); //3) 만약 배송중이라면 배송취소로 변경 if(&quot;IN_PROGRESS&quot;.equals(deliveryStatus)){ delivery.setStatus(&quot;CANCEL&quot;); deliveryDao.update(delivery); } //4) 각 테이블에 취소 상태 Update order.setStatus(&quot;CANCEL&quot;); orderDao.update(order); billing.setStatus(&quot;CANCEL&quot;); deliveryDao.update(billing); return order;} 모든 로직이 서비스 클래스 내부에서 처리된다면 서비스 계층이 무의미하며, 객체란 단순히 데이터 덩어리 역할만 하게 된다. 반면 도메인 모델에서 처리할 경우 아래와 같은 코드가 될 수 있다. 1234567891011121314151617@TransactionalPublic Order cancelOrder(int orderId){ //1) OrdersDto order = ordersDao.selectOrders(orderId); BillingDto billing = billingDao.selectBilling(orderId); DeliveryDto delivery = deliveryDao.selectDelivery(orderId); //2-3) delivery.cancel(); //4) order.cancel(); billing.cancel(); return order;} order, billing, delivery가 각자 본인의 취소 이벤트 처리를 하며, 서비스 메소드는 트랜잭션과 도메인 간의 순서만 보장해 준다. 스프링에서 Bean을 주입하는 방식은 다음과 같다. @Autowired setter 생성자 가장 권장하는 방식은 생성자로 주입받는 방식이며 @Autowired는 권장하지 않는다. 아래 Service 코드에서 생성자는 직접 쓰지 않고 @RequiredArgsConstructor어노테이션에서 해결해 준다. final이 선언된 모든 필드를 인자값으로 하는 생성자를 롬북에서 대신 생성해준다. 이처럼 어노테이션을 사용하는 이유는 해당 클래스의 의존성 관계가 변경될 때마다 생성자 코드를 변경하는 수고를 덜기 위함이다. 1234567891011121314151617import com.jojoldu.book.springboot.domain.posts.PostsRepository;import com.jojoldu.book.springboot.web.dto.PostsSaveRequestDto;import lombok.RequiredArgsConstructor;import org.springframework.stereotype.Service;import org.springframework.transaction.annotation.Transactional;@RequiredArgsConstructor@Servicepublic class PostsService { private final PostsRepository postsRepository; @Transactional public Long save(PostsSaveRequestDto requestDto) { return postsRepository.save(requestDto.toEntity()).getId(); }} Controller와 Service에서 사용할 Dto 클래스는 언듯 Entity 클래스와 유사한 형태지만 추가로 생성해야한다. 즉, 절대로 Entity 클래스를 Request/Response 클래스로 사용해선 안된다. 이유는 Entity 클래스는 데이터베이스와 맞닿은 핵심 클래스로 화면 변경은 사소한 변경인데 이를 건들기 위해 테이블과 연결된 Entity 클래스를 변경하는 것은 너무 큰 변경이다. 수많은 서비스 클래스나 비즈니스 로직들이 Entity 클래스를 기준으로 동작하며 Entity 클래스가 변경되면 여러 클래스에 영향을 끼지지만 Request/Response 용 Dto는 View를 위한 클래스라 자주 변경이 필요하다. 이처럼 View Layer 와 DB Layer 의 역할을 철저히 분리하는게 좋다. 예를 들어 Controller에서 결과값으로 여러 테이블을 조인해야 하는 경우 Entity 클래스만으로 표현하기 어려운 경우도 있다. 다음은 JPA를 사용한 게시판의 등록 API 테스트 코드이다. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960package com.jojoldu.book.springboot.web;import com.jojoldu.book.springboot.domain.posts.Posts;import com.jojoldu.book.springboot.domain.posts.PostsRepository;import com.jojoldu.book.springboot.web.dto.PostsSaveRequestDto;import org.junit.jupiter.api.AfterEach;import org.junit.jupiter.api.Test;import org.junit.jupiter.api.extension.ExtendWith;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.boot.test.context.SpringBootTest;import org.springframework.boot.test.web.client.TestRestTemplate;import org.springframework.boot.web.server.LocalServerPort;import org.springframework.http.HttpStatus;import org.springframework.http.ResponseEntity;import org.springframework.test.context.junit.jupiter.SpringExtension;import java.util.List;import static org.assertj.core.api.Assertions.assertThat;@ExtendWith(SpringExtension.class)@SpringBootTest(webEnvironment = SpringBootTest.WebEnvironment.RANDOM_PORT)public class PostsApiControllerTest { @LocalServerPort private int port; @Autowired private TestRestTemplate restTemplate; @Autowired private PostsRepository postsRepository; @AfterEach public void tearDown() throws Exception { postsRepository.deleteAll(); } @Test public void Posts_등록된다() throws Exception { //given String title = &quot;title&quot;; String content = &quot;content&quot;; PostsSaveRequestDto requestDto = PostsSaveRequestDto.builder().title(title).content(content).author(&quot;author&quot;).build(); String url = &quot;http://localhost:&quot; + port + &quot;/api/v1/posts&quot;; //when ResponseEntity&lt;Long&gt; responseEntity = restTemplate.postForEntity(url, requestDto, Long.class); //then assertThat(responseEntity.getStatusCode()).isEqualTo(HttpStatus.OK); assertThat(responseEntity.getBody()).isGreaterThan(0L); List&lt;Posts&gt; all = postsRepository.findAll(); assertThat(all.get(0).getTitle()).isEqualTo(title); assertThat(all.get(0).getContent()).isEqualTo(content); }} HelloController와 달리 @WebMvcTest를 사용하지 않는데 @WebMvcTest의 경우 JPA기능이 작동하지 않기 때문인데 Controller와 ControllerAdvice 등 외부 연동과 관련된 부분만 활성화되니 지금 처럼 JPA 기능까지 한번에 테스트할 때는 @SpringBootTest 와 TestRestTemplate을 사용하면 된다. JPA를 사용할 때 update 기능에서 데이터베이스에 쿼리를 날리는 부분이 없다. 이게 가능한 이유는 JPA의 영속성 컨텍스트 때문이다. 영속성 컨텍스트란, 엔티티를 영구 저장하는 환경으로 일종의 논리적 개념이라고 보면 되며 JPA의 핵심 내용은 엔티티가 영속성 컨텍스트에 포함되어 있냐 아니냐로 갈린다. JPA의 엔티티 매니저가 활성화된 상태로(Spring Data Jpa를 쓴다면 기본 옵션) 트랜잭션 안에서 데이터베이스에서 데이터를 가져오면 이 데이터는 영속성 컨텍스트가 유지된 상태이다. 이 상태에서 해당 데이터의 값을 변경하면 트랜잭션이 끝나는 시점에 해당 테이블에 변경분을 반영한다. 즉, Entity 객체의 값만 변경하면 별도로 Update쿼리를 날릴 필요가 없는데 이를 더티 체킹(dirty checking)이라고 한다. 3.5 JPA Auditing으로 생성시간/수정시간 자동화하기 보통 엔티티에는 해당 데이터의 생성시간과 수정시간을 포함하는데 반복적인 코드를 모든 테이블과 서비스 메소드에 포함하면 너무 귀찮으니 JPA Auditing를 사용해보자. LocalDate 사용 Java8부터 LocalDate와 LocalDateTime이 등장하여 그간 Java의 기본 날짜 타입인 Date의 문제점을 제대로 고쳤으니 꼭 사용하자. BaseTimeEntity.java 123456789101112131415161718192021import lombok.Getter;import org.springframework.data.annotation.CreatedDate;import org.springframework.data.annotation.LastModifiedDate;import org.springframework.data.jpa.domain.support.AuditingEntityListener;import javax.persistence.EntityListeners;import javax.persistence.MappedSuperclass;import java.time.LocalDateTime;@Getter@MappedSuperclass@EntityListeners(AuditingEntityListener.class)public abstract class BaseTimeEntity { @CreatedDate private LocalDateTime createDate; @LastModifiedDate private LocalDateTime modifiedDate;} BaseTimeEntity클래스는 모든 Entity의 상위 클래스가 되어 Entity들의 createdDate, modifiedDate를 자동으로 관리하는 역할이다. @MappedSuperclass JPA Entity 클래스들이 BaseTimeEntity을 상속할 경우 필드들(createdDate, modifiedDate)도 컬럼으로 인식하도록 한다. @EntityListeners(AuditingEntityListener.class) BaseTimeEntity 클래스에 Auditing 기능을 포함시킨다. CreatedDate Entity가 생성되어 저장될 때 시간이 자동 저장된다. LastModifiedDate 조회한 Entity의 값을 변경할 때 시간이 자동 저장된다. 이후 앞서 만든 Posts 클래스가 BaseTimeEntity를 상속받도록 변경한다. 1234 ...public class Posts extends BaseTimeEntity { ...} 마지막으로 JPA Auditing 어노테이션들을 모두 활성화할 수 있도록 Application 클래스에 활성화 어노테이션을 추가한다. 12345678@EnableJpaAuditing // JPA Auditing 활성화@SpringBootApplicationpublic class Application { public static void main(String[] args) { SpringApplication.run(Application.class, args); }}","link":"/2021/04/22/book/springboot-aws-webservice/3/"},{"title":"4.머스테치로 화면 구성하기","text":"4. 머스테치로 화면 구성하기 4.1 서버 템플릿 엔진 템플릿 엔진이란, 지정된 템플릿 양식과 데이터가 합쳐져 HTML문서를 출력하는 소프트웨어를 이야기한다. 서버 템플릿 엔진을 이용한 화면 생성은 서버에서 Java 코드로 문자열을 만든 뒤 이 문자열을 HTML로 변환하여 브라우저로 전달한다. 반면 클라이언트 템플릿 엔진(Vue, React 등)을 이용한 SPA(Single Page Application)은 브라우저에서 화면을 생성한다. 즉, 서버에서 이미 코드가 벗어난 경우라서 서버에서는 Json 혹은 Xml 형식의 데이터만 전달하고 클라이언트에서 조립한다. 최근엔 리액트나 뷰와 같은 자바스크립트 프레임워크에서 서버사이드렌더링을 지원하는 모습을 볼 수 있지만 그건 나중에 생각하자. 머스테치 머스테치는 많은 언어를 지원하는 심플한 템플릿 엔진이다. 스프링 부트에서 공식 지원하는 템플릿 엔진으로 gradle에 의존성 한줄 추가하면 바로 사용할 수 있다. 파일위치는 기본적으로 src/main/resources/templates이며 이 위치에 머스테치 파일을 두면 스프링 부트에서 자동으로 로딩한다. 해당 위치에 index.mustache를 생성한 후 이 머스테치에 URL을 매핑하는데 이는 Controller에서 진행한다. IndexController 12345678910111213import org.springframework.stereotype.Controller;import org.springframework.web.bind.annotation.GetMapping;@Controllerpublic class IndexController { @GetMapping(&quot;/&quot;) public String index() { return &quot;index&quot;; }} 머스테치 스타터 의존성을 추가했기 때문에 컨트롤러에서 문자열을 반환할 때 앞의 경로와 뒤의 파일 확장자는 자동으로 지정된다. 즉, 여기선 &quot;index&quot;를 반환하므로 src/main/resources/templates/index.mustache로 전환되어 View Resolver가 처리하게 된다. (View Resolver는 URL 요청의 결과를 전달할 타입과 값을 지정하는 관리자 격으로 볼 수 있다.) 화면 구성 시 bootstrap을 사용하는데 공통된 부분에 대해선 layout을 따로 둬서 header와 footer 파일을 각각 만들어서 공통된 코드는 해당 위치에 생성한다. 여기서 페이지 로딩속도를 높이기 위해 css는 header에, js는 footer에 두는데 HTML은 위에서부터 코드가 실행되기 때문에 head가 다 실행되고서야 body가 실행된다. 즉, head가 다 불러지지 않으면 사용자 쪽에선 백지 화면만 노출되며 특히 js의 용량이 크면 클수록 body 부분의 실행이 늦어지기 때문에 js는 body 하단에 두어 화면이 다 그려진 뒤에 호출하는 것이 좋다. header와 footer를 index에 추가하는건 아래와 같다. 12345678910{{&gt;layout/header}}&lt;h1&gt;스프링 부트로 시작하는 웹 서비스&lt;/h1&gt;&lt;div class=&quot;col-md-12&quot;&gt; &lt;div class=&quot;row&quot;&gt; &lt;div class=&quot;col-md-6&quot;&gt; &lt;a href=&quot;/posts/save&quot; role=&quot;button&quot; clas=&quot;btn btn-primary&quot;&gt;글 등록&lt;/a&gt; &lt;/div&gt; &lt;/div&gt;&lt;/div&gt;{{&gt;layout/footer}} {{> }} 는 현재 머스테치 파일을 기준으로 다른 파일을 가져온다. 화면의 버튼에 API를 호출하는 js파일을 작성하여 footer.mustache에 추가한다. 1234567&lt;script src=&quot;https://code.jquery.com/jquery-3.3.1.min.js&quot;&gt;&lt;/script&gt;&lt;script src=&quot;https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js&quot;&gt;&lt;/script&gt;&lt;!--index.js 추가--&gt;&lt;script src=&quot;/js/app/index.js&quot;&gt;&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; index.js 호출 코드는 절대경로(/)로 바로 시작하는데 스프링 부트는 기본적으로 src/main/resources/static에 위치한 자바스크립트, CSS, 이미지 등 정적 파일들은 URL에서 / 로 설정된다. PostsRepository 인터페이스 123456public interface PostsRepository extends JpaRepository&lt;Posts, Long&gt; { @Query(&quot;SELECT p FROM Posts p ORDER BY p.id DESC&quot;) List&lt;Posts&gt; findAllDesc();} @Query 어노테이션을 사용하면 SpringDataJpa에서 제공하지 않는 메소드를 쿼리로 직접 작성할 수 있다. 보통 규모가 있는 프로젝트에선 데이터 조회는 FK의 조인, 복잡한 조건 등으로 인해 Entity 클래스만으로 처리가 어려워 조회용 프레임워크를 추가로 사용한다. 대표적 예로 querydsl, jooq, MyBatis 등이 있는데 해당 프레임워크 중 하나로 조회를 하고 그 외 등록/수정/삭제 등은 SpringDataJpa를 통해 진행한다. PostsService 123456... @Transactional(readOnly = true) public List&lt;PostsListResponseDto&gt; findAllDesc() { return postsRepository.findAllDesc().stream().map(PostsListResponseDto::new).collect(Collectors.toList()); }... @Transactional 어노테이션에 추가된 readOnly 옵션을 true로 주면 트랜잭션 범위는 유지하되, 조회기능만 남겨두어 조회 속도가 개선되기 때문에 등록, 수정, 삭제 기능이 전혀 없는 서비스 메소드에서 사용하는 것을 추천한다. IndexController 123456789101112@RequiredArgsConstructor@Controllerpublic class IndexController { private final PostsService postsService; @GetMapping(&quot;/&quot;) public String index(Model model) { model.addAttribute(&quot;posts&quot;, postsService.findAllDesc()); return &quot;index&quot;; }} Model 서버 템플릿 엔진에서 사용할 수 있는 객체를 저장할 수 있다. 여기서는 postsService.findAllDesc()로 가져온 결과를 posts로 index.mustache에 전달한다. REST에서 CURD는 다음과 같이 HTTP Method에 매핑된다. 생성(Create) : POST 읽기(Read) : GET 수정(Update) : PUT 삭제(Delete) : DELETE","link":"/2021/04/22/book/springboot-aws-webservice/4/"},{"title":"[WebFlux] publishOn과 subscribeOn","text":"회사에서 WebFlux로 개발하면서 publishOn과 subscribeOn의 사용법이 매번 헷갈려서 최대한 간단하게 정리를 해봤다. 123456789101112131415Flux.range(1,3) .subscribeOn(Schedulers.newBoundedElastic(1,1,&quot;sub2&quot;)) .doOnNext(i -&gt; log.info(&quot;main 1: {}, number: {}&quot;, Thread.currentThread().getName(), i)) .publishOn(Schedulers.newBoundedElastic(1,1,&quot;pub1&quot;)) .doOnNext(i -&gt; log.info(&quot;main 2: {}, number: {}&quot;, Thread.currentThread().getName(), i)) .subscribeOn(Schedulers.newBoundedElastic(1,1,&quot;sub1&quot;)) .subscribe();// logmain 1: sub2-3, number: 1main 1: sub2-3, number: 2main 1: sub2-3, number: 3main 2: pub1-2, number: 1main 2: pub1-2, number: 2main 2: pub1-2, number: 3 subscribeOn subscribeOn은 호출된 곳의 앞쪽 publisher 체인과 뒷쪽의 subscriber 체인을 한꺼번에 별도의 스레드로 분리한다. 느린 publisher 와 빠른 subscriber 의 조합에서 쓰기 적합하다고 하는데… 무슨 말인지 잘 모르겠다. 외부서비스에서 데이터를 읽어올때 쓰기 좋다고 한다. 위 예시처럼 subscribeOn을 동일한 체인에서 중첩해서 사용할 경우 첫번째 subscribeOn 앞뒤 체인을 묶어서 sub2 스케줄러에서 담당하고 두번째 subscribeOn 앞뒤 체인을 묶어서 sub1 스케줄러가 담당해야 하지만 이미 sub2에서 담당해서 그대로 sub2로 돌게 된다. publishOn publishOn은 호출된 곳의 앞쪽 publisher 체인은 그대로 두고 뒷쪽 subscriber 체인만 별도의 스레드로 분리한다. 빠른 publisher와 느린 subscriber 일때 쓰기 좋다고 하고 외부서비스로 데이터를 쓰는 작업할 때 좋닫고 한다. 위 예시에서 subscribeOn으로 sub2 스레드에서 돌다가 publishOn을 만나면서 pub1 스케줄러에 작업을 할당하고 나머지 연산을 진행한다. 🔗 출처 Reactor Flux의 publishOn subscribeOn을 이용한 스케쥴링","link":"/2023/01/19/spring/WebFlux/spring-webflux-1/"},{"title":"5.중간메모","text":"복습 겸 다시 프로젝트 생성해서 만들어보다가 몇가지 간략히 정리 1. springboot에서 html파일을 templates에서 읽도록 하기 템플릿 엔진과 관련된 의존성을 추가하면 자동으로 /recourses/templates 아래에서 템플릿 파일을 찾도록 하는거 같은데 원래 기본 경로는 /recourses/static 이다. 따라서 별다른 설정을 하지 않고 html 파일을 templates 폴더 밑에 놓고 찾으려고 하면 당연히 404 에러를 볼수 밖에 없다. html 파일을 templates 아래에 관리하고 싶다면 몇가지 작업을 해야하는데 우선 스프링부트에서 WebMVC 설정을 유지하면서 기능을 확장하기 위해 WebMvcConfigurer를 implements 하고 addResourceHandlers 를 오버라이드하여 아래와 같이 작성한다. 123456789101112@RequiredArgsConstructor@Configurationpublic class WebConfig implements WebMvcConfigurer { @Override public void addResourceHandlers(ResourceHandlerRegistry registry) { registry.addResourceHandler(&quot;/**&quot;) .addResourceLocations(&quot;classpath:/templates/&quot;) .setCacheControl(CacheControl.maxAge(10, TimeUnit.MINUTES)); }} 2. application.properties -&gt; application.yml 변환 가독성 측면에서 yaml 파일이 더 좋아보여서 기존 내용을 변환해서 쓰려다보니 naver oauth2 설정을 하면 오류가 발생한다. redirect-uri: '{baseUrl}/{action}/oauth2/code/{registrationId}' yaml 에서는 / (슬러시)를 그대로 쓰면 파싱 에러가 난다. 따옴표나 작은 따옴표로 감싸주면 된다.","link":"/2021/04/25/book/springboot-aws-webservice/5/"},{"title":"[WebFlux] Mono.defer(), Mono.fromCallable()","text":"Mono.defer()와 Mono.fromCallable() 모두 지연 평가(lazy evaluation)를 제공하는데 사용된다. 지연 평가란 호출 시점을 지연시켜서 필요한 시점에서 코드를 실행하는 방식을 의미한다. Mono.defer() 메서드는 Mono의 factory 메서드 중 하나로, 인자로 전달받은 Supplier 함수를 사용하여 Mono 객체를 생성한다. 이때 Supplier 함수가 호출되는 시점은 Mono 객체를 subscribe 할 때이다. Mono.fromCallable() 메서드는 Mono의 factory 메서드 중 하나로, 인자로 전달받은 Callable 함수를 사용하여 Mono 객체를 생성한다. 이때 Callable함수가 호출되는 시점은 Mono객체 생성 시점이다. 따라서 Mono.defer()는 Mono 객체를 생성할 때마다 Supplier함수를 실행하여 객체를 생성하는 방식을 사용하며, Mono.fromCallable()은 Mono객체를 생성하는 시점에서 Callable함수를 실행하여 객체를 생성하는 방식을 사용한다. Mono.fromCallable()은 즉시 실행되므로, Mono.defer()보다 더 많은 자원을 소비할 수 있다. Mono.defer()는 Mono의 객체 생성 시점을 늦추고, 객체 생성과 동시에 상태를 초기화할 수 있는 장점이 있고 반면에 Mono.fromCallable()은 Callable함수의 실행 결과를 즉시 Mono객체에 포함시키므로, 결과를 즉시 필요로 하는 경우에 유용하다. 123456789101112131415161718192021public static void main(String[] args){ callExternalService() .repeat(3) .doOnNext(System.out::println) .subscribe();}private static Mono&lt;?&gt; callExternalService(){ System.out.println(&quot;External service is called.&quot;); return Mono.just(&quot;result&quot;);}/*External service is called.resultresultresultresult*/ Mono.just(“result”)의 결과를 repeat(3)으로 세 번 반복하게 되면, 해당 Mono를 세 번 실행하게 된다. 하지만, Mono.just(“result”)는 호출될 때마다 값을 생성하는 게 아니라, 최초 1회 호출시 생성한 값을 계속해서 방출하게 된다. 그래서 &quot;External service is called.&quot;는 처음 1회만 출력되고, 세 번 반복되는 Mono.just(“result”)는 이미 생성된 값을 방출하게 되므로, 추가로 호출할 필요가 없어지게 된다. 따라서 &quot;External service is called.&quot;는 최초 1회만 출력되고, 세 번 반복되는 Mono.just(“result”)는 이미 생성된 값을 방출하게 된다.","link":"/2023/04/04/spring/WebFlux/spring-webflux-3/"},{"title":"[WebFlux] map과 flatMap에 대해서","text":"spring WebFlux를 사용하다보면 체인 연산자로 FlatMap을 사용해야할지, Map을 사용해야할지 헷갈리는 경우가 있어서 정리해봤다. map 함수 public final Flux map(Function&lt;? superT,? extends V&gt; mapper) Transform the items emitted by this [Flux](https://projectreactor.io/docs/core/release/api/reactor/core/publisher/Flux.html) by applying a synchronous function to each item. map 함수는 데이터 스트림의 각 요소를 변환하는데 사용되는 함수로 각 요소를 동기적으로 처리하며, 각 요소를 변환하여 새로운 데이터 스트림을 반환한다. 간단한 샘플코드를 보자. 123456789101112131415@Testvoid sampleMapTest() { // Arrange Function&lt;String, String&gt; mapper = (str) -&gt; str.toUpperCase(); Flux&lt;String&gt; testFlux = Flux.just(&quot;apple&quot;, &quot;banana&quot;, &quot;carrot&quot;); // Act Flux&lt;String&gt; resultFlux = testFlux.map(mapper); // Assert StepVerifier.create(resultFlux) .expectNext(&quot;APPLE&quot;, &quot;BANANA&quot;, &quot;CARROT&quot;) .expectComplete() .verify();} Function mapper는 스트림을 구독할 때 실행되며 각 요소의 순서를 유지하며 입력값을 변환하여 새로운 값으로 출력하게 된다. flatMap 함수 public final Flux flatMap(Function&lt;? superT,? extendsPublisher&lt;? extends V&gt;&gt; mapper, int concurrency) Transform the elements emitted by this [Flux](https://projectreactor.io/docs/core/release/api/reactor/core/publisher/Flux.html) asynchronously into Publishers, then flatten these inner publishers into a single [Flux](https://projectreactor.io/docs/core/release/api/reactor/core/publisher/Flux.html) through merging, which allow them to interleave. flatMap 함수는 데이터 스트림의 각 요소별로 publisher를 생성하여 여러개의 하위 스트림이 생성되고 subscribe를 통해 단일 스트림으로 묶은 Publisher로 반환한다. 이러한 Publisher를 비동기적으로 처리하여 순서를 보장하지 않기 때문에 순서를 보장하고 싶으면 flatMapSequential 이나 concatMap 등을 사용할 수 있다. 샘플코드를 보자. 123456789101112131415@Testvoid sampleFlatMapTest() { // Arrange Function&lt;String, Publisher&lt;String&gt;&gt; mapper = (str) -&gt; Flux.just(str.toUpperCase().split(&quot;&quot;)).subscribeOn(Schedulers.boundedElastic()); Flux&lt;String&gt; testFlux = Flux.just(&quot;apple&quot;, &quot;banana&quot;, &quot;carrot&quot;); // Act Flux&lt;String&gt; resultFlux = testFlux.flatMap(mapper); List&lt;String&gt; output = new ArrayList&lt;&gt;(); resultFlux.subscribe(output::add); // Assert assertThat(output).containsExactlyInAnyOrder(&quot;A&quot;, &quot;P&quot;, &quot;P&quot;, &quot;L&quot;, &quot;E&quot;, &quot;B&quot;, &quot;A&quot;, &quot;N&quot;, &quot;A&quot;, &quot;N&quot;, &quot;A&quot;, &quot;C&quot;, &quot;A&quot;, &quot;R&quot;, &quot;R&quot;, &quot;O&quot;, &quot;T&quot;);} mapper에서 Publisher로 반환되는데 별도의 스레드를 할당하고 flatMap에서 비동기로 동작하게 되면서 순서를 보장하지 않는 flatMap의 응답값은 apple, banana, carrot의 글자가 섞이게 된다. 사용 시 고려사항 map은 각 요소가 1:1로 매핑되서 동기적으로 처리된다. flatMap은 각 요소에 대해서 1:다 매핑이 되서 단일 스트림으로 병합되고 publisher의 동작에 따라서 동기적, 혹은 비동기적으로 동작할 수 있다. 단순히 DTO 객체변환이라든지, 단일요소에 대한 간단한 계산같은건 map을 사용하면 좋고 각 요소의 처리 시간이 다를 수 있는 작업이나 외부 서비스, DB 호출 등의 작업을 할때는 비동기 동작이 가능한 flatMap을 사용하는게 좋다. map은 단일 데이터 스트림의 각 요소를 독립적으로 처리하기 때문에 요소의 크기에 따라 성능차이가 크진 않지만 flatMap은 데이터 스트림을 개별적으로 처리 후 병합하기 위해 데이터 스트림의 크기나 작업량에 따라서 성능적으로 영향이 map에 비해서 상대적으로 클 수 있다.","link":"/2023/06/13/spring/WebFlux/spring-webflux-4/"},{"title":"[WebFlux] Flux와 Stream의 reduce, 그리고 chatGPT","text":"평소처럼 개발하던 중 reduce를 사용한 코드에서 오류가 발생했다. 12345java: cannot find symbol symbol: variable accumulatedMapjava: cannot find symbol symbol: variable currentMap 오류가 발생한 상황을 샘플코드로 대략적으로 재현해봤다. 123456789101112131415161718192021222324252627282930313233343536// sample 1List&lt;Integer&gt; list = IntStream.range(0,10).boxed().collect(Collectors.toList());var collectMap = list.stream() .map(i -&gt; { Map&lt;String, Integer&gt; map = new HashMap&lt;&gt;(); map.put(&quot;key&quot; + i, i * 10); return map; }) .reduce(**Collections.emptyMap()**, (accumulatedMap, currentMap) -&gt; new HashMap&lt;&gt;() { { putAll(accumulatedMap); putAll(currentMap); } });System.out.println(&quot;Stream reduce Map size: &quot; + collectMap.size());// Stream reduce Map size: 10// sample 2Flux.range(1, 10) .map(i -&gt; { Map&lt;String, Integer&gt; map = new HashMap&lt;&gt;(); map.put(&quot;key&quot; + i, i * 10); return map; }) //.reduce(**Collections.emptyMap()** ,(accumulatedMap, currentMap) -&gt; new HashMap&lt;&gt;() { // compile error!! &gt; `java: cannot find symbol` .reduce((accumulatedMap, currentMap) -&gt; new HashMap&lt;&gt;() { { putAll(accumulatedMap); putAll(currentMap); } }) .doOnNext(map -&gt; { System.out.println(&quot;Flux reduce Map size: &quot; + map.size()); }) .subscribe();// Flux reduce Map size: 10 Stream과 Flux의 reduce를 호출 시 initial 파라미터로 각각 Collections.emptyMap()를 사용하였으나 Stream은 정상동작하지만 Flux에선 컴파일 오류가 발생하였다. 여러 케이스로 바꿔가면서 테스트를 해보니 Collections.emptyMap()의 반환값이 불변객체라서 안되는거 같다는 추론까진 도달했지만 확신은 없었다. 구글링으로 찾아보려고 했지만 공식문서를 봐도 Flux에선 안되고 Stream에선 되는 이유에 대해서 명확한 답변을 찾기 어려워서 고민하던차에 요즘 핫한 chatGPT에 물어볼까 하는 생각이 들어서 바로 실행해봤다. 아래는 chatGPT 와 대화한 내용 i want public final &lt;A&gt; Mono&lt;A&gt; reduce(A initial, BiFunction&lt;A,? super T,A&gt; accumulator) example code. Here’s an example of how you could use the reduce method in Spring WebFlux: 123456789101112131415import reactor.core.publisher.Flux;import reactor.core.publisher.Mono;import java.util.function.BiFunction;public class ReduceExample { public static void main(String[] args) { Flux&lt;Integer&gt; numbers = Flux.just(1, 2, 3, 4, 5); BiFunction&lt;Integer, Integer, Integer&gt; accumulator = (a, b) -&gt; a + b; Mono&lt;Integer&gt; sum = numbers.reduce(0, accumulator); sum.subscribe(result -&gt; System.out.println(&quot;The sum of the numbers is: &quot; + result)); }} This example creates a Flux of integers and then uses the reduce method to sum them up. The reduce method takes an initial value, in this case 0, and a BiFunction that specifies how the current accumulation (a) is combined with each element from the flux (b) to produce the next accumulation. The subscribe method is then used to subscribe to the Mono returned by reduce and print out the result. initial Type is HashMap Here’s an example of how you could use the reduce method with an initial value of a HashMap: 1234567891011121314151617181920212223import reactor.core.publisher.Flux;import reactor.core.publisher.Mono;import java.util.HashMap;import java.util.function.BiFunction;public class ReduceExample { public static void main(String[] args) { Flux&lt;String&gt; words = Flux.just(&quot;apple&quot;, &quot;banana&quot;, &quot;cherry&quot;, &quot;apple&quot;, &quot;banana&quot;); BiFunction&lt;HashMap&lt;String, Integer&gt;, String, HashMap&lt;String, Integer&gt;&gt; accumulator = (map, word) -&gt; { if (!map.containsKey(word)) { map.put(word, 1); } else { map.put(word, map.get(word) + 1); } return map; }; Mono&lt;HashMap&lt;String, Integer&gt;&gt; wordCounts = words.reduce(new HashMap&lt;&gt;(), accumulator); wordCounts.subscribe(result -&gt; System.out.println(&quot;The word counts are: &quot; + result)); }} This example creates a Flux of strings and uses the reduce method to count the occurrences of each word. The reduce method takes an initial value, in this case a new empty HashMap, and a BiFunction that specifies how the current HashMap (map) is combined with each word from the flux (word) to produce the next HashMap. The subscribe method is then used to subscribe to the Mono returned by reduce and print out the result. Why does Mono&lt;HashMap&lt;String, Integer&gt;&gt; wordCounts = words.reduce(Collections.emptyMap(), accumulator); error occur? The error occurs because Collections.emptyMap() returns an unmodifiable map, which means you can’t modify it. The reduce method expects the accumulator function to modify the accumulator object, which is not possible with an unmodifiable map. You can resolve the issue by creating a new empty HashMap as the initial value instead of using Collections.emptyMap(), like this: 1Mono&lt;HashMap&lt;String, Integer&gt;&gt; wordCounts = words.reduce(new HashMap&lt;&gt;(), accumulator); This will give you a modifiable map that can be updated by the accumulator function as required by the reduce method. But why does Java Stream API succeed? The Java Stream API allows you to use the reduce method with an initial value of Collections.emptyMap() because the reduce operation in the Stream API operates on an internal data structure that is separate from the original data source. This allows the Stream API to handle unmodifiable maps as the initial value without any issues. However, in reactive programming with Spring WebFlux, the reduce method operates on the data stream directly, so you need to provide an accumulator object that is mutable. That’s why you need to use a new empty HashMap instead of Collections.emptyMap() as the initial value. 별로 기대하지 않았는데 원하는 내용에 대해서 생각보다 정확한 답변을 들을 수 있었다. 먼저 Flux의 reduce 호출 시initial가 HashMap인 케이스로 샘플코드를 요청하고 Collections.emptyMap() 을 초기값으로 사용 시 왜 오류가 나는지 물어봤고 Collections.emptyMap()으로 생성되는 객체는 returns an unmodifiable map 즉, immutable(불변)객체인데 reduce 메서드는 초기값으로 들어온 accumulator 객체를 수정할 것으로 예상했지만 수정이 불가능한 불변객체가 들어와서 오류가 발생하는거라고 한다. Stream의 reduce에선 왜 성공하는지? 에 대해선 Stream의 reduce는 원래 데이터와는 다른 별도의 내부 데이터 구조에서 동작하기 때문에 Collections.emptyMap()의 초기값으로 reduce를 사용할 수 있지만 리액티브 프로그래밍 기반의 WebFlux에선 데이터 스트림에서 reduce 메서드가 직접 동작하기 때문에 변경가능한 accumulator 객체를 제공해야 하므로 변경이 가능한 빈 HashMap 객체를 생성해야 한다고 답변하였다. 앞으로 나의 코딩선생님이 하나 더 늘었다는 생각에 흥분되면서도 한편으론 chatGPT를 사용해서 구글의 코딩인터뷰 Level3도 통과했다는 뉴스처럼 문맥을 파악해서 의도한 바를 정확하게 알려주는 답변을 보고 소름이 돋았다.","link":"/2023/02/14/spring/WebFlux/spring-webflux-2/"},{"title":"PostgreSQL 테이블 중복데이터 제거","text":"설계, 운영이 잘 되는 DB에선 많이 발생하는 문제는 아닐테지만 테이블의 특정 컬럼값을 기준으로 중복된 데이터를 삭제해야 하는 경우가 있다. 필자의 경우 PostgreSQL을 사용하던 중 이전 담당자가 테이블을 TEST_01_TB ~ TEST_10_TB 이런식으로 물리적으로 10개로 나눠서 설계하여 사용하던걸 인수받았는데 감리 검수에서 10개의 테이블을 하나로 합치라는 개선안내를 받았다. Hash Partition을 적용하여 테이블을 합쳤으나 파티션을 적용하니 기존에 복합키로 사용하던 Unique 조건이나 데이터를 생성 시 Insert쿼리문에서 ON CONFLICT 컬럼 DO NOTHING 옵션도 적용할 수 없어서 빠지게 되었는데 이 때문에 중복데이터가 발생하였다. 중복된 데이터를 어떻게 찾아서 지우느냐에 따라 수행시간에 큰 차이가 날거라고 생각해서 구글링을 해보니 몇가지 방법을 찾을 수 있었다. Test Table Schema 12345678910CREATE TABLE MEMBER_TB ( MEM_ID BIGSERIAL PRIMARY KEY, MEM_NAME VARCHAR(20) NOT NULL, MEM_PNUM VARCHAR(11) NOT NULL, MEM_EMAIL TEXT NOT NULL);CREATE TABLE BLACKLIST_TB ( MEM_PNUM VARCHAR(11) PRIMARY KEY); ROW_NUMBER() 유저 테이블에의 MEM_PNUM 컬럼을 기준으로 중복된 데이터가 있어서 이를 지우기 위해선 어떻게 해야할까? 구글링을 하고 제일먼저 찾은 방법은 ROW_NUMBER() 를 활용한 방법이었다. 1234567DELETE FROM MEMBER_TBWHERE MEM_ID IN ( SELECT MEM_ID FROM (SELECT MEM_ID, ROW_NUMBER() OVER(PARTITION BY MEM_PNUM ORDER BY MEM_ID ) RNUM FROM MEMBER_TB ) WHERE RNUM &gt; 1 ); 바로 적용할까 싶었지만 타겟 테이블의 데이터건수는 수십억개라 좀 더 빠른 방법이 없는지 더 찾아보던 중 Self Join과 Delete Using을 활용한 방법을 찾을 수 있었다. DELETE USING 특정 테이블을 참조하여 데이터를 삭제할 때 PostgreSQL의 경우 MySQL처럼 DELETE JOIN을 제공하진 않지만 USING을 사용하여 DELETE JOIN과 유사한 기능을 지원한다. 123DELETE FROM MEMBER_TB T1USING BLACKLIST_TB T2WHERE T1.MEM_PNUM = T2.MEM_PNUM; 예를 들면 이 처럼 USING 키워드를 사용하여 특정 테이블을 참조하여 유저테이블에서 블랙리스트 유저정보만 지울 수 있다. 그럼 여기서 SELF JOIN을 활용하여 MEM_PNUM, MEM_EMAIL 컬럼의 중복데이터를 삭제하고 싶으면 SQL문을 다음과 같은 식으로 작성할 수 있다. 12345DELETE FROM MEMBER_TB T1USING MEMBER_TB T2WHERE T1.CTID &lt; T2.CTID AND T1.MEM_PNUM = T2.MEM_PNUM AND T1.MEM_EMAIL = T2.MEM_EMAIL; 참고한 자료에선 1000만건의 데이터 중 10만건이 중복발생했을 때 약 42초정도 걸렸다고 했고 적용 전 확인해보기 위해 샘플로 Row가 약 800만에 중복데이터가 32만건 정도 들어있는 테이블로 테스트를 해보니 쿼리 실행에 약 1분정도 소요된 것을 확인하고 본래 타겟 테이블에 적용하였다. 그 후 데이터 생성 로직을 변경하여 중복데이터가 들어올 수 없도록 수정하였고 설계와 테스트를 더 잘 해야겠다는 생각이 절실하게 드는 하루였다. Reference PostgreSQL DELETE JOIN 중복된 관측치 제거하기 How to delete duplicate rows in postgresql?","link":"/2020/11/23/sql/postgresql/postgresql-1/"}],"tags":[{"name":"algorithm","slug":"algorithm","link":"/tags/algorithm/"},{"name":"programmers","slug":"programmers","link":"/tags/programmers/"},{"name":"kakao","slug":"kakao","link":"/tags/kakao/"},{"name":"java","slug":"java","link":"/tags/java/"},{"name":"blog","slug":"blog","link":"/tags/blog/"},{"name":"hexo","slug":"hexo","link":"/tags/hexo/"},{"name":"open graph","slug":"open-graph","link":"/tags/open-graph/"},{"name":"kakaotalk","slug":"kakaotalk","link":"/tags/kakaotalk/"},{"name":"googleTagManager","slug":"googleTagManager","link":"/tags/googleTagManager/"},{"name":"docker","slug":"docker","link":"/tags/docker/"},{"name":"functional","slug":"functional","link":"/tags/functional/"},{"name":"interview","slug":"interview","link":"/tags/interview/"},{"name":"git","slug":"git","link":"/tags/git/"},{"name":"spring","slug":"spring","link":"/tags/spring/"},{"name":"JWT","slug":"JWT","link":"/tags/JWT/"},{"name":"web","slug":"web","link":"/tags/web/"},{"name":"book","slug":"book","link":"/tags/book/"},{"name":"sql","slug":"sql","link":"/tags/sql/"},{"name":"postgresql","slug":"postgresql","link":"/tags/postgresql/"}],"categories":[{"name":"algorithm","slug":"algorithm","link":"/categories/algorithm/"},{"name":"blog","slug":"blog","link":"/categories/blog/"},{"name":"programmers","slug":"algorithm/programmers","link":"/categories/algorithm/programmers/"},{"name":"etc","slug":"etc","link":"/categories/etc/"},{"name":"docker","slug":"docker","link":"/categories/docker/"},{"name":"functional","slug":"functional","link":"/categories/functional/"},{"name":"interview","slug":"interview","link":"/categories/interview/"},{"name":"git","slug":"git","link":"/categories/git/"},{"name":"spring","slug":"spring","link":"/categories/spring/"},{"name":"web","slug":"web","link":"/categories/web/"},{"name":"book","slug":"book","link":"/categories/book/"},{"name":"WebFlux","slug":"spring/WebFlux","link":"/categories/spring/WebFlux/"},{"name":"spring5-reactive","slug":"book/spring5-reactive","link":"/categories/book/spring5-reactive/"},{"name":"clean-architecture","slug":"book/clean-architecture","link":"/categories/book/clean-architecture/"},{"name":"springboot-aws-webservice","slug":"book/springboot-aws-webservice","link":"/categories/book/springboot-aws-webservice/"},{"name":"sql","slug":"sql","link":"/categories/sql/"},{"name":"postgresql","slug":"sql/postgresql","link":"/categories/sql/postgresql/"}]}